{
  "contexts": {
    "topic_0": "It is important for this diverse\ngroup to be able to understand the models, intuitions, and strengths and\nweaknesses of the various approaches. But for this audience, many of the\ntechnical details behind statistical learning methods, such as optimiza-\ntion algorithms and theoretical properties, are not of primary interest. We believe that these students do not need a deep understanding of these\naspectsinordertobecomeinformedusersofthevariousmethodologies,and\n\n8 1. Introduction\nin order to contribute to their chosen fields through the use of statistical\nlearning tools. ISL is based on the following four premises. 1. Many statistical learning methods are relevant and useful in a wide\nrange of academic and non-academic disciplines, beyond just the sta-\ntistical sciences.Webelievethatmanycontemporarystatisticallearn-\ning procedures should, and will, become as widely available and used\nas is currently the case for classical methods such as linear regres-\nsion. As a result, rather than attempting to consider every possible\napproach (an impossible task), we have concentrated on presenting\nthe methods that we believe are most widely applicable. 2. Statistical learning should not be viewed as a series of black boxes.No\nsingle approach will perform well in all possible applications. With-\nout understanding all of the cogs inside the box, or the interaction\nbetween those cogs, it is impossible to select the best box. Hence, we\nhave attempted to carefully describe the model, intuition, assump-\ntions, and trade-offs behind each of the methods that we consider. 3. While it is important to know what job is performed by each cog, it is\nnot necessary to have the skills to construct the machine inside the\nbox! Thus, we have minimized discussion of technical details related\nto fitting procedures and theoretical properties.\n\nWe provide an overview of these shared characteristics in this\nsection. We will always assume that we have observed a set ofn different\ndata points. For example in Figure2.2 we observedn = 30 data points. These observations are called thetraining databecause we will use thesetraining\ndataobservations to train, or teach, our method how to estimatef. Letxij\nrepresent the value of thejth predictor, or input, for observationi, where\ni =1 , 2,...,n and j =1 , 2,...,p . Correspondingly, letyi represent the\nresponse variable for theith observation. Then our training data consist of\n{(x1,y 1), (x2,y 2),..., (xn,y n)} where xi =( xi1,x i2,...,x ip)T . Our goal is to apply a statistical learning method to the training data\nin order to estimate the unknown functionf. In other words, we want to\nfind a function\u02c6f such thatY \u2248 \u02c6f(X) for any observation(X, Y). Broadly\nspeaking, most statistical learning methods for this task can be character-\nized as eitherparametricor non-parametric. We now briefly discuss theseparametric\nnon-\nparametric\ntwo types of approaches. Parametric Methods\nParametric methods involve a two-step model-based approach. 1.\n\n4 1. Introduction\nDown Up\n0.46 0.48 0.50 0.52\nToday\u2019s Direction\nPredicted Probability\nFIGURE 1.3.We fit a quadratic discriminant analysis model to the subset\nof theSmarket data corresponding to the 2001\u20132004 time period, and predicted\nthe probability of a stock market decrease using the 2005 data. On average, the\npredicted probability of decrease is higher for the days in which the market does\ndecrease. Based on these results, we are able to correctly predict the direction of\nmovement in the market 60% of the time. to generate profits from the market. Nevertheless, in Chapter4, we explore\nthese data using several different statistical learning methods. Interestingly,\nthere are hints of some weak trends in the data that suggest that, at least\nfor this 5-year period, it is possible to correctly predict the direction of\nmovement in the market approximately 60% of the time (Figure1.3). Gene Expression Data\nThe previous two applications illustrate data sets with both input and\noutput variables. However, another important class of problems involves\nsituations in which we only observe input variables, with no corresponding\noutput. For example, in a marketing setting, we might have demographic\ninformation for a number of current or potential customers. We may wish to\nunderstand which types of customers are similar to each other by grouping\nindividuals according to their observed characteristics. This is known as a\nclustering problem. Unlike in the previous examples, here we are not trying\nto predict an output variable.\n\nFor example,linear modelsallow for relatively simple and in-linear model\nterpretable inference, but may not yield as accurate predictions as some\nother approaches. In contrast, some of the highly non-linear approaches\nthat we discuss in the later chapters of this book can potentially provide\nquite accurate predictions forY , but this comes at the expense of a less\ninterpretable model for which inference is more challenging. 2.1 What Is Statistical Learning? 21\n2.1.2 How Do We Estimatef? Throughout this book, we explore many linear and non-linear approaches\nfor estimatingf. However, these methods generally share certain charac-\nteristics.",
    "topic_1": "Consider an absurd example to illustrate the point. Running\na regression of shark attacks versus ice cream sales for data collected at\na given beach community over a period of time would show a positive\nrelationship, similar to that seen betweensales and newspaper. Of course\nno one has (yet) suggested that ice creams should be banned at beaches\nto reduce shark attacks. In reality, higher temperatures cause more people\nto visit the beach, which in turn results in more ice cream sales and more\nshark attacks. A multiple regression of shark attacks onto ice cream sales\nand temperature reveals that, as intuition implies, ice cream sales is no\nlonger a significant predictor after adjusting for temperature. 3.2.2 Some Important Questions\nWhen we perform multiple linear regression, we usually are interested in\nanswering a few important questions.\n\n62 3. Linear Regression\n0 50 100 150 200 250 300\n5 10 15 20 25\nTV\nSales\nFIGURE 3.1.For theAdvertising data, the least squares fit for the regression\nof sales onto TV is shown. The fit is found by minimizing the residual sum of\nsquares. Each grey line segment represents a residual. In this case a linear fit\ncaptures the essence of the relationship, although it overestimates the trend in the\nleft of the plot. Let \u02c6yi = \u02c6\u03b20 + \u02c6\u03b21xi be the prediction forY based on theith value ofX. Then ei = yi \u2212 \u02c6yi represents theith residual\u2014this is the difference betweenresidual\nthe ith observed response value and theith response value that is predicted\nby our linear model. We define theresidual sum of squares(RSS) as residual sum\nof squaresRSS =e2\n1 + e2\n2 + \u00b7\u00b7\u00b7 + e2\nn,\nor equivalently as\nRSS = (y1 \u2212 \u02c6\u03b20 \u2212 \u02c6\u03b21x1)2 +(y2 \u2212 \u02c6\u03b20 \u2212 \u02c6\u03b21x2)2 +\u00b7\u00b7\u00b7 +(yn \u2212 \u02c6\u03b20 \u2212 \u02c6\u03b21xn)2. (3.3)\nThe least squares approach chooses\u02c6\u03b20 and \u02c6\u03b21 to minimize the RSS. Using\nsome calculus, one can show that the minimizers are\n\u02c6\u03b21 =\n\u2211 n\ni=1(xi \u2212 \u00afx)(yi \u2212 \u00afy)\u2211 n\ni=1(xi \u2212 \u00afx)2 ,\n\u02c6\u03b20 =\u00af y \u2212 \u02c6\u03b21 \u00afx,\n(3.4)\nwhere \u00afy \u2261 1\nn\n\u2211 n\ni=1 yi and \u00afx \u2261 1\nn\n\u2211 n\ni=1 xi are the sample means. In other\nwords, (3.4) defines theleast squares coe\ufb00icient estimatesfor simple linear\nregression. Figure 3.1 displays the simple linear regression fit to theAdvertising\ndata, where\u02c6\u03b20 =7 .03 and \u02c6\u03b21 =0 .0475. In other words, according to\n\n3.1 Simple Linear Regression 63\n\u03b20\n\u03b21  2.11  2.15 \n 2.2  2.3  2.5 \n 2.5 \n 3 \n 3 \n5 6 7 8 9\n0.030.040.050.06\n\u25cf RSS\n\u03b21\u03b20\nFIGURE 3.2. Contour and three-dimensional plots of the RSS on the\nAdvertising data, usingsales as the response andTV as the predictor. The\nred dots correspond to the least squares estimates\u02c6\u03b20 and \u02c6\u03b21, given by (3.4). this approximation, an additional$1,000 spent on TV advertising is asso-\nciated with selling approximately47.5 additional units of the product. In\nFigure 3.2, we have computed RSS for a number of values of\u03b20 and \u03b21,\nusing the advertising data withsales as the response andTV as the predic-\ntor. In each plot, the red dot represents the pair of least squares estimates\n( \u02c6\u03b20, \u02c6\u03b21) given by (3.4). These values clearly minimize the RSS. 3.1.2 Assessing the Accuracy of the Coe\ufb00icient Estimates\nRecall from (2.1) that we assume that thetrue relationship betweenX and\nY takes the formY = f(X)+ \u03f5 for some unknown functionf, where\u03f5\nis a mean-zero random error term. Iff is to be approximated by a linear\nfunction, then we can write this relationship as\nY = \u03b20 + \u03b21X + \u03f5. (3.5)\nHere\u03b20 is the intercept term\u2014that is, the expected value ofY whenX =0 ,\nand \u03b21 is the slope\u2014the average increase inY associated with a one-unit\nincrease inX. The error term is a catch-all for what we miss with this\nsimple model: the true relationship is probably not linear, there may be\nother variables that cause variation inY , and there may be measurement\nerror. We typically assume that the error term is independent ofX. The model given by (3.5) defines thepopulation regression line, whichpopulation\nregression\nline\nis the best linear approximation to the true relationship betweenX and\n\n64 3. Linear Regression\n\u22122 \u221210 1 2\n\u221210 \u22125 0 5 10\nX\nY\n\u22122 \u221210 1 2\n\u221210 \u22125 0 5 10\nX\nY\nFIGURE 3.3. A simulated data set.Left: The red line represents the true\nrelationship, f(X)=2+3 X, which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate forf(X)\nbased on the observed data, shown in black.Right: The population regression line\nis again shown in red, and the least squares line in dark blue. In light blue, ten\nleast squares lines are shown, each computed on the basis of a separate random\nset of observations. Each least squares line is different, but on average, the least\nsquares lines are quite close to the population regression line. Y .1 The least squares regression coe\ufb00icient estimates (3.4) characterize the\nleast squares line(3.2). The left-hand panel of Figure3.3 displays theseleast squares\nlinetwo lines in a simple simulated example. We created100 random Xs, and\ngenerated 100 corresponding Y s from the model\nY =2+3 X + \u03f5, (3.6)\nwhere \u03f5 was generated from a normal distribution with mean zero. The\nred line in the left-hand panel of Figure3.3 displays thetrue relationship,\nf(X)=2 + 3 X, while the blue line is the least squares estimate based\non the observed data. The true relationship is generally not known for\nreal data, but the least squares line can always be computed using the\ncoe\ufb00icient estimates given in (3.4). In other words, in real applications,\nwe have access to a set of observations from which we can compute the\nleast squares line; however, the population regression line is unobserved. In the right-hand panel of Figure3.3 we have generated ten different data\nsets from the model given by (3.6) and plotted the corresponding ten least\nsquares lines. Notice that different data sets generated from the same true\nmodel result in slightly different least squares lines, but the unobserved\npopulation regression line does not change. 1The assumption of linearity is often a useful working model. However, despite what\nmany textbooks might tell us, we seldom believe that the true relationship is linear. 3.1 Simple Linear Regression 65\nAt first glance, the difference between the population regression line and\nthe least squares line may seem subtle and confusing. We only have one\ndata set, and so what does it mean that two different lines describe the\nrelationship between the predictor and the response? Fundamentally, the\nconcept of these two lines is a natural extension of the standard statistical\napproach of using information from a sample to estimate characteristics of a\nlarge population. For example, suppose that we are interested in knowing\nthe population mean\u00b5 of some random variableY . Unfortunately,\u00b5 is\nunknown, but we do have access ton observations fromY , y1,...,y n,\nwhich we can use to estimate\u00b5. A reasonable estimate is\u02c6\u00b5 =\u00af y, where\n\u00afy = 1\nn\n\u2211 n\ni=1 yi is the sample mean. The sample mean and the population\nmean are different, but in general the sample mean will provide a good\nestimate of the population mean. In the same way, the unknown coe\ufb00icients\n\u03b20 and \u03b21 in linear regression define the population regression line. We seek\nto estimate these unknown coe\ufb00icients using\u02c6\u03b20 and \u02c6\u03b21 given in (3.4). These\ncoe\ufb00icient estimates define the least squares line. The analogy between linear regression and estimation of the mean of a\nrandom variable is an apt one based on the concept ofbias. If we use thebias\nsample mean\u02c6\u00b5 to estimate\u00b5, this estimate isunbiased, in the sense thatunbiased\non average, we expect\u02c6\u00b5 to equal\u00b5. What exactly does this mean? It means\nthat on the basis of one particular set of observationsy1,...,y n, \u02c6\u00b5 might\noverestimate \u00b5, and on the basis of another set of observations,\u02c6\u00b5 might\nunderestimate \u00b5. But if we could average a huge number of estimates of\n\u00b5 obtained from a huge number of sets of observations, then this average\nwouldexactly equal\u00b5.Hence,anunbiasedestimatordoesnot systematically\nover- or under-estimate the true parameter. The property of unbiasedness\nholds for the least squares coe\ufb00icient estimates given by (3.4) as well: if\nwe estimate\u03b20 and \u03b21 on the basis of a particular data set, then our\nestimates won\u2019t be exactly equal to\u03b20 and \u03b21. But if we could average\nthe estimates obtained over a huge number of data sets, then the average\nof these estimates would be spot on! In fact, we can see from the right-\nhand panel of Figure3.3 that the average of many least squares lines, each\nestimated from a separate data set, is pretty close to the true population\nregression line. We continue the analogy with the estimation of the population mean\n\u00b5 of a random variableY . A natural question is as follows: how accurate\nis the sample mean\u02c6\u00b5 as an estimate of\u00b5? We have established that the\naverage of\u02c6\u00b5\u2019s over many data sets will be very close to\u00b5, but that a\nsingle estimate\u02c6\u00b5 may be a substantial underestimate or overestimate of\u00b5. How far off will that single estimate of\u02c6\u00b5 be? In general, we answer this\nquestion by computing thestandard errorof \u02c6\u00b5, written asSE(\u02c6\u00b5). We havestandard\nerrorthe well-known formula\nVar(\u02c6\u00b5) = SE(\u02c6\u00b5)2 = \u03c32\nn , (3.7)\n\n66 3. Linear Regression\nwhere \u03c3 is the standard deviation of each of the realizationsyi of Y .2\nRoughly speaking, the standard error tells us the average amount that this\nestimate \u02c6\u00b5 differs from the actual value of\u00b5. Equation3.7 also tells us how\nthis deviation shrinks withn\u2014the more observations we have, the smaller\nthe standard error of\u02c6\u00b5. In a similar vein, we can wonder how close\u02c6\u03b20\nand \u02c6\u03b21 are to the true values\u03b20 and \u03b21. To compute the standard errors\nassociated with\u02c6\u03b20 and \u02c6\u03b21, we use the following formulas:\nSE( \u02c6\u03b20)2\n= \u03c32\n[1\nn + \u00afx2\n\u2211 n\ni=1(xi \u2212 \u00afx)2\n]\n, SE( \u02c6\u03b21)2\n= \u03c32\n\u2211 n\ni=1(xi \u2212 \u00afx)2 , (3.8)\nwhere \u03c32 = Var(\u03f5). For these formulas to be strictly valid, we need to\nassumethattheerrors \u03f5i foreachobservationhavecommonvariance \u03c32 and\nare uncorrelated. This is clearly not true in Figure3.1, but the formula still\nturns out to be a good approximation. Notice in the formula thatSE( \u02c6\u03b21) is\nsmaller when thexi are more spread out; intuitively we have moreleverage\nto estimate a slope when this is the case. We also see thatSE( \u02c6\u03b20) would be\nthe same asSE(\u02c6\u00b5) if \u00afx were zero (in which case\u02c6\u03b20 would be equal to\u00afy). In\ngeneral,\u03c32 is not known, but can be estimated from the data. This estimate\nof \u03c3 is known as theresidual standard error, and is given by the formularesidual\nstandard\nerror\nRSE =\n\u221a\nRSS/(n \u2212 2). Strictly speaking, when\u03c32 is estimated from the\ndata we should write\u02c6SE( \u02c6\u03b21) to indicate that an estimate has been made,\nbut for simplicity of notation we will drop this extra \u201chat\u201d. Standard errors can be used to computeconfidence intervals. A 95%confidence\nintervalconfidence interval is defined as a range of values such that with 95%\nprobability, the range will contain the true unknown value of the param-\neter. The range is defined in terms of lower and upper limits computed\nfrom the sample of data. A 95% confidence interval has the following prop-\nerty: if we take repeated samples and construct the confidence interval for\neach sample, 95% of the intervals will contain the true unknown value of\nthe parameter. For linear regression, the 95% confidence interval for\u03b21\napproximately takes the form\n\u02c6\u03b21 \u00b1 2 \u00b7 SE( \u02c6\u03b21). (3.9)\nThat is, there is approximately a 95% chance that the interval\n[\n\u02c6\u03b21 \u2212 2 \u00b7 SE( \u02c6\u03b21), \u02c6\u03b21 +2 \u00b7 SE( \u02c6\u03b21)\n]\n(3.10)\n2This formula holds provided that then observations are uncorrelated. 3.1 Simple Linear Regression 67\nwill contain the true value of\u03b21.3 Similarly, a confidence interval for\u03b20\napproximately takes the form\n\u02c6\u03b20 \u00b1 2 \u00b7 SE( \u02c6\u03b20). (3.11)\nIn the case of the advertising data, the 95% confidence interval for\u03b20\nis [6.130, 7.935] and the 95% confidence interval for\u03b21 is [0.042, 0.053]. Therefore,wecanconcludethatintheabsenceofanyadvertising,saleswill,\non average, fall somewhere between6,130 and 7,935 units. Furthermore,\nfor each$1,000 increase in television advertising, there will be an average\nincrease in sales of between42 and 53 units. Standard errors can also be used to performhypothesis testson thehypothesis\ntestcoe\ufb00icients. The most common hypothesis test involves testing thenull\nhypothesis of null\nhypothesisH0 : There is no relationship betweenX and Y (3.12)\nversus thealternative hypothesis alternative\nhypothesisHa : There is some relationship betweenX and Y. (3.13)\nMathematically, this corresponds to testing\nH0 : \u03b21 =0\nversus\nHa : \u03b21 \u0338=0 ,\nsince if\u03b21 =0 then the model (3.5) reduces toY = \u03b20 + \u03f5, andX is\nnot associated withY . To test the null hypothesis, we need to determine\nwhether \u02c6\u03b21, our estimate for\u03b21, is su\ufb00iciently far from zero that we can\nbe confident that\u03b21 is non-zero. How far is far enough? This of course\ndepends on the accuracy of\u02c6\u03b21\u2014that is, it depends onSE( \u02c6\u03b21). IfSE( \u02c6\u03b21) is\nsmall, then even relatively small values of\u02c6\u03b21 may provide strong evidence\nthat \u03b21 \u0338=0 , and hence that there is a relationship betweenX and Y . In\ncontrast, ifSE( \u02c6\u03b21) is large, then\u02c6\u03b21 must be large in absolute value in order\nfor us to reject the null hypothesis. In practice, we compute at-statistic, t-statistic\ngiven by\nt =\n\u02c6\u03b21 \u2212 0\nSE( \u02c6\u03b21)\n, (3.14)\n3Approximately for several reasons. Equation3.10 relies on the assumption that the\nerrors are Gaussian. Also, the factor of2 in front of theSE( \u02c6\u03b21) term will vary slightly\ndepending on the number of observationsn in the linear regression. To be precise, rather\nthan the number 2, (3.10) should contain the 97.5% quantile of at-distribution with\nn\u22122 degrees of freedom. Details of how to compute the 95% confidence interval precisely\nin R will be provided later in this chapter. 68 3.\n\n14. This problem focuses on thecollinearity problem. (a) Perform the following commands inR:\n> set.seed (1)\n>x 1< - runif (100)\n>x 2< -0 . 5*x 1+ rnorm (100) / 10\n>y< -2+2*x 1+0 . 3*x 2+ rnorm (100)\nThe last line corresponds to creating a linear model in whichy is\na function ofx1 and x2. Write out the form of the linear model. What are the regression coe\ufb00icients? (b) What is the correlation betweenx1 and x2? Create a scatterplot\ndisplaying the relationship between the variables. (c) Using this data, fit a least squares regression to predicty using\nx1 and x2. Describe the results obtained. What are\u02c6\u03b20, \u02c6\u03b21, and\n\u02c6\u03b22? How do these relate to the true\u03b20, \u03b21, and\u03b22? Can you\nreject the null hypothesisH0 : \u03b21 =0 ? How about the null\nhypothesis H0 : \u03b22 =0 ? (d) Now fit a least squares regression to predicty using onlyx1. Comment on your results. Can you reject the null hypothesis\nH0 : \u03b21 =0 ? (e) Now fit a least squares regression to predicty using onlyx2. Comment on your results. Can you reject the null hypothesis\nH0 : \u03b21 =0 ? 128 3. Linear Regression\n(f) Do the results obtained in (c)\u2013(e) contradict each other? Explain\nyour answer.\n\n4. Suppose we fit a curve with basis functionsb1(X)= I(0 \u2264 X \u2264 2) \u2212\n(X \u22121)I(1 \u2264 X \u2264 2), b2(X)=( X \u22123)I(3 \u2264 X \u2264 4) +I(4 <X \u2264 5). We fit the linear regression model\nY = \u03b20 + \u03b21b1(X)+ \u03b22b2(X)+ \u03f5,\nand obtain coe\ufb00icient estimates\u02c6\u03b20 =1 , \u02c6\u03b21 =1 , \u02c6\u03b22 =3 . Sketch the\nestimated curve betweenX = \u22122 and X =6 . Note the intercepts,\nslopes, and other relevant information.",
    "topic_2": "Classification\n4.1 An Overview of Classification\nClassification problems occur often, perhaps even more so than regression\nproblems. Some examples include:\n1. A person arrives at the emergency room with a set of symptoms\nthat could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\n\nWhich\nof these approaches yields the best performance? This is page 366\nPrinter: Opaque this\n\nThis is page 367\nPrinter: Opaque this\n9\nSupport Vector Machines\nIn this chapter, we discuss thesupport vector machine(SVM), an approach\nfor classification that was developed in the computer science community in\nthe 1990s and that has grown in popularity since then. SVMs have been\nshown to perform well in a variety of settings, and are often considered one\nof the best \u201cout of the box\u201d classifiers. The support vector machine is a generalization of a simple and intu-\nitive classifier called themaximal margin classifier, which we introduce in\nSection 9.1. Though it is elegant and simple, we will see that this classifier\nunfortunately cannot be applied to most data sets, since it requires that\nthe classes be separable by a linear boundary. In Section9.2, we introduce\nthe support vector classifier, an extension of the maximal margin classifier\nthat can be applied in a broader range of cases. Section9.3 introduces the\nsupport vector machine, which is a further extension of the support vec-\ntor classifier in order to accommodate non-linear class boundaries. Support\nvector machines are intended for the binary classification setting in which\nthere are two classes; in Section9.4 we discuss extensions of support vector\nmachines to the case of more than two classes. In Section9.5 we discuss\nthe close connections between support vector machines and other statistical\nmethods such as logistic regression. People often loosely refer to the maximal margin classifier, the support\nvector classifier, and the support vector machine as \u201csupport vector\nmachines\u201d. To avoid confusion, we will carefully distinguish between these\nthree notions in this chapter. 368 9. Support Vector Machines\n9.1 Maximal Margin Classifier\nIn this section, we define a hyperplane and introduce the concept of an\noptimal separating hyperplane. 9.1.1 What Is a Hyperplane? In a p-dimensional space, ahyperplane is a flat a\ufb00ine subspace ofhyperplane\ndimension p \u2212 1.1 For instance, in two dimensions, a hyperplane is a flat\none-dimensional subspace\u2014in other words, a line. In three dimensions, a\nhyperplane is a flat two-dimensional subspace\u2014that is, a plane. Inp> 3\ndimensions, it can be hard to visualize a hyperplane, but the notion of a\n(p \u2212 1)-dimensional flat subspace still applies. The mathematical definition of a hyperplane is quite simple. In two di-\nmensions, a hyperplane is defined by the equation\n\u03b20 + \u03b21X1 + \u03b22X2 =0 (9.1)\nfor parameters\u03b20, \u03b21, and\u03b22. When we say that (9.1) \u201cdefines\u201d the hyper-\nplane, we mean that anyX =( X1,X 2)T for which (9.1) holds is a point\non the hyperplane. Note that (9.1) is simply the equation of a line, since\nindeed in two dimensions a hyperplane is a line. Equation 9.1 can be easily extended to thep-dimensional setting:\n\u03b20 + \u03b21X1 + \u03b22X2 + \u00b7\u00b7\u00b7 + \u03b2pXp =0 (9.2)\ndefines ap-dimensional hyperplane, again in the sense that if a pointX =\n(X1,X 2,...,X p)T in p-dimensional space (i.e. a vector of lengthp) satisfies\n(9.2), thenX lies on the hyperplane. Now, suppose thatX does not satisfy (9.2); rather,\n\u03b20 + \u03b21X1 + \u03b22X2 + \u00b7\u00b7\u00b7 + \u03b2pXp > 0. (9.3)\nThen this tells us thatX lies to one side of the hyperplane. On the other\nhand, if\n\u03b20 + \u03b21X1 + \u03b22X2 + \u00b7\u00b7\u00b7 + \u03b2pXp < 0, (9.4)\nthen X lies on the other side of the hyperplane. So we can think of the\nhyperplane as dividingp-dimensional space into two halves. One can easily\ndetermine on which side of the hyperplane a point lies by simply calculating\nthe sign of the left-hand side of (9.2). A hyperplane in two-dimensional\nspace is shown in Figure9.1. 1The worda\ufb00ine indicates that the subspace need not pass through the origin. 9.1 Maximal Margin Classifier 369\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\nX1\nX2\nFIGURE 9.1.The hyperplane1+2 X1 +3 X2 =0 is shown. The blue region is\nthe set of points for which1+2 X1 +3 X2 > 0, and the purple region is the set of\npoints for which1+2 X1 +3 X2 < 0. 9.1.2 Classification Using a Separating Hyperplane\nNow suppose that we have ann \u00d7 p data matrixX that consists ofn\ntraining observations inp-dimensional space,\nx1 =\n\u239b\n\u239c\u239d\nx11\n... x1p\n\u239e\n\u239f\u23a0,...,x n =\n\u239b\n\u239c\u239d\nxn1\n... xnp\n\u239e\n\u239f\u23a0, (9.5)\nand that these observations fall into two classes\u2014that is,y1,...,y n \u2208\n{\u22121, 1} where \u22121 represents one class and1 the other class. We also have a\ntest observation, ap-vector of observed featuresx\u2217 =\n(x\u2217\n1 ... x \u2217\np\n)T\n. Our\ngoal is to develop a classifier based on the training data that will correctly\nclassify the test observation using its feature measurements. We have seen\na number of approaches for this task, such as linear discriminant analysis\nand logistic regression in Chapter4, and classification trees, bagging, and\nboosting in Chapter8. We will now see a new approach that is based upon\nthe concept of aseparating hyperplane. separating\nhyperplaneSuppose that it is possible to construct a hyperplane that separates the\ntraining observations perfectly according to their class labels. Examples\nof three suchseparating hyperplanesare shown in the left-hand panel of\nFigure 9.2. We can label the observations from the blue class asyi =1 and\nthose from the purple class asyi = \u22121. Then a separating hyperplane has\nthe property that\n\u03b20 + \u03b21xi1 + \u03b22xi2 + \u00b7\u00b7\u00b7 + \u03b2pxip > 0 ifyi =1 , (9.6)\n\n370 9. Support Vector Machines\n\u221210 1 2 3\n\u221210 1 2 3\n\u221210 1 2 3\n\u221210 1 2 3\nX1X1\nX2\nX2\nFIGURE 9.2.Left: There are two classes of observations, shown in blue and\nin purple, each of which has measurements on two variables. Three separating\nhyperplanes, out of many possible, are shown in black.Right: A separating hy-\nperplane is shown in black. The blue and purple grid indicates the decision rule\nmade by a classifier based on this separating hyperplane: a test observation that\nfalls in the blue portion of the grid will be assigned to the blue class, and a test\nobservation that falls into the purple portion of the grid will be assigned to the\npurple class. and\n\u03b20 + \u03b21xi1 + \u03b22xi2 + \u00b7\u00b7\u00b7 + \u03b2pxip < 0 ifyi = \u22121. (9.7)\nEquivalently, a separating hyperplane has the property that\nyi(\u03b20 + \u03b21xi1 + \u03b22xi2 + \u00b7\u00b7\u00b7 + \u03b2pxip) > 0 (9.8)\nfor alli =1 ,...,n . Ifaseparatinghyperplaneexists,wecanuseittoconstructaverynatural\nclassifier: a test observation is assigned a class depending on which side of\nthe hyperplane it is located. The right-hand panel of Figure9.2 shows\nan example of such a classifier. That is, we classify the test observationx\u2217\nbasedonthesignof f(x\u2217)= \u03b20+\u03b21x\u2217\n1+\u03b22x\u2217\n2+\u00b7\u00b7\u00b7 +\u03b2px\u2217\np.If f(x\u2217) ispositive,\nthen we assign the test observation to class1, and iff(x\u2217) is negative, then\nwe assign it to class\u22121. We can also make use of themagnitude of f(x\u2217). If\nf(x\u2217) is far from zero, then this means thatx\u2217 lies far from the hyperplane,\nand so we can be confident about our class assignment forx\u2217. On the other\nhand, iff(x\u2217) is close to zero, thenx\u2217 is located near the hyperplane, and so\nwe are less certain about the class assignment forx\u2217. Not surprisingly, and\nas we see in Figure9.2, a classifier that is based on a separating hyperplane\nleads to a linear decision boundary. 9.1 Maximal Margin Classifier 371\n9.1.3 The Maximal Margin Classifier\nIn general, if our data can be perfectly separated using a hyperplane, then\nthere will in fact exist an infinite number of such hyperplanes. This is\nbecauseagivenseparatinghyperplanecanusuallybeshiftedatinybitupor\ndown, or rotated, without coming into contact with any of the observations. Three possible separating hyperplanes are shown in the left-hand panel\nof Figure9.2. In order to construct a classifier based upon a separating\nhyperplane, we must have a reasonable way to decide which of the infinite\npossible separating hyperplanes to use. A natural choice is themaximal margin hyperplane(also known as themaximal\nmargin\nhyperplane\noptimal separating hyperplane), which is the separating hyperplane that\noptimal\nseparating\nhyperplane\nis farthest from the training observations. That is, we can compute the\n(perpendicular) distance from each training observation to a given separat-\ning hyperplane; the smallest such distance is the minimal distance from the\nobservations to the hyperplane, and is known as themargin. The maximal\nmarginmargin hyperplane is the separating hyperplane for which the margin is\nlargest\u2014that is, it is the hyperplane that has the farthest minimum dis-\ntance to the training observations. We can then classify a test observation\nbasedonwhichside ofthemaximal marginhyperplaneit lies.Thisis known\nas themaximal margin classifier. We hope that a classifier that has a largemaximal\nmargin\nclassifier\nmargin on the training data will also have a large margin on the test data,\nand hence will classify the test observations correctly. Although the maxi-\nmal margin classifier is often successful, it can also lead to overfitting when\np is large. If \u03b20, \u03b21,..., \u03b2p are the coe\ufb00icients of the maximal margin hyperplane,\nthen the maximal margin classifier classifies the test observationx\u2217 based\non the sign off(x\u2217)= \u03b20 + \u03b21x\u2217\n1 + \u03b22x\u2217\n2 + \u00b7\u00b7\u00b7 + \u03b2px\u2217\np. Figure 9.3 shows the maximal margin hyperplane on the data set of\nFigure 9.2. Comparing the right-hand panel of Figure9.2 to Figure9.3,\nwe see that the maximal margin hyperplane shown in Figure9.3 does in-\ndeed result in a greater minimal distance between the observations and the\nseparating hyperplane\u2014that is, a larger margin. In a sense, the maximal\nmargin hyperplane represents the mid-line of the widest \u201cslab\u201d that we can\ninsert between the two classes. ExaminingFigure 9.3,weseethatthreetrainingobservationsareequidis-\ntant from the maximal margin hyperplane and lie along the dashed lines\nindicating the width of the margin. These three observations are known as\nsupport vectors, since they are vectors inp-dimensional space (in Figure9.3, support\nvectorp =2 ) and they \u201csupport\u201d the maximal margin hyperplane in the sense\nthat if these points were moved slightly then the maximal margin hyper-\nplane would move as well. Interestingly, the maximal margin hyperplane\ndepends directly on the support vectors, but not on the other observations:\na movement to any of the other observations would not affect the separating\nhyperplane, provided that the observation\u2019s movement does not cause it to\n\n372 9. Support Vector Machines\n\u221210 1 2 3\n\u221210 1 2 3\nX1\nX2\nFIGURE 9.3. There are two classes of observations, shown in blue and in\npurple. The maximal margin hyperplane is shown as a solid line. The margin\nis the distance from the solid line to either of the dashed lines. The two blue\npoints and the purple point that lie on the dashed lines are the support vectors,\nand the distance from those points to the hyperplane is indicated by arrows. The\npurple and blue grid indicates the decision rule made by a classifier based on this\nseparating hyperplane. cross the boundary set by the margin. The fact that the maximal margin\nhyperplane depends directly on only a small subset of the observations is\nan important property that will arise later in this chapter when we discuss\nthe support vector classifier and support vector machines. 9.1.4 Construction of the Maximal Margin Classifier\nWe now consider the task of constructing the maximal margin hyperplane\nbased on a set ofn training observationsx1,...,x n \u2208 Rp and associated\nclass labelsy1,...,y n \u2208 {\u22121, 1}. Briefly, the maximal margin hyperplane\nis the solution to the optimization problem\nmaximize\u03b20,\u03b21,...,\u03b2p,M\nM (9.9)\nsubject to\np\u2211\nj=1\n\u03b22\nj =1 , (9.10)\nyi(\u03b20 + \u03b21xi1 + \u03b22xi2 + \u00b7\u00b7\u00b7 + \u03b2pxip) \u2265 M \u2200 i =1 , . .\n\n5. Suppose we produce ten bootstrapped samples from a data set\ncontaining red and green classes. We then apply a classification tree\nto each bootstrapped sample and, for a specific value ofX, produce\n10 estimates ofP(Class is Red|X):\n0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75. There are two common ways to combine these results together into a\nsingle class prediction. One is the majority vote approach discussed in\nthis chapter. The second approach is to classify based on the average\nprobability. In this example, what is the final classification under each\nof these two approaches? 6. Provide a detailed explanation of the algorithm that is used to fit a\nregression tree. 8.4 Exercises 363\nApplied\n7. In the lab, we applied random forests to theBoston data usingmtry =\n6 and usingntree = 25 and ntree = 500. Create a plot displaying the\ntest error resulting from random forests on this data set for a more\ncomprehensive range of values formtry and ntree. You can model\nyour plot after Figure8.10.\n\n2. An online banking service must be able to determine whether or not\na transaction being performed on the site is fraudulent, on the basis\nof the user\u2019s IP address, past transaction history, and so forth. 3. On the basis of DNA sequence data for a number of patients with\nand without a given disease, a biologist would like to figure out which\nDNA mutations are deleterious (disease-causing) and which are not. Just as in the regression setting, in the classification setting we have a\nset of training observations(x1,y 1),..., (xn,y n) that we can use to build\na classifier. We want our classifier to perform well not only on the training\ndata, but also on test observations that were not used to train the classifier. In this chapter, we will illustrate the concept of classification using the\nsimulated Default data set. We are interested in predicting whether an\nindividual will default on his or her credit card payment, on the basis of\nannual income and monthly credit card balance. The data set is displayed\nin Figure4.1. In the left-hand panel of Figure4.1, we have plotted annual\nincome and monthly credit cardbalance for a subset of10, 000 individuals. The individuals who defaulted in a given month are shown in orange, and\nthose who did not in blue. (The overall default rate is about 3%, so we\nhave plotted only a fraction of the individuals who did not default.) It\nappears that individuals who defaulted tended to have higher credit card\nbalances than those who did not. In the center and right-hand panels of\nFigure 4.1, two pairs of boxplots are shown. The first shows the distribution\nof balance split by the binarydefault variable; the second is a similar plot\nforincome. In this chapter, we learn how to build a model to predictdefault\n(Y ) for any given value ofbalance (X1) andincome (X2). SinceY is not\nquantitative, the simple linear regression model of Chapter3 is not a good\nchoice: we will elaborate on this further in Section4.2. It is worth noting that Figure4.1 displays a very pronounced relation-\nship between the predictorbalance and the responsedefault. In most real\napplications, the relationship between the predictor and the response will\nnot be nearly so strong. However, for the sake of illustrating the classifica-\ntion procedures discussed in this chapter, we use an example in which the\nrelationship between the predictor and the response is somewhat exagger-\nated. 4.2 Why Not Linear Regression? 131\n0 500 1000 1500 2000 25000 20000 40000 60000\nBalance\nIncome\nNo Y es0 500 1000 1500 2000 2500\nDefault\nBalance\nNo Y es0 20000 40000 60000\nDefault\nIncome\nFIGURE 4.1.The Default data set.Left: The annual incomes and monthly\ncredit card balances of a number of individuals. The individuals who defaulted on\ntheir credit card payments are shown in orange, and those who did not are shown\nin blue.Center: Boxplots ofbalance as a function ofdefault status. Right:\nBoxplots ofincome as a function ofdefault status. 4.2 Why Not Linear Regression? We have stated that linear regression is not appropriate in the case of a\nqualitative response. Why not?",
    "topic_3": ">b o o t . f n< -function (data, index )\n+ coef(lm(mpg \u223c horsepower, data = data, subset = index ))\n> boot.fn(Auto, 1:392)\n(Intercept) horsepower\n39.936 -0.158\nThe boot.fn() function can also be used in order to create bootstrap esti-\nmates for the intercept and slope terms by randomly sampling from among\nthe observations with replacement. Here we give two examples. > set.seed (1)\n> boot.fn(Auto, sample (392, 392, replace =T ) )\n(Intercept) horsepower\n40.341 -0.164\n\n218 5. Resampling Methods\n> boot.fn(Auto, sample (392, 392, replace =T ) )\n(Intercept) horsepower\n40.119 -0.158\nNext, we use theboot() function to compute the standard errors of 1,000\nbootstrap estimates for the intercept and slope terms. > boot(Auto, boot.fn, 1000)\nORDINARY NONPARAMETRIC BOOTSTRAP\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\nBootstrap Statistics :\noriginal bias std. error\nt1* 39.936 0.0545 0.8413\nt2* -0.158 -0.0006 0.0073\nThis indicates that the bootstrap estimate forSE( \u02c6\u03b20) is 0.84, and that\nthe bootstrap estimate forSE( \u02c6\u03b21) is 0.0073. As discussed in Section3.1.2,\nstandard formulas can be used to compute the standard errors for the\nregression coe\ufb00icients in a linear model. These can be obtained using the\nsummary() function. > summary (lm(mpg \u223c horsepower, data = Auto))$ coef\nEstimate Std. Error t value Pr(>|t|)\n(Intercept) 39.936 0.71750 55.7 1.22e-187\nhorsepower -0.158 0.00645 -24.5 7.03e-81\nThe standard error estimates for\u02c6\u03b20 and \u02c6\u03b21 obtained using the formulas\nfrom Section3.1.2 are 0.717 for the intercept and0.0064 for the slope. Interestingly, these are somewhat different from the estimates obtained\nusing the bootstrap. Does this indicate a problem with the bootstrap? In\nfact, it suggests the opposite.\n\nHint: You will have to create the response variable yourself, using the\nvariables that are contained in theBoston data set. This is page 196\nPrinter: Opaque this\n\nThis is page 197\nPrinter: Opaque this\n5\nResampling Methods\nResampling methodsare an indispensable tool in modern statistics. They\ninvolverepeatedlydrawingsamplesfromatrainingsetandrefittingamodel\nof interest on each sample in order to obtain additional information about\nthe fitted model. For example, in order to estimate the variability of a linear\nregression fit, we can repeatedly draw different samples from the training\ndata, fit a linear regression to each new sample, and then examine the\nextent to which the resulting fits differ. Such an approach may allow us to\nobtain information that would not be available from fitting the model only\nonce using the original training sample. Resampling approaches can be computationally expensive, because they\ninvolve fitting the same statistical method multiple times using different\nsubsets of the training data. However, due to recent advances in computing\npower, the computational requirements of resampling methods generally\nare not prohibitive. In this chapter, we discuss two of the most commonly\nused resampling methods,cross-validationand thebootstrap. Both methods\nare important tools in the practical application of many statistical learning\nprocedures. For example, cross-validation can be used to estimate the test\nerrorassociatedwithagivenstatisticallearningmethodinordertoevaluate\nits performance, or to select the appropriate level of flexibility. The process\nof evaluating a model\u2019s performance is known asmodel assessment, whereasmodel\nassessmentthe process of selecting the proper level of flexibility for a model is known as\nmodel selection. The bootstrap is used in several contexts, most commonlymodel\nselectionto provide a measure of accuracy of a parameter estimate or of a given\nstatistical learning method. 198 5. Resampling Methods\n5.1 Cross-Validation\nIn Chapter2 we discuss the distinction between thetest error rateand the\ntraining error rate.Thetesterroristheaverageerrorthatresultsfromusing\nastatisticallearningmethodtopredicttheresponseonanewobservation\u2014\nthat is, a measurement that was not used in training the method. Given\na data set, the use of a particular statistical learning method is warranted\nif it results in a low test error.\n\n10. We now use boosting to predictSalary in theHitters data set. (a) Remove the observations for whom the salary information is\nunknown, and then log-transform the salaries. (b) Create a training set consisting of the first 200 observations, and\na test set consisting of the remaining observations. (c) Perform boosting on the training set with 1,000 trees for a range\nof values of the shrinkage parameter\u03bb. Produce a plot with\ndifferent shrinkage values on thex-axis and the corresponding\ntraining set MSE on they-axis. (d) Produce a plot with different shrinkage values on thex-axis and\nthe corresponding test set MSE on they-axis. (e) Compare the test MSE of boosting to the test MSE that results\nfrom applying two of the regression approaches seen in\nChapters 3 and 6. (f) Which variables appear to be the most important predictors in\nthe boosted model? (g) Now apply bagging to the training set. What is the test set MSE\nfor this approach? 11. This question uses theCaravan data set. (a) Create a training set consisting of the first 1,000 observations,\nand a test set consisting of the remaining observations. (b) Fit a boosting model to the training set withPurchase as the\nresponse and the other variables as predictors. Use 1,000 trees,\nand a shrinkage value of0.01. Which predictors appear to be\nthe most important? 8.4 Exercises 365\n(c) Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated prob-\nability of purchase is greater than20 %. Form a confusion ma-\ntrix. What fraction of the people predicted to make a purchase\ndo in fact make one? How does this compare with the results\nobtained from applying KNN or logistic regression to this data\nset?\n\n12. Apply boosting, bagging, random forests, and BART to a data set\nof your choice. Be sure to fit the models on a training set and to\nevaluate their performance on a test set. How accurate are the results\ncompared to simple methods like linear or logistic regression?",
    "topic_4": "For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer. (a) The lasso, relative to least squares, is:\ni. More flexible and hence will give improved prediction ac-\ncuracy when its increase in bias is less than its decrease in\nvariance. ii. More flexible and hence will give improved prediction accu-\nracy when its increase in variance is less than its decrease\nin bias. iii. Less flexible and hence will give improved prediction accu-\nracy when its increase in bias is less than its decrease in\nvariance. iv. Less flexible and hence will give improved prediction accu-\nracy when its increase in variance is less than its decrease\nin bias. (b) Repeat (a) for ridge regression relative to least squares. 284 6. Linear Model Selection and Regularization\n(c) Repeat (a) for non-linear methods relative to least squares. 3. Suppose we estimate the regression coe\ufb00icients in a linear regression\nmodel by minimizing\nn\u2211\ni=1\n\u239b\n\u239dyi \u2212 \u03b20 \u2212\np\u2211\nj=1\n\u03b2jxij\n\u239e\n\u23a0\n2\nsubject to\np\u2211\nj=1\n|\u03b2j| \u2264 s\nfor a particular value ofs. For parts (a) through (e), indicate which\nof i.\n\nFor example, in theCredit\ndata set, it appears that the most important variables areincome, limit,\nrating, andstudent. So we might wish to build a model including just\nthese predictors. However, ridge regression will always generate a model\ninvolving all ten predictors. Increasing the value of\u03bb will tend to reduce\nthe magnitudes of the coe\ufb00icients, but will not result in exclusion of any of\nthe variables. 242 6. Linear Model Selection and Regularization\nThe lasso is a relatively recent alternative to ridge regression that over-lasso\ncomes this disadvantage. The lasso coe\ufb00icients,\u02c6\u03b2L\n\u03bb , minimize the quantity\nn\u2211\ni=1\n\u239b\n\u239dyi \u2212 \u03b20 \u2212\np\u2211\nj=1\n\u03b2jxij\n\u239e\n\u23a0\n2\n+ \u03bb\np\u2211\nj=1\n|\u03b2j| = RSS +\u03bb\np\u2211\nj=1\n|\u03b2j|. (6.7)\nComparing (6.7) to (6.5), we see that the lasso and ridge regression have\nsimilar formulations. The only difference is that the\u03b22\nj term in the ridge\nregression penalty (6.5) has been replaced by|\u03b2j| in the lasso penalty (6.7). In statistical parlance, the lasso uses an\u21131 (pronounced \u201cell 1\u201d) penalty\ninstead of an\u21132 penalty. The\u21131 norm of a coe\ufb00icient vector\u03b2 is given by\n\u2225\u03b2\u22251 = \u2211 |\u03b2j|. As with ridge regression, the lasso shrinks the coe\ufb00icient estimates to-\nwards zero. However, in the case of the lasso, the\u21131 penalty has the effect\nof forcing some of the coe\ufb00icient estimates to be exactly equal to zero when\nthe tuning parameter\u03bb is su\ufb00iciently large. Hence, much like best subset se-\nlection, the lasso performsvariable selection. As a result, models generated\nfrom the lasso are generally much easier to interpret than those produced\nby ridge regression. We say that the lasso yieldssparse models\u2014that is,sparse\nmodels that involve only a subset of the variables. As in ridge regression,\nselecting a good value of\u03bb for the lasso is critical; we defer this discussion\nto Section6.2.3, where we use cross-validation. As an example, consider the coe\ufb00icient plots in Figure6.6, which are gen-\nerated from applying the lasso to theCredit data set. When\u03bb =0 , then\nthe lasso simply gives the least squares fit, and when\u03bb becomes su\ufb00iciently\nlarge, the lasso gives the null model in which all coe\ufb00icient estimates equal\nzero. However, in between these two extremes, the ridge regression and\nlasso models are quite different from each other. Moving from left to right\nin the right-hand panel of Figure6.6, we observe that at first the lasso re-\nsults in a model that contains only therating predictor. Thenstudent and\nlimit enter the model almost simultaneously, shortly followed byincome. Eventually, the remaining variables enter the model. Hence, depending on\nthe value of\u03bb, the lasso can produce a model involving any number of vari-\nables. In contrast, ridge regression will always include all of the variables in\nthe model, although the magnitude of the coe\ufb00icient estimates will depend\non \u03bb. 6.2 Shrinkage Methods 243\n20 50 100 200 500 2000 5000\n\u2212200 0 100 200 300 400\nStandardized Coefficients\n0.0 0.2 0.4 0.6 0.8 1.0\n\u2212300 \u2212100 0 100 200 300 400\nStandardized Coefficients\nIncomeLimitRatingStudent\n\u03bb \u2225\u02c6\u03b2L\u03bb\u22251/\u2225\u02c6\u03b2\u22251\nFIGURE 6.6. The standardized lasso coe\ufb00icients on theCredit data set are\nshown as a function of\u03bb and \u2225\u02c6\u03b2L\n\u03bb \u22251/\u2225\u02c6\u03b2\u22251. Another Formulation for Ridge Regression and the Lasso\nOne can show that the lasso and ridge regression coe\ufb00icient estimates solve\nthe problems\nminimize\u03b2\n\u23a7\n\u23aa\u23a8\n\u23aa\u23a9\nn\u2211\ni=1\n\u239b\n\u239dyi \u2212 \u03b20 \u2212\np\u2211\nj=1\n\u03b2jxij\n\u239e\n\u23a0\n2\u23ab\n\u23aa\u23ac\n\u23aa\u23ad\nsubject to\np\u2211\nj=1\n|\u03b2j| \u2264 s\n(6.8)\nand\nminimize\u03b2\n\u23a7\n\u23aa\u23a8\n\u23aa\u23a9\nn\u2211\ni=1\n\u239b\n\u239dyi \u2212 \u03b20 \u2212\np\u2211\nj=1\n\u03b2jxij\n\u239e\n\u23a0\n2\u23ab\n\u23aa\u23ac\n\u23aa\u23ad\nsubject to\np\u2211\nj=1\n\u03b22\nj \u2264 s,\n(6.9)\nrespectively. In other words, for every value of\u03bb, there is somes such that\nthe Equations (6.7) and (6.8) will give the same lasso coe\ufb00icient estimates. Similarly, for every value of\u03bb there is a correspondings such that Equa-\ntions(6.5)and( 6.9)willgivethesameridgeregressioncoe\ufb00icientestimates. When p =2 , then (6.8) indicates that the lasso coe\ufb00icient estimates have\nthe smallest RSS out of all points that lie within the diamond defined by\n|\u03b21| + |\u03b22| \u2264 s. Similarly, the ridge regression estimates have the smallest\nRSS out of all points that lie within the circle defined by\u03b22\n1 + \u03b22\n2 \u2264 s. We can think of (6.8) as follows. When we perform the lasso we are trying\nto find the set of coe\ufb00icient estimates that lead to the smallest RSS, subject\nto the constraint that there is abudget s for how large\u2211 p\nj=1 |\u03b2j| can be. When s is extremely large, then this budget is not very restrictive, and so\nthe coe\ufb00icient estimates can be large. In fact, ifs is large enough that the\nleast squares solution falls within the budget, then (6.8) will simply yield\nthe least squares solution. In contrast, ifs is small, then\u2211 p\nj=1 |\u03b2j| must be\n\n244 6. Linear Model Selection and Regularization\nsmall in order to avoid violating the budget. Similarly, (6.9) indicates that\nwhen we perform ridge regression, we seek a set of coe\ufb00icient estimates\nsuch that the RSS is as small as possible, subject to the requirement that\u2211 p\nj=1 \u03b22\nj not exceed the budgets. The formulations (6.8) and (6.9) reveal a close connection between the\nlasso, ridge regression, and best subset selection. Consider the problem\nminimize\u03b2\n\u23a7\n\u23aa\u23a8\n\u23aa\u23a9\nn\u2211\ni=1\n\u239b\n\u239dyi \u2212 \u03b20 \u2212\np\u2211\nj=1\n\u03b2jxij\n\u239e\n\u23a0\n2\u23ab\n\u23aa\u23ac\n\u23aa\u23ad\nsubject to\np\u2211\nj=1\nI(\u03b2j \u0338= 0)\u2264 s. (6.10)\nHereI(\u03b2j \u0338= 0)is an indicator variable: it takes on a value of 1 if\u03b2j \u0338=0 , and\nequals zero otherwise. Then (6.10) amounts to finding a set of coe\ufb00icient\nestimates such that RSS is as small as possible, subject to the constraint\nthat no more thans coe\ufb00icients can be nonzero. The problem (6.10) is\nequivalent to best subset selection. Unfortunately, solving (6.10) is com-\nputationally infeasible whenp is large, since it requires considering all\n(p\ns\n)\nmodels containings predictors. Therefore, we can interpret ridge regression\nand the lasso as computationally feasible alternatives to best subset selec-\ntion that replace the intractable form of the budget in (6.10) with forms\nthat are much easier to solve. Of course, the lasso is much more closely\nrelated to best subset selection, since the lasso performs feature selection\nfor s su\ufb00iciently small in (6.8), while ridge regression does not. The Variable Selection Property of the Lasso\nWhy is it that the lasso, unlike ridge regression, results in coe\ufb00icient esti-\nmates that are exactly equal to zero? The formulations (6.8) and (6.9) can\nbe used to shed light on the issue. Figure6.7 illustrates the situation. The\nleast squares solution is marked as\u02c6\u03b2, while the blue diamond and circle\nrepresent the lasso and ridge regression constraints in (6.8) and (6.9), re-\nspectively. Ifs is su\ufb00iciently large, then the constraint regions will contain\n\u02c6\u03b2, and so the ridge regression and lasso estimates will be the same as the\nleast squares estimates. (Such a large value ofs corresponds to\u03bb =0 in\n(6.5) and (6.7).) However, in Figure6.7 the least squares estimates lie out-\nside of the diamond and the circle, and so the least squares estimates are\nnot the same as the lasso and ridge regression estimates. Each of the ellipses centered around\u02c6\u03b2 represents acontour: this meanscontour\nthat all of the points on a particular ellipse have the same RSS value. As\nthe ellipses expand away from the least squares coe\ufb00icient estimates, the\nRSS increases. Equations (6.8) and (6.9) indicate that the lasso and ridge\nregression coe\ufb00icient estimates are given by the first point at which an\nellipse contacts the constraint region. Since ridge regression has a circular\nconstraint with no sharp points, this intersection will not generally occur on\nan axis, and so the ridge regression coe\ufb00icient estimates will be exclusively\n\n6.2 Shrinkage Methods 245\nFIGURE 6.7. Contours of the error and constraint functions for the lasso\n(left) and ridge regression(right). The solid blue areas are the constraint regions,\n|\u03b21| +|\u03b22| \u2264 s and \u03b22\n1 +\u03b22\n2 \u2264 s, while the red ellipses are the contours of the RSS. non-zero. However, the lasso constraint hascornersat each of the axes, and\nso the ellipse will often intersect the constraint region at an axis. When this\noccurs, one of the coe\ufb00icients will equal zero.\n\nHow does your answer compare to the\nresults in (c)? (e) Now fit a lasso model to the simulated data, again usingX, X2,\n...,X 10 as predictors. Use cross-validation to select the optimal\nvalue of\u03bb. Create plots of the cross-validation error as a function\nof \u03bb. Report the resulting coe\ufb00icient estimates, and discuss the\nresults obtained. (f) Now generate a response vectorY according to the model\nY = \u03b20 + \u03b27X7 + \u03f5,\nand perform best subset selection and the lasso. Discuss the\nresults obtained.\n\nSee Section10.8 for a more detailed discussion. 6.1 Subset Selection 227\nIn the following sections we describe each of these approaches in greater de-\ntail, along with their advantages and disadvantages. Although this chapter\ndescribes extensions and modifications to the linear model for regression\nseen in Chapter3, the same concepts apply to other methods, such as the\nclassification models seen in Chapter4. 6.1 Subset Selection\nIn this section we consider some methods for selecting subsets of predictors. These include best subset and stepwise model selection procedures. 6.1.1 Best Subset Selection\nTo performbest subset selection, we fit a separate least squares regressionbest subset\nselectionfor each possible combination of thep predictors. That is, we fit allp models\nthat contain exactly one predictor, all\n(p\n2\n)\n= p(p\u22121)/2 models that contain\nexactly two predictors, and so forth. We then look at all of the resulting\nmodels, with the goal of identifying the one that isbest. The problem of selecting thebest modelfrom among the2p possibilities\nconsidered by best subset selection is not trivial. This is usually broken up\ninto two stages, as described in Algorithm6.1. Algorithm 6.1Best subset selection\n1. Let M0 denote thenull model, which contains no predictors. This\nmodel simply predicts the sample mean for each observation. 2. For k =1 , 2,...p :\n(a) Fit all\n(p\nk\n)\nmodels that contain exactlyk predictors. (b) Pick the best among these\n(p\nk\n)\nmodels, and call itMk. Herebest\nis defined as having the smallest RSS, or equivalently largestR2. 3. Select a single best model from amongM0,..., Mp using using the\nprediction error on a validation set,Cp (AIC), BIC, or adjustedR2. Or use the cross-validation method. In Algorithm6.1, Step 2 identifies the best model (on the training data)\nfor each subset size, in order to reduce the problem from one of2p possible\nmodels to one ofp +1 possible models. In Figure6.1, these models form\nthe lower frontier depicted in red. Now in order to select a single best model, we must simply choose among\nthese p +1 options. This task must be performed with care, because the\n\n228 6. Linear Model Selection and Regularization\nRSS of thesep +1 models decreases monotonically, and theR2 increases\nmonotonically, as the number of features included in the models increases. Therefore, if we use these statistics to select the best model, then we will\nalways end up with a model involving all of the variables. The problem is\nthat a low RSS or a highR2 indicates a model with a lowtraining error,\nwhereas we wish to choose a model that has a lowtest error. (As shown in\nChapter 2 in Figures2.9\u20132.11, training error tends to be quite a bit smaller\nthan test error, and a low training error by no means guarantees a low test\nerror.) Therefore, in Step 3, we use the error on a validation set,Cp, BIC, or\nadjusted R2 in order to select amongM0, M1,..., Mp. If cross-validation\nis used to select the best model, then Step 2 is repeated on each training\nfold, and the validation errors are averaged to select the best value ofk. Then the modelMk fit on the full training set is delivered for the chosen\nk. These approaches are discussed in Section6.1.3. An application of best subset selection is shown in Figure6.1. Each\nplotted point corresponds to a least squares regression model fit using a\ndifferent subset of the 10 predictors in theCredit data set, discussed in\nChapter 3. Here the variableregion is a three-level qualitative variable,\nand so is represented by two dummy variables, which are selected sepa-\nrately in this case. Hence, there are a total of 11 possible variables which\ncan be included in the model. We have plotted the RSS andR2 statistics\nfor each model, as a function of the number of variables. The red curves\nconnect the best models for each model size, according to RSS orR2. The\nfigure shows that, as expected, these quantities improve as the number of\nvariablesincreases;however,fromthethree-variablemodelon,thereislittle\nimprovement in RSS andR2 as a result of including additional predictors. Although we have presented best subset selection here for least squares\nregression, the same ideas apply to other types of models, such as logistic\nregression. In the case of logistic regression, instead of ordering models by\nRSS in Step 2 of Algorithm6.1, we instead use thedeviance, a measuredeviance\nthat plays the role of RSS for a broader class of models. The deviance is\nnegative two times the maximized log-likelihood; the smaller the deviance,\nthe better the fit. While best subset selection is a simple and conceptually appealing ap-\nproach, it suffers from computational limitations. The number of possible\nmodels that must be considered grows rapidly asp increases. In general,\nthere are2p models that involve subsets ofp predictors. So ifp = 10,\nthen there are approximately 1,000 possible models to be considered, and if\np = 20, then there are over one million possibilities! Consequently, best sub-\nset selection becomes computationally infeasible for values ofp greater than\naround 40, even with extremely fast modern computers. There are compu-\ntational shortcuts\u2014so called branch-and-bound techniques\u2014for eliminat-\ning some choices, but these have their limitations asp gets large. They also\nonly work for least squares linear regression. We present computationally\ne\ufb00icient alternatives to best subset selection next. 6.1 Subset Selection 229\n2 4 6 8 10\n2e+07 4e+07 6e+07 8e+07\nNumber of Predictors\nResidual Sum of Squares\n2 4 6 8 10\n0.0 0.2 0.4 0.6 0.8 1.0\nNumber of Predictors\nR2\nFIGURE 6.1.For each possible model containing a subset of the ten predictors\nin theCredit data set, the RSS andR2 are displayed. The red frontier tracks the\nbest model for a given number of predictors, according to RSS andR2. Though\nthe data set contains only ten predictors, thex-axis ranges from1 to 11, since one\nof the variables is categorical and takes on three values, leading to the creation of\ntwo dummy variables. 6.1.2 Stepwise Selection\nFor computational reasons, best subset selection cannot be applied with\nvery largep. Best subset selection may also suffer from statistical problems\nwhenp is large. The larger the search space, the higher the chance of finding\nmodels that look good on the training data, even though they might not\nhave any predictive power on future data. Thus an enormous search space\ncan lead to overfitting and high variance of the coe\ufb00icient estimates. For both of these reasons,stepwise methods, which explore a far more\nrestricted set of models, are attractive alternatives to best subset selection. Forward Stepwise Selection\nForward stepwise selectionis a computationally e\ufb00icient alternative to bestforward\nstepwise\nselection\nsubset selection. While the best subset selection procedure considers all\n2p possible models containing subsets of thep predictors, forward step-\nwise considers a much smaller set of models. Forward stepwise selection\nbegins with a model containing no predictors, and then adds predictors\nto the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatestadditional\nimprovement to the fit is added to the model. More formally, the forward\nstepwise selection procedure is given in Algorithm6.2. 230 6. Linear Model Selection and Regularization\nAlgorithm 6.2Forward stepwise selection\n1. Let M0 denote thenull model, which contains no predictors. 2. For k =0 ,...,p \u2212 1:\n(a) Consider allp \u2212 k models that augment the predictors inMk\nwith one additional predictor. (b) Choose thebest among thesep \u2212 k models, and call itMk+1. Here best is defined as having smallest RSS or highestR2. 3. Select a single best model from amongM0,..., Mp using the pre-\ndiction error on a validation set,Cp (AIC), BIC, or adjustedR2. Or\nuse the cross-validation method. Unlike best subset selection, which involved fitting2p models, forward\nstepwise selection involves fitting one null model, along withp \u2212 k models\nin thekth iteration, fork =0 ,...,p \u2212 1. This amounts to a total of1+\u2211 p\u22121\nk=0(p\u2212k)=1 + p(p+1)/2 models. This is a substantial difference: when\np = 20, best subset selection requires fitting1,048,576 models, whereas\nforward stepwise selection requires fitting only211 models.2\nIn Step 2(b) of Algorithm6.2, we must identify thebest model from\namong thosep\u2212k that augmentMk with one additional predictor. We can\ndo this by simply choosing the model with the lowest RSS or the highest\nR2.",
    "topic_5": "Why or why not? This is page 289\nPrinter: Opaque this\n7\nMoving Beyond Linearity\nSo far in this book, we have mostly focused on linear models. Linear models\nare relatively simple to describe and implement, and have advantages over\nother approaches in terms of interpretation and inference. However, stan-\ndard linear regression can have significant limitations in terms of predic-\ntive power. This is because the linearity assumption is almost always an\napproximation, and sometimes a poor one. In Chapter6 we see that we can\nimprove upon least squares using ridge regression, the lasso, principal com-\nponents regression, and other techniques. In that setting, the improvement\nis obtained by reducing the complexity of the linear model, and hence the\nvariance of the estimates. But we are still using a linear model, which can\nonly be improved so far! In this chapter we relax the linearity assumption\nwhile still attempting to maintain as much interpretability as possible. We\ndo this by examining very simple extensions of linear models like polyno-\nmial regression and step functions, as well as more sophisticated approaches\nsuch as splines, local regression, and generalized additive models. \u2022 Polynomial regressionextends the linear model by adding extra pre-\ndictors, obtained by raising each of the original predictors to a power. For example, acubic regression uses three variables,X, X2, andX3,\nas predictors. This approach provides a simple way to provide a non-\nlinear fit to data. \u2022 Step functionscut the range of a variable intoK distinct regions in\norder to produce a qualitative variable. This has the effect of fitting\na piecewise constant function. 290 7. Moving Beyond Linearity\n\u2022 Regression splinesare more flexible than polynomials and step func-\ntions, and in fact are an extension of the two. They involve dividing\nthe range ofX into K distinct regions. Within each region, a poly-\nnomial function is fit to the data. However, these polynomials are\nconstrained so that they join smoothly at the region boundaries, or\nknots. Provided that the interval is divided into enough regions, this\ncan produce an extremely flexible fit. \u2022 Smoothing splinesare similar to regression splines, but arise in a\nslightly different situation. Smoothing splines result from minimizing\na residual sum of squares criterion subject to a smoothness penalty. \u2022 Local regressionis similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very\nsmooth way. \u2022 Generalized additive modelsallow us to extend the methods above to\ndeal with multiple predictors. In Sections7.1\u20137.6, we present a number of approaches for modeling the\nrelationship between a responseY and a single predictorX in a flexible\nway. In Section7.7, we show that these approaches can be seamlessly in-\ntegrated in order to model a responseY as a function of several predictors\nX1,...,X p. 7.1 Polynomial Regression\nHistorically, the standard way to extend linear regression to settings in\nwhich the relationship between the predictors and the response is non-\nlinear has been to replace the standard linear model\nyi = \u03b20 + \u03b21xi + \u03f5i\nwith a polynomial function\nyi = \u03b20 + \u03b21xi + \u03b22x2\ni + \u03b23x3\ni + \u00b7\u00b7\u00b7 + \u03b2dxd\ni + \u03f5i, (7.1)\nwhere \u03f5i is the error term. This approach is known aspolynomial regression, polynomial\nregressionand in fact we saw an example of this method in Section3.3.2. For large\nenough degreed, a polynomial regression allows us to produce an extremely\nnon-linearcurve.Noticethatthecoe\ufb00icientsin( 7.1)canbeeasilyestimated\nusing least squares linear regression because this is just a standard linear\nmodel with predictorsxi,x 2\ni ,x 3\ni ,...,x d\ni . Generally speaking, it is unusual\nto used greater than3 or 4 because for large values ofd, the polynomial\ncurve can become overly flexible and can take on some very strange shapes. This is especially true near the boundary of theX variable. 7.1 Polynomial Regression 291\n20 30 40 50 60 70 80\n50 100 150 200 250 300\nAge\nWage\nDegree\u22124 Polynomial\n20 30 40 50 60 70 80\n0.00 0.05 0.10 0.15 0.20\nAge\n|| | | | | |||| ||| | | ||| ||| | |||\n|\n|||| |||| | | ||| || | | | ||\n|\n||| | |\n|\n||| || ||| | ||| ||||| |||| | |\n|\n| ||| | ||||| | | | ||||| ||| ||| |||| | ||||| ||| | | | |\n|\n||||| ||| || | |||| || ||| ||| ||| ||| ||| | ||| || | | |||| || || | | |||| |||| | | | ||| | |||||| | || | ||||| ||| ||| |\n|\n|||| | | | | |||| ||| | ||| || || ||| |||\n|\n|| ||||||| | | || | | ||| ||| | ||||| | | | | ||| || | ||| || || |||||| | | | | || |||| ||| ||| | | ||| || ||||| ||| | | ||||| ||| ||||||| | |||| | |||| ||\n|\n|| | || |||| |||| ||||| | | |\n|\n|||\n|\n|||||| || |||| ||| ||| |||| ||| | |||| | || ||| | |||| || ||||| || |||| | | |||||| | | | |||\n|\n|| |||| || | | ||\n|\n|| |||| || |||| | | ||| || ||||| | || ||||||\n|\n|| | ||| | | | ||||\n|\n||| | | | ||| ||\n|\n|||| |||||| ||||| | | ||\n|\n||| ||| ||| ||| | | | | | ||||| | ||| |\n|\n|| ||| || | | |||| |||\n|\n||| | | | || | |||||| | |||| | || ||| || ||||||| | || | || ||| | | ||| | ||| | |||| |||\n|\n||||| ||| || | |||\n|\n|| | | ||| | || ||| || | ||| || | |||| | | |||| ||| ||||| ||| |\n|\n||| ||||| ||| | ||| | ||||||| | | ||\n|\n|||||| || |||| | ||| |||| |||| |||||\n|\n|||| |||| ||| | | || ||||| | || ||| |||||\n|\n|||||| | ||| |||| | ||| | |||||||||| | | | | ||| |\n|\n|||| ||||| ||| | ||| ||| | | ||| ||| ||| || ||| || | ||| | | | | |||| || | ||| | | | ||| ||| || | |||\n|\n|||| | | ||||| |||\n|\n|| | | ||| | ||| | ||| | |||||| ||||| | ||| || | ||| |\n|\n|| | |||||| | | || ||| | | || ||||| ||||||| |||| | | || ||||||| ||||| | | |||| |\n|\n||||| |||||| | | ||| || ||||| ||| || | ||||| | | | | | || || ||| ||||| | | || | ||| ||| || ||||\n|\n||| |||||| | | ||| |||| | ||| | | |||\n|\n|| | || ||| | || |||\n|\n||| |||||| | ||| ||| | ||| | | | | ||| ||| |||| | | ||| | ||| ||| ||| | || | |||| ||| ||| ||||| || | ||| | | | ||| ||| | ||| | || || |||| |||| |\n|\n||| | ||| | || | ||| | ||| ||| |||| ||| ||| ||||| ||| ||\n|\n|||| |||| |\n||\n|| ||| ||| | |\n|\n|||||| || |||||| |\n|\n|||| ||||||| | |||\n|\n|||| || || | | ||| | | | | ||| | || ||| ||| | || | ||| | | |||||| |\n|\n|| | ||||| | | | | |||||| | ||| |||||| ||| ||| ||| | | | | ||| ||| | ||||||| | || ||| ||| || ||| | | |||| | ||| | ||| |||| | ||| | | || || ||||| || ||||| ||\n|\n|||||| |||| |||||| |||| | |||| || ||||||| | | ||| | | || ||| | | | |||| |||| | ||| || || |||| ||||| || | |||| |||||| ||| | |||| || || |\n|\n|| ||| ||||| || | ||| | ||| | | || |||| |||| |||| | | | ||||| ||| | | |\n|\n||| ||| |||| |||||| | ||| |||| | ||| | | ||||| ||||| |||| ||||| |\n|\n|| | ||| ||| ||||| | ||||| || ||| | |||| ||| | || |||| ||| || | ||| | | |||| | ||| ||| ||| |\n|\n|| | | | ||| |||| ||| | || ||||| | | | | ||||| || ||| |||||| ||| || | |||| ||||| || ||||| | ||||| | | |\n|\n||| |\n|\n|||| | | | ||||| | | | |||| | | ||| |||| | | |||| || | ||| ||| | ||| |\n|\n|||| | | | | ||||| | | ||| || | ||| ||| | ||||| | ||| | || |||||| | | ||| || |||| || | || ||||| |||| ||| | ||||| ||| |||| ||||| | ||| || |||\n|\n|||| | | || ||| || ||||| | | | |||| | | | ||| | ||\n|\n|| ||| |\n|\n||| | | ||| | ||| ||||| || ||| |||| | ||| | | |||||| | | || | |||| | ||| ||||| |||| | |||| | | ||| || | || | | | ||| ||\n|\n|||||\n|\n||| | ||| ||||||| || | | ||| || | |||| |||\n|\n|| |||| ||| | ||| | || | | | ||| | | |||| ||| | | | | ||| || | | | | ||| | ||| ||| |||| | ||| | | ||| |||||||| | | | |||| ||| | | ||||||| | | | | ||| | | || |||||| | |||\n|\n|||| | ||\n|\n|| ||| || | |||||\n|\n|| | ||||||| || | ||| | | || | | ||||| | ||| | ||| || || || ||| |||| | ||| | | |||| | ||| | |\n|\n||| | | |||| | | | ||||\n||\n|| | | |||| |||\n|\n|| ||||| | ||| || |\n|\n|| ||| ||||| | || | | | |||| || ||| || |||| ||| | |||| || ||| | |||||| ||| | | ||| |||| |||||| | | | | |\n|\n|| |||\n|\n||| |||| |||| | | |||||| || |||| | || || | ||||||| | || | | ||\n|\n|||||| |\n||\n||||| | || ||| ||| | | |||| | || | ||||| || || ||||| | ||| | ||| | | || | || ||| |||| | | |||||| | | | ||| ||| ||| ||| || | || ||| ||| | || ||| | ||| | ||| | ||| || ||| ||| || || ||| |||| | |||||| |||||| |||\n|\n|| ||\n|\n|||\n|\n||\n|\n|\n|\n|| | ||| || | |\n|\n|||| ||\n|\n||||| | ||| |||| | | ||||| | | |||| | ||| ||||| || |||| | |||| ||| | || ||| |||| | | || ||| | || | | | | ||| ||| | | ||\n|\n||||| || | ||| || | || | |||| | ||| |||| ||| |||| |\n|\n|| ||| | || ||| | ||| | || ||| || | | |||| || | |||| ||| | | ||| |\n|\n|| ||| | || | | ||\n|\n|| ||| | | ||| ||| ||| | ||| || | ||||| |||\n|\n||| || ||||| | |||| |||| || | ||| | |||| ||| |||| || ||| || ||| ||||| | |||||||| |||||| | | ||| | |\nPr(Wage>250|Age)\nFIGURE 7.1.The Wage data. Left: The solid blue curve is a degree-4 polynomial\nof wage (in thousands of dollars) as a function ofage, fit by least squares.\n\n324 7. Moving Beyond Linearity\n(b) Fit a step function to predictwage using age, and perform cross-\nvalidation to choose the optimal number of cuts. Make a plot of\nthe fit obtained. 7. The Wage data set contains a number of other features not explored\nin this chapter, such as marital status (maritl), job class (jobclass),\nand others. Explore the relationships between some of these other\npredictors andwage, and use non-linear fitting techniques in order to\nfit flexible models to the data. Create plots of the results obtained,\nand write a summary of your findings. 8. Fit some of the non-linear models investigated in this chapter to the\nAuto data set. Is there evidence for non-linear relationships in this\ndata set? Create some informative plots to justify your answer.\n\nIs the relationship linear? In Section3.3.3, we saw that residual plots can be used in order to\nidentify non-linearity. If the relationships are linear, then the residual\nplots should display no pattern. In the case of theAdvertising data,\n\n3.5 Comparison of Linear Regression withK-Nearest Neighbors 105\nwe observe a non-linear effect in Figure3.5, though this effect could\nalso be observed in a residual plot. In Section3.3.2, we discussed the\ninclusion of transformations of the predictors in the linear regression\nmodel in order to accommodate non-linear relationships. 7. Is there synergy among the advertising media? The standard linear regression model assumes an additive relation-\nship between the predictors and the response. An additive model\nis easy to interpret because the association between each predictor\nand the response is unrelated to the values of the other predictors. However, the additive assumption may be unrealistic for certain data\nsets. In Section3.3.2, we showed how to include an interaction term\nin the regression model in order to accommodate non-additive rela-\ntionships. A smallp-value associated with the interaction term indi-\ncates the presence of such relationships. Figure3.5 suggested that the\nAdvertising data may not be additive. Including an interaction term\nin the model results in a substantial increase inR2, from around 90%\nto almost 97%. 3.5 Comparison of Linear Regression\nwith K-Nearest Neighbors\nAs discussed in Chapter2, linear regression is an example of aparametric\napproach because it assumes a linear functional form forf(X). Parametric\nmethods have several advantages.\n\nFor example,linear modelsallow for relatively simple and in-linear model\nterpretable inference, but may not yield as accurate predictions as some\nother approaches. In contrast, some of the highly non-linear approaches\nthat we discuss in the later chapters of this book can potentially provide\nquite accurate predictions forY , but this comes at the expense of a less\ninterpretable model for which inference is more challenging. 2.1 What Is Statistical Learning? 21\n2.1.2 How Do We Estimatef? Throughout this book, we explore many linear and non-linear approaches\nfor estimatingf. However, these methods generally share certain charac-\nteristics.",
    "topic_6": "This is page 327\nPrinter: Opaque this\n8\nTree-Based Methods\nIn this chapter, we describetree-basedmethods for regression and classifi-\ncation. These involvestratifying or segmenting the predictor space into a\nnumber of simple regions. In order to make a prediction for a given ob-\nservation, we typically use the mean or the mode response value for the\ntraining observations in the region to which it belongs. Since the set of\nsplitting rules used to segment the predictor space can be summarized in\na tree, these types of approaches are known asdecision treemethods. decision tree\nTree-based methods are simple and useful for interpretation. However,\nthey typically are not competitive with the best supervised learning ap-\nproaches, such as those seen in Chapters6 and 7, in terms of prediction\naccuracy. Hence in this chapter we also introducebagging, random forests,\nboosting, andBayesian additive regression trees. Each of these approaches\ninvolves producing multiple trees which are then combined to yield a single\nconsensus prediction. We will see that combining a large number of trees\ncan often result in dramatic improvements in prediction accuracy, at the\nexpense of some loss in interpretation. 8.1 The Basics of Decision Trees\nDecision trees can be applied to both regression and classification problems. We first consider regression problems, and then move on to classification. 328 8. Tree-Based Methods\n|Y ears < 4.5\nHits < 117.55.11\n6.00 6.74\nFIGURE 8.1.For theHitters data, a regression tree for predicting the log\nsalary of a baseball player, based on the number of years that he has played in\nthe major leagues and the number of hits that he made in the previous year. At a\ngiven internal node, the label (of the formXj <t k) indicates the left-hand branch\nemanating from that split, and the right-hand branch corresponds toXj \u2265 tk. For instance, the split at the top of the tree results in two large branches. The\nleft-hand branch corresponds toYears<4.5, and the right-hand branch corresponds\nto Years>=4.5. The tree has two internal nodes and three terminal nodes, or\nleaves. The number in each leaf is the mean of the response for the observations\nthat fall there. 8.1.1 Regression Trees\nIn order to motivateregression trees, we begin with a simple example.regression\ntree\nPredicting Baseball Players\u2019 Salaries Using Regression Trees\nWe use theHitters data set to predict a baseball player\u2019sSalary based on\nYears (the number of years that he has played in the major leagues) and\nHits(thenumberofhitsthathemadeinthepreviousyear).Wefirstremove\nobservations that are missingSalary values, and log-transformSalary so\nthat its distribution has more of a typical bell-shape. (Recall thatSalary\nis measured in thousands of dollars.)\nFigure 8.1 shows a regression tree fit to this data. It consists of a series\nof splitting rules, starting at the top of the tree. The top split assigns\nobservations havingYears<4.5 to the left branch.1 The predicted salary\nfor these players is given by the mean response value for the players in\nthe data set withYears<4.5. For such players, the mean log salary is5.107,\nand so we make a prediction ofe5.107 thousands of dollars, i.e. $165,174, for\n1Both Years and Hits are integers in these data; the function used to fit this tree\nlabels the splits at the midpoint between two adjacent values. 8.1 The Basics of Decision Trees 329\nY ears\nHits\n1\n117.5\n238\n1 4.5 24\nR1\nR3\nR2\nFIGURE 8.2.The three-region partition for theHitters data set from the\nregression tree illustrated in Figure8.1. these players. Players withYears>=4.5 are assigned to the right branch, and\nthen that group is further subdivided byHits. Overall, the tree stratifies\nor segments the players into three regions of predictor space: players who\nhave played for four or fewer years, players who have played for five or more\nyears and who made fewer than 118 hits last year, and players who have\nplayed for five or more years and who made at least 118 hits last year. These\nthree regions can be written asR1 ={X | Years<4.5}, R2 ={X | Years>=4.5,\nHits<117.5}, andR3 ={X | Years>=4.5, Hits>=117.5}.\n\nTree-Based Methods\nOne of the most attractive properties of trees is that they can be\ngraphically displayed. We use theplot() function to display the tree struc-\nture, and thetext() function to display the node labels. The argument\npretty = 0 instructs R to include the category names for any qualitative\npredictors, rather than simply displaying a letter for each category. > plot(tree.carseats)\n> text(tree.carseats, pretty =0 )\nThe most important indicator ofSales appears to be shelving location,\nsince the first branch differentiatesGood locations fromBad and Medium\nlocations. If we just type the name of the tree object,R prints output corresponding\nto each branch of the tree.R displays the split criterion (e.g.Price < 92.5),\nthe number of observations in that branch, the deviance, the overall pre-\ndiction for the branch (Yes or No), and the fraction of observations in that\nbranch that take on values ofYes and No. Branches that lead to terminal\nnodes are indicated using asterisks. >t r e e . c a r s e a t s\nnode), split, n, deviance, yval, (yprob)\n*d e n o t e st e r m i n a ln o d e\n1) root 400 541.5 No ( 0.590 0.410 )\n2) ShelveLoc: Bad,Medium 315 390.6 No ( 0.689 0.311 )\n4) Price < 92.5 46 56.53 Yes ( 0.304 0.696 )\n8) Income < 57 10 12.22 No ( 0.700 0.300 )\nIn order to properly evaluate the performance of a classification tree on\nthese data, we must estimate the test error rather than simply computing\nthe training error. We split the observations into a training set and a test\nset,buildthetreeusingthetrainingset,andevaluateitsperformanceonthe\ntest data. Thepredict() function can be used for this purpose. In the case\nof a classification tree, the argumenttype = \"class\" instructs R to return\nthe actual class prediction. This approach leads to correct predictions for\naround 77 %of the locations in the test data set. > set.seed (2)\n>t r a i n< -sample (1:nrow(Carseats), 200)\n>C a r s e a t s . t e s t< -C a r s e a t s [ - t r a i n ,]\n>H i g h . t e s t< -H i g h [ - t r a i n ]\n>t r e e . c a r s e a t s< -tree(High \u223c .-S a l e s ,C a r s e a t s ,\nsubset =t r a i n )\n>t r e e . p r e d< -predict (tree.carseats, Carseats.test,\ntype = \"class \")\n> table (tree.pred, High.test)\nHigh.test\ntree.pred No Yes\nNo 104 33\nYes 13 50\n>( 1 0 4+5 0 )/2 0 0\n[1] 0.77\n\n8.3 Lab: Decision Trees 355\n(If you re-run thepredict() function then you might get slightly different\nresults, due to \u201cties\u201d: for instance, this can happen when the training ob-\nservations corresponding to a terminal node are evenly split betweenYes\nand No response values.)\nNext, we consider whether pruning the tree might lead to improved\nresults. The functioncv.tree() performs cross-validation in order tocv.tree()\ndetermine the optimal level of tree complexity; cost complexity pruning\nis used in order to select a sequence of trees for consideration. We use\nthe argumentFUN = prune.misclass in order to indicate that we want the\nclassification error rate to guide the cross-validation and pruning process,\nrather than the default for thecv.tree() function, which is deviance. The\ncv.tree() function reports the number of terminal nodes of each tree con-\nsidered (size) as well as the corresponding error rate and the value of the\ncost-complexity parameter used (k, which corresponds to\u03b1 in (8.4)). > set.seed (7)\n>c v . c a r s e a t s< -cv.tree(tree.carseats, FUN = prune.misclass)\n> names (cv.carseats)\n[1] \"size\" \"dev\" \"k\" \"method \"\n> cv.carseats\n$size\n[1] 21 19 14 9 8 5 3 2 1\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n$k\n[1] -Inf 0.0 1.0 1.4 2.0 3.0 4.0 9.0 18.0\n$method\n[1] \"misclass\"\nattr(,\"class\")\n[1] \"prune\" \"tree.sequence\"\nDespite its name,dev corresponds to the number of cross-validation errors. The tree with 9 terminal nodes results in only 74 cross-validation errors. We plot the error rate as a function of bothsize and k. > par(mfrow = c(1, 2))\n> plot(cv.carseats$size, cv.carseats$dev ,t y p e= \"b\")\n> plot(cv.carseats$k, cv.carseats$dev ,t y p e= \"b\")\nWe now apply theprune.misclass() function in order to prune the tree toprune.misclass()\nobtain the nine-node tree. >p r u n e . c a r s e a t s< -prune .misclass (tree.carseats, best = 9)\n> plot(prune.carseats)\n> text(prune.carseats, pretty =0 )\nHow well does this pruned tree perform on the test data set? Once again,\nwe apply thepredict() function. >t r e e . p r e d< -predict (prune.carseats, Carseats.test,\ntype = \"class \")\n> table (tree.pred, High.test)\nHigh.test\n\n356 8. Tree-Based Methods\ntree.pred No Yes\nNo 97 25\nYes 20 58\n>( 9 7+5 8 )/2 0 0\n[1] 0.775\nNow 77.5% of the test observations are correctly classified, so not only has\nthe pruning process produced a more interpretable tree, but it has also\nslightly improved the classification accuracy. If we increase the value ofbest, we obtain a larger pruned tree with lower\nclassification accuracy:\n>p r u n e . c a r s e a t s< -prune .misclass (tree.carseats, best = 14)\n> plot(prune.carseats)\n> text(prune.carseats, pretty =0 )\n>t r e e . p r e d< -predict (prune.carseats, Carseats.test,\ntype = \"class \")\n> table (tree.pred, High.test)\nHigh.test\ntree.pred No Yes\nNo 102 31\nYes 15 52\n>( 1 0 2+5 2 )/2 0 0\n[1] 0.77\n8.3.2 Fitting Regression Trees\nHere we fit a regression tree to theBoston data set. First, we create a\ntraining set, and fit the tree to the training data. > set.seed (1)\n>t r a i n< -sample (1:nrow(Boston), nrow(Boston) / 2)\n>t r e e . b o s t o n< -tree(medv \u223c ., Boston, subset =t r a i n )\n> summary (tree.boston)\nRegression tree:\ntree( formula = medv \u223c ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\" \"lstat\" \"crim\" \"age\"\nNumber of terminal nodes: 7\nResidual mean deviance: 10.4 = 2550 / 246\nDistribution of residuals:\nMin. 1st Qu.\n\nDescribe the results obtained. 8. In the lab, a classification tree was applied to theCarseats data set af-\nter convertingSales into a qualitative response variable. Now we will\nseek to predictSales using regression trees and related approaches,\ntreating the response as a quantitative variable. (a) Split the data set into a training set and a test set. (b) Fit a regression tree to the training set. Plot the tree, and inter-\npret the results. What test MSE do you obtain? (c) Use cross-validation in order to determine the optimal level of\ntree complexity. Does pruning the tree improve the test MSE? (d) Use the bagging approach in order to analyze this data. What\ntest MSE do you obtain? Use theimportance() function to de-\ntermine which variables are most important. (e) Use random forests to analyze this data. What test MSE do you\nobtain? Use theimportance() function to determine which vari-\nablesaremostimportant.Describetheeffectof m,thenumberof\nvariables considered at each split, on the error rate\nobtained. (f) Now analyze the data using BART, and report your results.\n\n4. Compute the mean afterL burn-in samples,\n\u02c6f(x)= 1\nB \u2212 L\nB\u2211\nb=L+1\n\u02c6fb(x). WhenweapplyBART,wemustselectthenumberoftrees K,thenumber\nof iterationsB, and the number of burn-in iterationsL. We typically choose\nlargevaluesfor B andK,andamoderatevaluefor L:forinstance, K = 200,\nB =1 ,000, andL = 100 is a reasonable choice. BART has been shown to\nhave very impressive out-of-box performance \u2014 that is, it performs well\nwith minimal tuning. 8.2.5 Summary of Tree Ensemble Methods\nTrees are an attractive choice of weak learner for an ensemble method\nfor a number of reasons, including their flexibility and ability to handle\npredictors of mixed types (i.e. qualitative as well as quantitative). We have\nnow seen four approaches for fitting an ensemble of trees: bagging, random\nforests, boosting, and BART. \u2022 In bagging, the trees are grown independently on random samples of\nthe observations. Consequently, the trees tend to be quite similar to\neach other. Thus, bagging can get caught in local optima and can fail\nto thoroughly explore the model space. 352 8. Tree-Based Methods\n5 10 50100 500 5000\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of Iterations\nError\nBART Training ErrorBART Test ErrorBoosting Training ErrorBoosting Test Error\nFIGURE 8.13.BART and boosting results for theHeart data. Both training\nand test errors are displayed. After a burn-in period of100 iterations (shown in\ngray), the error rates for BART settle down. Boosting begins to overfit after a\nfew hundred iterations. \u2022 In random forests, the trees are once again grown independently on\nrandom samples of the observations. However, each split on each tree\nis performed using a random subset of the features, thereby decorre-\nlating the trees, and leading to a more thorough exploration of model\nspace relative to bagging. \u2022 In boosting, we only use the original data, and do not draw any ran-\ndom samples. The trees are grown successively, using a \u201cslow\u201d learn-\ning approach: each new tree is fit to the signal that is left over from\nthe earlier trees, and shrunken down before it is used. \u2022 In BART, we once again only make use of the original data, and we\ngrow the trees successively. However, each tree is perturbed in order\nto avoid local minima and achieve a more thorough exploration of\nthe model space. 8.3 Lab: Decision Trees 353\n8.3 Lab: Decision Trees\n8.3.1 Fitting Classification Trees\nThe tree library is used to construct classification and regression trees. > library (tree)\nWe first use classification trees to analyze theCarseats data set. In these\ndata, Sales is a continuous variable, and so we begin by recoding it as a\nbinary variable. We use theifelse() function to create a variable, calledifelse()\nHigh, which takes on a value ofYes if theSales variable exceeds8, and\ntakes on a value ofNo otherwise. > library (ISLR2)\n> attach (Carseats)\n>H i g h< -factor (ifelse (Sales <= 8, \"No\", \"Yes\"))\nFinally, we use thedata.frame() function to mergeHigh with the rest of\nthe Carseats data. >C a r s e a t s< -data.frame (Carseats, High)\nWe now use thetree() function to fit a classification tree in order to predicttree()\nHigh using all variables butSales. The syntax of thetree() function is quite\nsimilar to that of thelm() function. >t r e e . c a r s e a t s< -tree(High \u223c .-S a l e s ,C a r s e a t s )\nThe summary() function lists the variables that are used as internal nodes\nin the tree, the number of terminal nodes, and the (training) error rate. > summary (tree.carseats)\nClassification tree:\ntree( formula = High \u223c .-S a l e s ,d a t a=C a r s e a t s )\nVariables actually used in tree construction:\n[1] \"ShelveLoc\" \"Price\" \"Income\" \"CompPrice\"\n[5] \"Population\" \"Advertising\" \"Age\" \"US\"\nNumber of terminal nodes: 27\nResidual mean deviance: 0.4575 = 170.7 / 373\nMisclassification error rate: 0.09 = 36 / 400\nWe see that the training error rate is9%. For classification trees, the de-\nviance reported in the output ofsummary() is given by\n\u22122\n\u2211\nm\n\u2211\nk\nnmk log \u02c6pmk,\nwhere nmk is the number of observations in themth terminal node that\nbelong to thekth class. This is closely related to the entropy, defined in\n(8.7). A small deviance indicates a tree that provides a good fit to the\n(training) data. Theresidual mean deviancereported is simply the deviance\ndivided byn \u2212 |T0|, which in this case is400 \u2212 27 = 373. 354 8.",
    "topic_7": "Which\nof these approaches yields the best performance? This is page 366\nPrinter: Opaque this\n\nThis is page 367\nPrinter: Opaque this\n9\nSupport Vector Machines\nIn this chapter, we discuss thesupport vector machine(SVM), an approach\nfor classification that was developed in the computer science community in\nthe 1990s and that has grown in popularity since then. SVMs have been\nshown to perform well in a variety of settings, and are often considered one\nof the best \u201cout of the box\u201d classifiers. The support vector machine is a generalization of a simple and intu-\nitive classifier called themaximal margin classifier, which we introduce in\nSection 9.1. Though it is elegant and simple, we will see that this classifier\nunfortunately cannot be applied to most data sets, since it requires that\nthe classes be separable by a linear boundary. In Section9.2, we introduce\nthe support vector classifier, an extension of the maximal margin classifier\nthat can be applied in a broader range of cases. Section9.3 introduces the\nsupport vector machine, which is a further extension of the support vec-\ntor classifier in order to accommodate non-linear class boundaries. Support\nvector machines are intended for the binary classification setting in which\nthere are two classes; in Section9.4 we discuss extensions of support vector\nmachines to the case of more than two classes. In Section9.5 we discuss\nthe close connections between support vector machines and other statistical\nmethods such as logistic regression. People often loosely refer to the maximal margin classifier, the support\nvector classifier, and the support vector machine as \u201csupport vector\nmachines\u201d. To avoid confusion, we will carefully distinguish between these\nthree notions in this chapter. 368 9. Support Vector Machines\n9.1 Maximal Margin Classifier\nIn this section, we define a hyperplane and introduce the concept of an\noptimal separating hyperplane. 9.1.1 What Is a Hyperplane? In a p-dimensional space, ahyperplane is a flat a\ufb00ine subspace ofhyperplane\ndimension p \u2212 1.1 For instance, in two dimensions, a hyperplane is a flat\none-dimensional subspace\u2014in other words, a line. In three dimensions, a\nhyperplane is a flat two-dimensional subspace\u2014that is, a plane. Inp> 3\ndimensions, it can be hard to visualize a hyperplane, but the notion of a\n(p \u2212 1)-dimensional flat subspace still applies. The mathematical definition of a hyperplane is quite simple. In two di-\nmensions, a hyperplane is defined by the equation\n\u03b20 + \u03b21X1 + \u03b22X2 =0 (9.1)\nfor parameters\u03b20, \u03b21, and\u03b22. When we say that (9.1) \u201cdefines\u201d the hyper-\nplane, we mean that anyX =( X1,X 2)T for which (9.1) holds is a point\non the hyperplane. Note that (9.1) is simply the equation of a line, since\nindeed in two dimensions a hyperplane is a line. Equation 9.1 can be easily extended to thep-dimensional setting:\n\u03b20 + \u03b21X1 + \u03b22X2 + \u00b7\u00b7\u00b7 + \u03b2pXp =0 (9.2)\ndefines ap-dimensional hyperplane, again in the sense that if a pointX =\n(X1,X 2,...,X p)T in p-dimensional space (i.e. a vector of lengthp) satisfies\n(9.2), thenX lies on the hyperplane. Now, suppose thatX does not satisfy (9.2); rather,\n\u03b20 + \u03b21X1 + \u03b22X2 + \u00b7\u00b7\u00b7 + \u03b2pXp > 0. (9.3)\nThen this tells us thatX lies to one side of the hyperplane. On the other\nhand, if\n\u03b20 + \u03b21X1 + \u03b22X2 + \u00b7\u00b7\u00b7 + \u03b2pXp < 0, (9.4)\nthen X lies on the other side of the hyperplane. So we can think of the\nhyperplane as dividingp-dimensional space into two halves. One can easily\ndetermine on which side of the hyperplane a point lies by simply calculating\nthe sign of the left-hand side of (9.2). A hyperplane in two-dimensional\nspace is shown in Figure9.1. 1The worda\ufb00ine indicates that the subspace need not pass through the origin. 9.1 Maximal Margin Classifier 369\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\nX1\nX2\nFIGURE 9.1.The hyperplane1+2 X1 +3 X2 =0 is shown. The blue region is\nthe set of points for which1+2 X1 +3 X2 > 0, and the purple region is the set of\npoints for which1+2 X1 +3 X2 < 0. 9.1.2 Classification Using a Separating Hyperplane\nNow suppose that we have ann \u00d7 p data matrixX that consists ofn\ntraining observations inp-dimensional space,\nx1 =\n\u239b\n\u239c\u239d\nx11\n... x1p\n\u239e\n\u239f\u23a0,...,x n =\n\u239b\n\u239c\u239d\nxn1\n... xnp\n\u239e\n\u239f\u23a0, (9.5)\nand that these observations fall into two classes\u2014that is,y1,...,y n \u2208\n{\u22121, 1} where \u22121 represents one class and1 the other class. We also have a\ntest observation, ap-vector of observed featuresx\u2217 =\n(x\u2217\n1 ... x \u2217\np\n)T\n. Our\ngoal is to develop a classifier based on the training data that will correctly\nclassify the test observation using its feature measurements. We have seen\na number of approaches for this task, such as linear discriminant analysis\nand logistic regression in Chapter4, and classification trees, bagging, and\nboosting in Chapter8. We will now see a new approach that is based upon\nthe concept of aseparating hyperplane. separating\nhyperplaneSuppose that it is possible to construct a hyperplane that separates the\ntraining observations perfectly according to their class labels. Examples\nof three suchseparating hyperplanesare shown in the left-hand panel of\nFigure 9.2. We can label the observations from the blue class asyi =1 and\nthose from the purple class asyi = \u22121. Then a separating hyperplane has\nthe property that\n\u03b20 + \u03b21xi1 + \u03b22xi2 + \u00b7\u00b7\u00b7 + \u03b2pxip > 0 ifyi =1 , (9.6)\n\n370 9. Support Vector Machines\n\u221210 1 2 3\n\u221210 1 2 3\n\u221210 1 2 3\n\u221210 1 2 3\nX1X1\nX2\nX2\nFIGURE 9.2.Left: There are two classes of observations, shown in blue and\nin purple, each of which has measurements on two variables. Three separating\nhyperplanes, out of many possible, are shown in black.Right: A separating hy-\nperplane is shown in black. The blue and purple grid indicates the decision rule\nmade by a classifier based on this separating hyperplane: a test observation that\nfalls in the blue portion of the grid will be assigned to the blue class, and a test\nobservation that falls into the purple portion of the grid will be assigned to the\npurple class. and\n\u03b20 + \u03b21xi1 + \u03b22xi2 + \u00b7\u00b7\u00b7 + \u03b2pxip < 0 ifyi = \u22121. (9.7)\nEquivalently, a separating hyperplane has the property that\nyi(\u03b20 + \u03b21xi1 + \u03b22xi2 + \u00b7\u00b7\u00b7 + \u03b2pxip) > 0 (9.8)\nfor alli =1 ,...,n . Ifaseparatinghyperplaneexists,wecanuseittoconstructaverynatural\nclassifier: a test observation is assigned a class depending on which side of\nthe hyperplane it is located. The right-hand panel of Figure9.2 shows\nan example of such a classifier. That is, we classify the test observationx\u2217\nbasedonthesignof f(x\u2217)= \u03b20+\u03b21x\u2217\n1+\u03b22x\u2217\n2+\u00b7\u00b7\u00b7 +\u03b2px\u2217\np.If f(x\u2217) ispositive,\nthen we assign the test observation to class1, and iff(x\u2217) is negative, then\nwe assign it to class\u22121. We can also make use of themagnitude of f(x\u2217). If\nf(x\u2217) is far from zero, then this means thatx\u2217 lies far from the hyperplane,\nand so we can be confident about our class assignment forx\u2217. On the other\nhand, iff(x\u2217) is close to zero, thenx\u2217 is located near the hyperplane, and so\nwe are less certain about the class assignment forx\u2217. Not surprisingly, and\nas we see in Figure9.2, a classifier that is based on a separating hyperplane\nleads to a linear decision boundary. 9.1 Maximal Margin Classifier 371\n9.1.3 The Maximal Margin Classifier\nIn general, if our data can be perfectly separated using a hyperplane, then\nthere will in fact exist an infinite number of such hyperplanes. This is\nbecauseagivenseparatinghyperplanecanusuallybeshiftedatinybitupor\ndown, or rotated, without coming into contact with any of the observations. Three possible separating hyperplanes are shown in the left-hand panel\nof Figure9.2. In order to construct a classifier based upon a separating\nhyperplane, we must have a reasonable way to decide which of the infinite\npossible separating hyperplanes to use. A natural choice is themaximal margin hyperplane(also known as themaximal\nmargin\nhyperplane\noptimal separating hyperplane), which is the separating hyperplane that\noptimal\nseparating\nhyperplane\nis farthest from the training observations. That is, we can compute the\n(perpendicular) distance from each training observation to a given separat-\ning hyperplane; the smallest such distance is the minimal distance from the\nobservations to the hyperplane, and is known as themargin. The maximal\nmarginmargin hyperplane is the separating hyperplane for which the margin is\nlargest\u2014that is, it is the hyperplane that has the farthest minimum dis-\ntance to the training observations. We can then classify a test observation\nbasedonwhichside ofthemaximal marginhyperplaneit lies.Thisis known\nas themaximal margin classifier. We hope that a classifier that has a largemaximal\nmargin\nclassifier\nmargin on the training data will also have a large margin on the test data,\nand hence will classify the test observations correctly. Although the maxi-\nmal margin classifier is often successful, it can also lead to overfitting when\np is large. If \u03b20, \u03b21,..., \u03b2p are the coe\ufb00icients of the maximal margin hyperplane,\nthen the maximal margin classifier classifies the test observationx\u2217 based\non the sign off(x\u2217)= \u03b20 + \u03b21x\u2217\n1 + \u03b22x\u2217\n2 + \u00b7\u00b7\u00b7 + \u03b2px\u2217\np. Figure 9.3 shows the maximal margin hyperplane on the data set of\nFigure 9.2. Comparing the right-hand panel of Figure9.2 to Figure9.3,\nwe see that the maximal margin hyperplane shown in Figure9.3 does in-\ndeed result in a greater minimal distance between the observations and the\nseparating hyperplane\u2014that is, a larger margin. In a sense, the maximal\nmargin hyperplane represents the mid-line of the widest \u201cslab\u201d that we can\ninsert between the two classes. ExaminingFigure 9.3,weseethatthreetrainingobservationsareequidis-\ntant from the maximal margin hyperplane and lie along the dashed lines\nindicating the width of the margin. These three observations are known as\nsupport vectors, since they are vectors inp-dimensional space (in Figure9.3, support\nvectorp =2 ) and they \u201csupport\u201d the maximal margin hyperplane in the sense\nthat if these points were moved slightly then the maximal margin hyper-\nplane would move as well. Interestingly, the maximal margin hyperplane\ndepends directly on the support vectors, but not on the other observations:\na movement to any of the other observations would not affect the separating\nhyperplane, provided that the observation\u2019s movement does not cause it to\n\n372 9. Support Vector Machines\n\u221210 1 2 3\n\u221210 1 2 3\nX1\nX2\nFIGURE 9.3. There are two classes of observations, shown in blue and in\npurple. The maximal margin hyperplane is shown as a solid line. The margin\nis the distance from the solid line to either of the dashed lines. The two blue\npoints and the purple point that lie on the dashed lines are the support vectors,\nand the distance from those points to the hyperplane is indicated by arrows. The\npurple and blue grid indicates the decision rule made by a classifier based on this\nseparating hyperplane. cross the boundary set by the margin. The fact that the maximal margin\nhyperplane depends directly on only a small subset of the observations is\nan important property that will arise later in this chapter when we discuss\nthe support vector classifier and support vector machines. 9.1.4 Construction of the Maximal Margin Classifier\nWe now consider the task of constructing the maximal margin hyperplane\nbased on a set ofn training observationsx1,...,x n \u2208 Rp and associated\nclass labelsy1,...,y n \u2208 {\u22121, 1}. Briefly, the maximal margin hyperplane\nis the solution to the optimization problem\nmaximize\u03b20,\u03b21,...,\u03b2p,M\nM (9.9)\nsubject to\np\u2211\nj=1\n\u03b22\nj =1 , (9.10)\nyi(\u03b20 + \u03b21xi1 + \u03b22xi2 + \u00b7\u00b7\u00b7 + \u03b2pxip) \u2265 M \u2200 i =1 , . .\n\n382 9. Support Vector Machines\nwhich would just give us back the support vector classifier. Equation9.21\nis known as alinear kernel because the support vector classifier is linear\nin the features; the linear kernel essentially quantifies the similarity of a\npair of observations using Pearson (standard) correlation. But one could\ninstead choose another form for (9.20). For instance, one could replace\nevery instance of\u2211 p\nj=1 xijxi\u2032j with the quantity\nK(xi,x i\u2032) = (1 +\np\u2211\nj=1\nxijxi\u2032j)d. (9.22)\nThis is known as apolynomial kernelof degreed, whered is a positivepolynomial\nkernelinteger. Using such a kernel withd> 1, instead of the standard linear\nkernel(9.21),inthesupportvectorclassifieralgorithmleadstoamuchmore\nflexible decision boundary. It essentially amounts to fitting a support vector\nclassifier in a higher-dimensional space involving polynomials of degreed,\nrather than in the original feature space. When the support vector classifier\nis combined with a non-linear kernel such as (9.22), the resulting classifier is\nknown as a support vector machine. Note that in this case the (non-linear)\nfunction has the form\nf(x)= \u03b20 +\n\u2211\ni\u2208S\n\u03b1iK(x, xi). (9.23)\nThe left-hand panel of Figure9.9 shows an example of an SVM with a\npolynomial kernel applied to the non-linear data from Figure9.8. The fit is\na substantial improvement over the linear support vector classifier. When\nd =1 , then the SVM reduces to the support vector classifier seen earlier in\nthis chapter. The polynomial kernel shown in (9.22) is one example of a possible\nnon-linear kernel, but alternatives abound. Another popular choice is the\nradial kernel, which takes the form radial kernel\nK(xi,x i\u2032) = exp(\u2212\u03b3\np\u2211\nj=1\n(xij \u2212 xi\u2032j)2). (9.24)\nIn (9.24), \u03b3 is a positive constant. The right-hand panel of Figure9.9 shows\nan example of an SVM with a radial kernel on this non-linear data; it also\ndoes a good job in separating the two classes. How does the radial kernel (9.24) actually work? If a given test obser-\nvation x\u2217 =( x\u2217\n1,...,x \u2217\np)T is far from a training observationxi in terms of\nEuclidean distance, then\u2211 p\nj=1(x\u2217\nj \u2212xij)2 will be large, and soK(x\u2217,x i)=\nexp(\u2212\u03b3 \u2211 p\nj=1(x\u2217\nj \u2212 xij)2) will be tiny. This means that in (9.23), xi will\nplay virtually no role inf(x\u2217). Recall that the predicted class label for the\ntest observationx\u2217 is based on the sign off(x\u2217). In other words, training\nobservations that are far fromx\u2217 will play essentially no role in the pre-\ndicted class label forx\u2217. This means that the radial kernel has verylocal\n\n9.3 Support Vector Machines 383\n\u22124 \u221220 2 4\n\u22124 \u221220 2 4\n  \n  \n  \n  \n  \n  \n\u22124 \u221220 2 4\n\u22124 \u221220 2 4\n    \n  \n  \nX1X1\nX2\nX2\nFIGURE 9.9.Left: An SVM with a polynomial kernel of degree 3 is applied to\nthe non-linear data from Figure9.8, resulting in a far more appropriate decision\nrule. Right: An SVM with a radial kernel is applied. In this example, either kernel\nis capable of capturing the decision boundary. behavior, in the sense that only nearby training observations have an effect\non the class label of a test observation. What is the advantage of using a kernel rather than simply enlarging\nthe feature space using functions of the original features, as in (9.16)? One\nadvantage is computational, and it amounts to the fact that using kernels,\none need only computeK(xi,x \u2032\ni) for all\n(n\n2\n)\ndistinct pairsi, i\u2032. This can be\ndone without explicitly working in the enlarged feature space. This is im-\nportant because in many applications of SVMs, the enlarged feature space\nis so large that computations are intractable. For some kernels, such as the\nradial kernel (9.24), the feature space isimplicit and infinite-dimensional,\nso we could never do the computations there anyway! 9.3.3 An Application to the Heart Disease Data\nIn Chapter8we apply decision trees and related methods to theHeart data. The aim is to use13 predictors such asAge,Sex, andChol in order to predict\nwhether an individual has heart disease. We now investigate how an SVM\ncompares to LDA on this data. After removing 6 missing observations, the\ndata consist of 297 subjects, which we randomly split into 207 training and\n90 test observations. We first fit LDA and the support vector classifier to the training data. Note that the support vector classifier is equivalent to an SVM using a poly-\nnomial kernel of degreed =1 . The left-hand panel of Figure9.10 displays\nROC curves (described in Section4.4.2) for the training set predictions for\nboth LDA and the support vector classifier. Both classifiers compute scores\nof the form\u02c6f(X)= \u02c6\u03b20 + \u02c6\u03b21X1 + \u02c6\u03b22X2 + \u00b7\u00b7\u00b7 + \u02c6\u03b2pXp for each observation. 384 9. Support Vector Machines\nFalse positive rate\nTrue positive rate\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nSupport Vector ClassifierLDA\nFalse positive rate\nTrue positive rate\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nSupport Vector ClassifierSVM: \u03b3=10\u22123\nSVM: \u03b3=10\u22122\nSVM: \u03b3=10\u22121\nFIGURE 9.10.ROC curves for theHeart data training set.Left: The support\nvector classifier and LDA are compared.Right: The support vector classifier is\ncompared to an SVM using a radial basis kernel with\u03b3 = 10\u22123, 10\u22122, and10\u22121. For any given cutofft, we classify observations into theheart diseaseor\nno heart diseasecategories depending on whether\u02c6f(X) <t or \u02c6f(X) \u2265 t. The ROC curve is obtained by forming these predictions and computing\nthe false positive and true positive rates for a range of values oft. An opti-\nmal classifier will hug the top left corner of the ROC plot. In this instance\nLDA and the support vector classifier both perform well, though there is a\nsuggestion that the support vector classifier may be slightly superior. The right-hand panel of Figure9.10 displays ROC curves for SVMs using\na radial kernel, with various values of\u03b3. As\u03b3 increases and the fit becomes\nmore non-linear, the ROC curves improve. Using\u03b3 = 10\u22121 appears to give\nan almost perfect ROC curve. However, these curves represent training\nerror rates, which can be misleading in terms of performance on new test\ndata. Figure9.11 displays ROC curves computed on the90 test observa-\ntions. We observe some differences from the training ROC curves. In the\nleft-hand panel of Figure9.11, the support vector classifier appears to have\na small advantage over LDA (although these differences are not statisti-\ncally significant). In the right-hand panel, the SVM using\u03b3 = 10\u22121, which\nshowed the best results on the training data, produces the worst estimates\non the test data. This is once again evidence that while a more flexible\nmethod will often produce lower training error rates, this does not neces-\nsarily lead to improved performance on test data. The SVMs with\u03b3 = 10\u22122\nand \u03b3 = 10\u22123 perform comparably to the support vector classifier, and all\nthree outperform the SVM with\u03b3 = 10\u22121. 9.4 SVMs with More than Two Classes 385\nFalse positive rate\nTrue positive rate\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nSupport Vector ClassifierLDA\nFalse positive rate\nTrue positive rate\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nSupport Vector ClassifierSVM: \u03b3=10\u22123\nSVM: \u03b3=10\u22122\nSVM: \u03b3=10\u22121\nFIGURE 9.11.ROC curves for the test set of theHeart data. Left: The support\nvector classifier and LDA are compared.Right: The support vector classifier is\ncompared to an SVM using a radial basis kernel with\u03b3 = 10\u22123, 10\u22122, and10\u22121. 9.4 SVMs with More than Two Classes\nSo far, our discussion has been limited to the case of binary classification:\nthat is, classification in the two-class setting. How can we extend SVMs\nto the more general case where we have some arbitrary number of classes? It turns out that the concept of separating hyperplanes upon which SVMs\nare based does not lend itself naturally to more than two classes. Though\na number of proposals for extending SVMs to theK-class case have been\nmade, the two most popular are theone-versus-one and one-versus-all\napproaches. We briefly discuss those two approaches here. 9.4.1 One-Versus-One Classification\nSuppose that we would like to perform classification using SVMs, and there\nare K> 2 classes. Aone-versus-one or all-pairs approach constructs\n(K\n2\n)\none-versus-\noneSVMs, each of which compares a pair of classes. For example, one such\nSVM might compare thekth class, coded as+1, to thek\u2032th class, coded\nas \u22121. We classify a test observation using each of the\n(K\n2\n)\nclassifiers, and\nwe tally the number of times that the test observation is assigned to each\nof theK classes. The final classification is performed by assigning the test\nobservation to the class to which it was most frequently assigned in these(K\n2\n)\npairwise classifications. 9.4.2 One-Versus-All Classification\nThe one-versus-all approach (also referred to asone-versus-rest) is an al-one-versus-\nall\none-versus-\nrest\nternative procedure for applying SVMs in the case ofK> 2 classes. We\n\n386 9. Support Vector Machines\nfit K SVMs, each time comparing one of theK classes to the remaining\nK \u22121 classes. Let\u03b20k, \u03b21k,..., \u03b2pk denote the parameters that result from\nfitting an SVM comparing thekth class (coded as+1) to the others (coded\nas \u22121). Letx\u2217 denote a test observation. We assign the observation to the\nclass for which\u03b20k + \u03b21kx\u2217\n1 + \u03b22kx\u2217\n2 + \u00b7\u00b7\u00b7 + \u03b2pkx\u2217\np is largest, as this amounts\nto a high level of confidence that the test observation belongs to thekth\nclass rather than to any of the other classes. 9.5 Relationship to Logistic Regression\nWhen SVMs were first introduced in the mid-1990s, they made quite a\nsplash in the statistical and machine learning communities. This was due\nin part to their good performance, good marketing, and also to the fact\nthat the underlying approach seemed both novel and mysterious. The idea\nof finding a hyperplane that separates the data as well as possible, while al-\nlowing some violations to this separation, seemed distinctly different from\nclassical approaches for classification, such as logistic regression and lin-\near discriminant analysis. Moreover, the idea of using a kernel to expand\nthe feature space in order to accommodate non-linear class boundaries ap-\npeared to be a unique and valuable characteristic. However, since that time, deep connections between SVMs and other\nmore classical statistical methods have emerged. It turns out that one can\nrewrite the criterion (9.12)\u2013(9.15) for fitting the support vector classifier\nf(X)= \u03b20 + \u03b21X1 + \u00b7\u00b7\u00b7 + \u03b2pXp as\nminimize\u03b20,\u03b21,...,\u03b2p\n\u23a7\n\u23a8\n\u23a9\nn\u2211\ni=1\nmax [0, 1 \u2212 yif(xi)] +\u03bb\np\u2211\nj=1\n\u03b22\nj\n\u23ab\n\u23ac\n\u23ad, (9.25)\nwhere \u03bb is a nonnegative tuning parameter. When\u03bb is large then\u03b21,..., \u03b2p\nare small, more violations to the margin are tolerated, and a low-variance\nbut high-bias classifier will result. When\u03bb is small then few violations\nto the margin will occur; this amounts to a high-variance but low-bias\nclassifier. Thus, a small value of\u03bb in (9.25) amounts to a small value ofC\nin (9.15). Note that the\u03bb \u2211 p\nj=1 \u03b22\nj term in (9.25) is the ridge penalty term\nfrom Section6.2.1, and plays a similar role in controlling the bias-variance\ntrade-off for the support vector classifier. Now (9.25) takes the \u201cLoss + Penalty\u201d form that we have seen repeatedly\nthroughout this book:\nminimize\u03b20,\u03b21,...,\u03b2p\n{L(X, y, \u03b2)+ \u03bbP(\u03b2)} . (9.26)\nIn (9.26), L(X, y, \u03b2) is some loss function quantifying the extent to which\nthe model, parametrized by\u03b2, fits the data(X, y), andP(\u03b2) is a penalty\n\n9.5 Relationship to Logistic Regression 387\nfunction on the parameter vector\u03b2 whose effect is controlled by a nonneg-\native tuning parameter\u03bb. For instance, ridge regression and the lasso both\ntake this form with\nL(X, y, \u03b2)=\nn\u2211\ni=1\n\u239b\n\u239dyi \u2212 \u03b20 \u2212\np\u2211\nj=1\nxij\u03b2j\n\u239e\n\u23a0\n2\nand withP(\u03b2)= \u2211 p\nj=1 \u03b22\nj for ridge regression andP(\u03b2)= \u2211 p\nj=1 |\u03b2j| for\nthe lasso. In the case of (9.25) the loss function instead takes the form\nL(X, y, \u03b2)=\nn\u2211\ni=1\nmax [0, 1 \u2212 yi(\u03b20 + \u03b21xi1 + \u00b7\u00b7\u00b7 + \u03b2pxip)] . This is known ashinge loss, and is depicted in Figure9.12. However, ithinge loss\nturns out that the hinge loss function is closely related to the loss function\nused in logistic regression, also shown in Figure9.12. An interesting characteristic of the support vector classifier is that only\nsupport vectors play a role in the classifier obtained; observations on the\ncorrect side of the margin do not affect it. This is due to the fact that the\nloss function shown in Figure9.12 is exactly zero for observations for which\nyi(\u03b20 + \u03b21xi1 + \u00b7\u00b7\u00b7 + \u03b2pxip) \u2265 1; these correspond to observations that are\non the correct side of the margin.3 In contrast, the loss function for logistic\nregression shown in Figure9.12 is not exactly zero anywhere. But it is very\nsmall for observations that are far from the decision boundary. Due to the\nsimilarities between their loss functions, logistic regression and the support\nvector classifier often give very similar results. When the classes are well\nseparated, SVMs tend to behave better than logistic regression; in more\noverlapping regimes, logistic regression is often preferred. When the support vector classifier and SVM were first introduced, it was\nthought that the tuning parameterC in (9.15) was an unimportant \u201cnui-\nsance\u201d parameter that could be set to some default value, like 1. However,\nthe \u201cLoss + Penalty\u201d formulation (9.25) for the support vector classifier\nindicates that this is not the case. The choice of tuning parameter is very\nimportant and determines the extent to which the model underfits or over-\nfits the data, as illustrated, for example, in Figure9.7. We have established that the support vector classifier is closely related\nto logistic regression and other preexisting statistical methods. Is the SVM\nunique in its use of kernels to enlarge the feature space to accommodate\nnon-linear class boundaries? The answer to this question is \u201cno\u201d. We could\njust as well perform logistic regression or many of the other classification\nmethods seen in this book using non-linear kernels; this is closely related\n3With this hinge-loss + penalty representation, the margin corresponds to the value\none, and the width of the margin is determined by\u2211\u03b22\nj . 388 9. Support Vector Machines\n\u22126 \u22124 \u221220 2\n02468\nLoss\nSVM LossLogistic Regression Loss\nyi(\u03b20+\u03b21xi1+...+\u03b2pxip)\nFIGURE 9.12.The SVM and logistic regression loss functions are compared,\nas a function ofyi(\u03b20 + \u03b21xi1 + \u00b7\u00b7\u00b7 + \u03b2pxip). Whenyi(\u03b20 + \u03b21xi1 + \u00b7\u00b7\u00b7 + \u03b2pxip) is\ngreater than 1, then the SVM loss is zero, since this corresponds to an observation\nthat is on the correct side of the margin. Overall, the two loss functions have quite\nsimilar behavior. to some of the non-linear approaches seen in Chapter7. However, for his-\ntorical reasons, the use of non-linear kernels is much more widespread in\nthe context of SVMs than in the context of logistic regression or other\nmethods. Though we have not addressed it here, there is in fact an extension\nof the SVM for regression (i.e. for a quantitative rather than a qualita-\ntive response), calledsupport vector regression. In Chapter3, we saw thatsupport\nvector\nregression\nleast squares regression seeks coe\ufb00icients\u03b20, \u03b21,..., \u03b2p such that the sum\nof squared residuals is as small as possible. (Recall from Chapter3 that\nresiduals are defined asyi \u2212 \u03b20 \u2212 \u03b21xi1 \u2212 \u00b7\u00b7\u00b7 \u2212 \u03b2pxip.) Support vector\nregression instead seeks coe\ufb00icients that minimize a different type of loss,\nwhere only residuals larger in absolute value than some positive constant\ncontribute to the loss function. This is an extension of the margin used in\nsupport vector classifiers to the regression setting. 9.6 Lab: Support Vector Machines\nWe use thee1071 library inR to demonstrate the support vector classifier\nand the SVM. Another option is theLiblineaR library, which is useful for\nvery large linear problems. 9.6 Lab: Support Vector Machines 389\n9.6.1 Support Vector Classifier\nThe e1071 library contains implementations for a number of statistical\nlearning methods. In particular, thesvm() function can be used to fit asvm()\nsupport vector classifier when the argumentkernel = \"linear\" is used. This function uses a slightly different formulation from (9.14) and (9.25)\nfor the support vector classifier. Acost argument allows us to specify the\ncost of a violation to the margin. When thecost argument is small, then\nthe margins will be wide and many support vectors will be on the margin\nor will violate the margin. When thecost argument is large, then the mar-\ngins will be narrow and there will be few support vectors on the margin or\nviolating the margin. We now use thesvm() function to fit the support vector classifier for a\ngiven value of thecost parameter. Here we demonstrate the use of this\nfunction on a two-dimensional example so that we can plot the resulting\ndecision boundary. We begin by generating the observations, which belong\nto two classes, and checking whether the classes are linearly separable. > set.seed (1)\n>x< - matrix (rnorm (20 * 2), ncol =2 )\n>y< - c(rep(-1, 10), rep(1, 10))\n>x [ y= =1 ,]< -x [ y= =1 ,]+1\n> plot(x, col = (3 - y))\nThey are not. Next, we fit the support vector classifier. Note that in order\nfor thesvm() function to perform classification (as opposed to SVM-based\nregression), we must encode the response as a factor variable. We now\ncreate a data frame with the response coded as a factor. >d a t< - data.frame (x = x, y = as.factor (y))\n> library (e1071)\n>s v m f i t< -svm(y \u223c ., data = dat, kernel = \"linear \",\ncost = 10, scale =F A L S E )\nThe argumentscale = FALSE tells thesvm() function not to scale each\nfeature to have mean zero or standard deviation one; depending on the\napplication, one might prefer to usescale = TRUE. We can now plot the support vector classifier obtained:\n> plot(svmfit, dat)\nNote that the two arguments to the SVMplot() function are the output\nof the call tosvm(), as well as the data used in the call tosvm(). The region\nof feature space that will be assigned to the\u22121 class is shown in light\nyellow, and the region that will be assigned to the+1 class is shown in\nred. The decision boundary between the two classes is linear (because we\nused the argumentkernel = \"linear\"), though due to the way in which the\nplotting function is implemented in this library the decision boundary looks\nsomewhat jagged in the plot. (Note that here the second feature is plotted\non thex-axis and the first feature is plotted on they-axis, in contrast to\n\n390 9. Support Vector Machines\nthe behavior of the usualplot() function inR.) The support vectors are\nplotted as crosses and the remaining observations are plotted as circles;\nwe see here that there are seven support vectors. We can determine their\nidentities as follows:\n>s v m f i t $index\n[1] 1 2 5 7 14 16 17\nWe can obtain some basic information about the support vector classifier\nfit using thesummary() command:\n> summary (svmfit)\nCall:\nsvm( formula =y \u223c ., data = dat, kernel = \"linear\" ,c o s t=1 0 ,\nscale =F A L S E )\nParameters:\nSVM-Type: C-classification\nSVM-Kernel: linear\ncost: 10\nNumber of Support Vectors: 7\n(43)\nNumber of Classes: 2\nLevels:\n-1 1\nThis tells us, for instance, that a linear kernel was used withcost = 10,\nand that there were seven support vectors, four in one class and three in\nthe other. What if we instead used a smaller value of the cost parameter? >s v m f i t< -svm(y \u223c ., data = dat, kernel = \"linear \",\ncost = 0.1, scale = FALSE)\n> plot(svmfit, dat)\n>s v m f i t $index\n[1] 1 2 3 4 5 7 9 10 12 13 14 15 16 17 18 20\nNow that a smaller value of the cost parameter is being used, we obtain a\nlarger number of support vectors, because the margin is now wider. Unfor-\ntunately, thesvm() function does not explicitly output the coe\ufb00icients of\nthe linear decision boundary obtained when the support vector classifier is\nfit, nor does it output the width of the margin. The e1071 library includes a built-in function,tune(), to perform cross-tune()\nvalidation. By default,tune() performs ten-fold cross-validation on a set\nof models of interest. In order to use this function, we pass in relevant\ninformation about the set of models that are under consideration. The\nfollowing command indicates that we want to compare SVMs with a linear\nkernel, using a range of values of thecost parameter. > set.seed (1)\n>t u n e . o u t< -tune(svm, y \u223c ., data = dat, kernel = \"linear \",\nranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n\n9.6 Lab: Support Vector Machines 391\nWe can easily access the cross-validation errors for each of these models\nusing thesummary() command:\n> summary (tune.out)\nParameter tuning of `svm ':\n-s a m p l i n gm e t h o d :1 0 - f o l dc r o s sv a l i d a t i o n\n-b e s tp a r a m e t e r s :\ncost\n0.1\n-b e s tp e r f o r m a n c e :0 . 0 5\n-D e t a i l e dp e r f o r m a n c er e s u l t s :\ncost error dispersion\n11 e - 0 3 0 .\n\nMake\nplots and report training and test error rates in order to back up\nyour assertions. 5. We have seen that we can fit an SVM with a non-linear kernel in order\ntoperformclassificationusinganon-lineardecisionboundary.Wewill\nnow see that we can also obtain a non-linear decision boundary by\nperforming logistic regression using non-linear transformations of the\nfeatures. 400 9. Support Vector Machines\n(a) Generate a data set withn = 500andp =2 , such that the obser-\nvations belong to two classes with a quadratic decision boundary\nbetween them. For instance, you can do this as follows:\n>x 1< - runif (500) - 0.5\n>x 2< - runif (500) - 0.5\n>y< -1*( x 1 ^ 2-x 2 ^ 2>0 )\n(b) Plot the observations, colored according to their class labels. Your plot should displayX1 on thex-axis, andX2 on they-\naxis. (c) Fit a logistic regression model to the data, usingX1 and X2 as\npredictors. (d) Apply this model to thetraining datain order to obtain a pre-\ndicted class label for each training observation. Plot the ob-\nservations, colored according to thepredicted class labels. The\ndecision boundary should be linear. (e) Now fit a logistic regression model to the data using non-linear\nfunctions ofX1 and X2 as predictors (e.g.X2\n1 , X1 \u00d7X2, log(X2),\nand so forth). (f) Apply this model to thetraining datain order to obtain a pre-\ndicted class label for each training observation. Plot the ob-\nservations, colored according to thepredicted class labels. The\ndecision boundary should be obviously non-linear. If it is not,\nthen repeat (a)-(e) until you come up with an example in which\nthe predicted class labels are obviously non-linear. (g) Fit a support vector classifier to the data withX1 and X2 as\npredictors. Obtain a class prediction for each training observa-\ntion. Plot the observations, colored according to thepredicted\nclass labels. (h) Fit a SVM using a non-linear kernel to the data. Obtain a class\nprediction for each training observation. Plot the observations,\ncolored according to thepredicted class labels. (i) Comment on your results.\n\nThis problem involves theOJ data set which is part of theISLR2\npackage. 402 9. Support Vector Machines\n(a) Create a training set containing a random sample of800\nobservations, and a test set containing the remaining\nobservations. (b) Fit a support vector classifier to the training data using\ncost = 0.01, withPurchase as the response and the other vari-\nables as predictors. Use thesummary() function to produce sum-\nmary statistics, and describe the results obtained. (c) What are the training and test error rates? (d) Use thetune() function to select an optimalcost. Consider val-\nues in the range0.01 to 10. (e) Compute the training and test error rates using this new value\nfor cost. (f) Repeat parts (b) through (e) using a support vector machine\nwith a radial kernel. Use the default value forgamma. (g) Repeat parts (b) through (e) using a support vector machine\nwith a polynomial kernel. Setdegree = 2.",
    "topic_8": "(h) Overall, which approach seems to give the best results on this\ndata? This is page 403\nPrinter: Opaque this\n10\nDeep Learning\nThis chapter covers the important topic ofdeep learning. At the time ofdeep\nlearningwriting (2020), deep learning is a very active area of research in the machine\nlearning and artificial intelligence communities. The cornerstone of deep\nlearning is theneural network. neural\nnetworkNeural networks rose to fame in the late 1980s. There was a lot of excite-\nment and a certain amount of hype associated with this approach, and they\nwere the impetus for the popularNeural Information Processing Systems\nmeetings (NeurIPS, formerly NIPS) held every year, typically in exotic\nplaces like ski resorts. This was followed by a synthesis stage, where the\nproperties of neural networks were analyzed by machine learners, math-\nematicians and statisticians; algorithms were improved, and the method-\nology stabilized. Then along came SVMs, boosting, and random forests,\nand neural networks fell somewhat from favor. Part of the reason was that\nneural networks required a lot of tinkering, while the new methods were\nmore automatic. Also, on many problems the new methods outperformed\npoorly-trained neural networks. This was thestatus quofor the first decade\nin the new millennium. All the while, though, a core group of neural-network enthusiasts were\npushing their technology harder on ever-larger computing architectures and\ndata sets. Neural networks resurfaced after 2010 with the new namedeep\nlearning, with new architectures, additional bells and whistles, and a string\nof success stories on some niche problems such as image and video classifi-\ncation, speech and text modeling. Many in the field believe that the major\nreason for these successes is the availability of ever-larger training datasets,\nmade possible by the wide-scale use of digitization in science and industry. 404 10. Deep Learning\nInthischapterwediscussthebasicsofneuralnetworksanddeeplearning,\nand then go into some of the specializations for specific problems, such as\nconvolutional neural networks (CNNs) for image classification, and recur-\nrent neural networks (RNNs) for time series and other sequences. We will\nalso demonstrate these models using thePython package keras, which in-\nterfaces with thetensorflow deep-learning software developed at Google.1\nThe material in this chapter is slightly more challenging than elsewhere\nin this book. 10.1 Single Layer Neural Networks\nA neural network takes an input vector ofp variablesX =( X1,X 2,...,X p)\nand builds a nonlinear functionf(X) to predict the responseY . We have\nbuilt nonlinear prediction models in earlier chapters, using trees, boosting\nand generalized additive models. What distinguishes neural networks from\nthese methods is the particularstructure of the model. Figure10.1 shows\na simplefeed-forward neural networkfor modeling a quantitative responsefeed-forward\nneural\nnetwork\nusing p =4 predictors. In the terminology of neural networks, the four fea-\ntures X1,...,X 4 make up the units in theinput layer. The arrows indicate\ninput layerthat each of the inputs from the input layer feeds into each of theK hidden\nunits (we get to pickK; here we chose5). The neural network model hashidden units\nthe form\nf(X)= \u03b20 + \u2211 K\nk=1 \u03b2khk(X)\n= \u03b20 + \u2211 K\nk=1 \u03b2kg(wk0 + \u2211 p\nj=1 wkjXj). (10.1)\nIt is built up here in two steps. First theK activations Ak,k =1 ,...,K , in activations\nthehiddenlayerarecomputedasfunctionsoftheinputfeatures X1,...,X p,\nAk = hk(X)= g(wk0 + \u2211 p\nj=1 wkjXj), (10.2)\nwhere g(z) is a nonlinearactivation functionthat is specified in advance.activation\nfunctionWe can think of eachAk as a different transformationhk(X) of the original\nfeatures, much like the basis functions of Chapter 7. TheseK activations\nfrom the hidden layer then feed into the output layer, resulting in\nf(X)= \u03b20 +\nK\u2211\nk=1\n\u03b2kAk, (10.3)\na linear regression model in theK =5 activations. All the parameters\n\u03b20,..., \u03b2K and w10,...,w Kp need to be estimated from data. In the early\n1For more information aboutkeras, see Chollet et al. (2015) \u201cKeras\u201d, available at\nhttps://keras.io.\n\nTo avoid overfitting, some regularization is needed. In this\nexample, we used two forms of regularization: ridge regularization, which\nis similar to ridge regression from Chapter6, anddropout regularization. dropout\nWe discuss both forms of regularization in Section10.7. 10.3 Convolutional Neural Networks\nNeural networks rebounded around 2010 with big successes in image classi-\nfication. Around that time, massive databases of labeled images were being\naccumulated, with ever-increasing numbers of classes. Figure10.5 shows\n75 images drawn from theCIFAR100 database.5 This database consists of\n60,000 images labeled according to 20 superclasses (e.g. aquatic mammals),\nwith five classes per superclass (beaver, dolphin, otter, seal, whale). Each\nimage has a resolution of32 \u00d7 32 pixels, with three eight-bit numbers per\npixel representing red, green and blue. The numbers for each image are\norganized in a three-dimensional array called afeature map. The first twofeature mapaxes are spatial (both are32-dimensional), and the third is thechannel channelaxis,6 representing the three colors. There is a designated training set of\n50,000 images, and a test set of 10,000. A special family ofconvolutional neural networks(CNNs) has evolved forconvolutional\nneural\nnetworks\nclassifying images such as these, and has shown spectacular success on a\nwide range of problems. CNNs mimic to some degree how humans classify\nimages, by recognizing specific features or patterns anywhere in the image\n5See Chapter 3 of Krizhevsky (2009) \u201cLearning multiple layers of fea-\ntures from tiny images\u201d, available at https://www.cs.toronto.edu/~kriz/\nlearning-features-2009-TR.pdf. 6The termchannel is taken from the signal-processing literature.\n\nwe compute a gradient step. This process is known asstochastic gradient\ndescent(SGD) and is the state of the art for learning deep neural networks.stochastic\ngradient\ndescent\nFortunately, there is very good software for setting up deep learning mod-\nels, and for fitting them to data, so most of the technicalities are hidden\nfrom the user. We now turn to the multilayer network (Figure10.4) used in the digit\nrecognitionproblem.Thenetworkhasover235,000weights,whichisaround\nfour times the number of training examples. Regularization is essential here\nto avoid overfitting. The first row in Table10.1 uses ridge regularization on\nthe weights. This is achieved by augmenting the objective function (10.14)\nwith a penalty term:\nR(\u03b8; \u03bb)= \u2212\nn\u2211\ni=1\n9\u2211\nm=0\nyim log(fm(xi)) +\u03bb\n\u2211\nj\n\u03b82\nj . (10.31)\nThe parameter\u03bb is often preset at a small value, or else it is found using the\nvalidation-set approach of Section5.3.1. We can also use different values of\n\u03bb for the groups of weights from different layers; in this caseW1 and W2\nwere penalized, while the relatively few weightsB of the output layer were\nnot penalized at all. Lasso regularization is also popular as an additional\nform of regularization, or as an alternative to ridge. Figure 10.18 shows some metrics that evolve during the training of the\nnetwork on theMNIST data. It turns out that SGD naturally enforces its\nown form of approximately quadratic regularization.22 Here the minibatch\n22This and other properties of SGD for deep learning are the subject of much research\nin the machine learning literature at the time of writing. 438 10.\n\nMuch of the work in fitting a CNN is in learning the convolution filters\nat the hidden layers; these are the coe\ufb00icients of a CNN. For models fit to\nmassive corpora such asimagenet with many classes, the output of these\n12These resnet results can change with time, since the publicly-trained model gets\nupdated periodically. 10.4 Document Classification 419\nfilters can serve as features for general natural-image classification prob-\nlems. One can use these pretrained hidden layers for new problems with\nmuch smaller training sets (a process referred to asweight freezing), andweight\nfreezingjust train the last few layers of the network, which requires much less data. The vignettes and book13 that accompany thekeras package give more\ndetails on such applications. 10.4 Document Classification\nIn this section we introduce a new type of example that has important\napplications in industry and science: predicting attributes of documents. Examples of documents include articles in medical journals, Reuters news\nfeeds, emails, tweets, and so on. Our example will beIMDb (Internet Movie\nDatabase) ratings \u2014 short documents where viewers have written critiques\nof movies.14 The response in this case is thesentiment of the review, which\nwill bepositive or negative. Here is the beginning of a rather amusing negative review:\nThis has to be one of the worst films of the 1990s. When my\nfriends & I were watching this film (being the target audience it\nwas aimed at) we just sat & watched the first half an hour with\nour jaws touching the floor at how bad it really was. The rest\nof the time, everyone else in the theater just started talking to\neach other, leaving or generally crying into their popcorn... Each review can be a different length, include slang or non-words, have\nspelling errors, etc. We need to find a way tofeaturize such a document.featurizeThis is modern parlance for defining a set of predictors. The simplest and most common featurization is thebag-of-wordsmodel. bag-of-wordsWe score each document for the presence or absence of each of the words in\na language dictionary \u2014 in this case an English dictionary. If the dictionary\ncontainsM words, that means for each document we create a binary feature\nvector of lengthM, and score a1 for every word present, and0 otherwise. That can be a very wide feature vector, so we limit the dictionary \u2014 in\nthis case to the 10,000 most frequently occurring words in the training\ncorpus of 25,000 reviews. Fortunately there are nice tools for doing this\nautomatically. Here is the beginning of a positive review that has been\nredacted in this way:\n\u27e8START\u27e9 this film was just brilliant casting location scenery\nstory direction everyone\u2019s really suited the part they played and\n13Deep Learning with Rby F. Chollet and J.J.",
    "topic_9": "Consider the effects of varying the dictionary size. Try the values\n1000, 3000, 5000, and 10,000, and compare the results. This is page 461\nPrinter: Opaque this\n11\nSurvival Analysis and Censored Data\nIn this chapter, we will consider the topics ofsurvival analysisand censored survival\nanalysisdata. These arise in the analysis of a unique kind of outcome variable: the\ncensored\ndata\ntime until an event occurs. For example, suppose that we have conducted a five-year medical study,\nin which patients have been treated for cancer. We would like to fit a model\nto predict patient survival time, using features such as baseline health mea-\nsurements or type of treatment. At first pass, this may sound like a regres-\nsion problem of the kind discussed in Chapter3. But there is an important\ncomplication: hopefully some or many of the patients have survived until\nthe end of the study. Such a patient\u2019s survival time is said to becensored:w e\nknow that it is at least five years, but we do not know its true value. We do\nnot want to discard this subset of surviving patients, as the fact that they\nsurvived at least five years amounts to valuable information. However, it is\nnot clear how to make use of this information using the techniques covered\nthus far in this textbook. Though the phrase \u201csurvival analysis\u201d evokes a medical study, the ap-\nplications of survival analysis extend far beyond medicine. For example,\nconsider a company that wishes to modelchurn, the process by which cus-\ntomers cancel subscription to a service. The company might collect data on\ncustomers over some time period, in order to model each customer\u2019s time\nto cancellation as a function of demographics or other predictors. However,\npresumably not all customers will have canceled their subscription by the\nend of this time period; for such customers, the time to cancellation is\ncensored. 462 11. Survival Analysis and Censored Data\nIn fact, survival analysis is relevant even in application areas that are\nunrelated to time. For instance, suppose we wish to model a person\u2019s weight\nas a function of some covariates, using a dataset with measurements for a\nlarge number of people. Unfortunately, the scale used to weigh those people\nis unable to report weights above a certain number. Then, any weights that\nexceed that number are censored. The survival analysis methods presented\nin this chapter could be used to analyze this dataset. Survival analysis is a very well-studied topic within statistics, due to its\ncritical importance in a variety of applications, both in and out of medicine. However, it has received relatively little attention in the machine learning\ncommunity. 11.1 Survival and Censoring Times\nFor each individual, we suppose that there is a truesurvival time, T, as wellsurvival time\nas a truecensoring time, C. (The survival time is also known as thefailure censoring\ntimetime or theevent time.) The survival time represents the time at which the\nfailure time\nevent time\nevent of interest occurs: for instance, the time at which the patient dies,\nor the customer cancels his or her subscription. By contrast, the censoring\ntime is the time at which censoring occurs: for example, the time at which\nthe patient drops out of the study or the study ends. We observe either the survival timeT or else the censoring timeC. Specifically, we observe the random variable\nY = min(T,C ). (11.1)\nIn other words, if the event occurs before censoring (i.e.T<C ) then we\nobserve the true survival timeT; however, if censoring occurs before the\nevent (T>C ) then we observe the censoring time. We also observe a status\nindicator,\n\u03b4 =\n{\n1 if T \u2264 C\n0 if T>C . Thus, \u03b4 =1 if we observe the true survival time, and\u03b4 =0 if we instead\nobserve the censoring time. Now, suppose we observen (Y, \u03b4) pairs, which we denote as(y1, \u03b41),...,\n(yn, \u03b4n). Figure11.1 displays an example from a (fictitious) medical study\nin which we observen =4 patients for a 365-day follow-up period. For\npatients 1 and 3, we observe the time to event (such as death or disease\nrelapse) T = ti. Patient 2 was alive when the study ended, and patient 4\ndropped out of the study, or was \u201clost to follow-up\u201d; for these patients we\nobserve C = ci. Therefore,y1 = t1, y3 = t3, y2 = c2, y4 = c4, \u03b41 = \u03b43 =1 ,\nand \u03b42 = \u03b44 =0 . 11.2 A Closer Look at Censoring 463\n0 100 200 300\n1 2 3 4\nTime in Days\nPatient\nFIGURE 11.1.Illustration of censored survival data. For patients 1 and 3, the\nevent was observed. Patient 2 was alive when the study ended. Patient 4 dropped\nout of the study. 11.2 A Closer Look at Censoring\nIn order to analyze survival data, we need to make some assumptions about\nwhy censoringhasoccurred.Forinstance,supposethatanumberofpatients\ndrop out of a cancer study early because they are very sick. An analysis that\ndoes not take into consideration the reason why the patients dropped out\nwill likely overestimate the true average survival time. Similarly, suppose\nthat males who are very sick are more likely to drop out of the study than\nfemales who are very sick. Then a comparison of male and female survival\ntimes may wrongly suggest that males survive longer than females. In general, we need to assume that the censoring mechanism isindepen-\ndent: conditional on the features, the event timeT is independent of the\ncensoring timeC. The two examples above violate the assumption of inde-\npendent censoring. Typically, it is not possible to determine from the data\nitself whether the censoring mechanism is independent. Instead, one has to\ncarefully consider the data collection process in order to determine whether\nindependent censoring is a reasonable assumption. In the remainder of this\nchapter, we will assume that the censoring mechanism is independent.1\nIn this chapter, we focus onright censoring, which occurs whenT \u2265 Y ,\ni.e. the true event timeT is at least as large as the observed timeY . (Notice thatT \u2265 Y is a consequence of (11.1). Right censoring derives its\nname from the fact that time is typically displayed from left to right, as in\nFigure 11.1.) However, other types of censoring are possible. For instance,\nin left censoring, the true event timeT is less than or equal to the observed\n1The assumption of independent censoring can be relaxed somewhat using the notion\nof non-informative censoring; however, the definition of non-informative censoring is too\ntechnical for this book. 464 11. Survival Analysis and Censored Data\ntime Y . For example, in a study of pregnancy duration, suppose that we\nsurvey patients 250 days after conception, when some have already had\ntheir babies. Then we know that for those patients, pregnancy duration is\nless than 250 days. More generally,interval censoringrefers to the setting\nin which we do not know the exact event time, but we know that it falls\nin some interval. For instance, this setting arises if we survey patients once\nper week in order to determine whether the event has occurred. While left\ncensoring and interval censoring can be accommodated using variants of\nthe ideas presented in this chapter, in what follows we focus specifically on\nright censoring. 11.3 The Kaplan\u2013Meier Survival Curve\nThe survival curve, orsurvival function, is defined as survival\ncurve\nsurvival\nfunction\nS(t) = Pr(T>t ). (11.2)\nThis decreasing function quantifies the probability of surviving past time\nt.\n\nNow, we consider the task of estimating the survival curve (11.2) for\nthese data. To estimateS(20) = Pr(T> 20), the probability that a patient\nsurvives for at leastt = 20 months, it is tempting to simply compute the\nproportion of patients who are known to have survived past 20 months, i.e. the proportion of patients for whomY> 20. This turns out to be48/88,\nor approximately55%. However, this does not seem quite right, sinceY\nand T represent different quantities. In particular, 17 of the 40 patients\n2This dataset is described in the following paper: Selingerov\u00e1 et al. (2016) Survival\nof patients with primary brain tumors: Comparison of two statistical approaches. PLoS\nOne, 11(2):e0148733. 11.3 The Kaplan\u2013Meier Survival Curve 465\nwho did not survive to20 months were actually censored, and this analysis\nimplicitly assumes thatT< 20 for all of those censored patients; of course,\nwe do not know whether that is true. Alternatively, to estimateS(20), we could consider computing the pro-\nportion of patients for whomY> 20, out of the71 patients who werenot\ncensored by timet = 20; this comes out to48/71, or approximately68%. However, this is not quite right either, since it amounts to completely ig-\nnoring the patients who were censored before timet = 20, even though the\ntime at which they are censored is potentially informative. For instance, a\npatient who was censored at timet = 19.9 likely would have survived past\nt = 20had he or she not been censored. We have seen that estimatingS(t) is complicated by the presence of\ncensoring. We now present an approach to overcome these challenges. We\nlet d1 <d 2 < \u00b7\u00b7\u00b7 <d K denote theK unique death times among the non-\ncensored patients, and we letqk denote the number of patients who died\nat timedk. Fork =1 ,...,K , we letrk denote the number of patients alive\nand in the study just beforedk; these are theat riskpatients. The set of\npatients that are at risk at a given time are referred to as therisk set. risk set\nBy the law of total probability,3\nPr(T>d k) = Pr(T>d k|T>d k\u22121) Pr(T>d k\u22121)\n+ Pr(T>d k|T \u2264 dk\u22121) Pr(T \u2264 dk\u22121). The fact thatdk\u22121 <d k implies thatPr(T>d k|T \u2264 dk\u22121)=0 (it is\nimpossible for a patient to survive past timedk if he or she did not survive\nuntil an earlier timedk\u22121). Therefore,\nS(dk) = Pr(T>d k) = Pr(T>d k|T>d k\u22121) Pr(T>d k\u22121). Plugging in (11.2) again, we see that\nS(dk) = Pr(T>d k|T>d k\u22121)S(dk\u22121). This implies that\nS(dk) = Pr(T>d k|T>d k\u22121) \u00d7 \u00b7\u00b7\u00b7 \u00d7 Pr(T>d 2|T>d 1) Pr(T>d 1). We now must simply plug in estimates of each of the terms on the right-\nhand side of the previous equation. It is natural to use the estimator\n\u02c6Pr(T>d j|T>d j\u22121)=( rj \u2212 qj)/rj,\nwhich is the fraction of the risk set at timedj who survived past timedj. This leads to theKaplan\u2013Meier estimatorof the survival curve: Kaplan\u2013\nMeier\nestimator3The law of total probability states that for any two eventsA and B, Pr(A)=\nPr(A|B) Pr(B) + Pr(A|Bc) Pr(Bc), whereBc is the complement of the eventB, i.e. it\nis the event thatB does not hold. 466 11. Survival Analysis and Censored Data\n0 20 40 60 80\n0.0 0.2 0.4 0.6 0.8 1.0\nMonths\nEstimated Probability of Survival\nFIGURE 11.2.For theBrainCancer data, we display the Kaplan\u2013Meier survival\ncurve (solid curve), along with standard error bands (dashed curves). \u02c6S(dk)=\nk\u220f\nj=1\n( rj \u2212 qj\nrj\n)\n. (11.3)\nFor timest between dk and dk+1, we set\u02c6S(t)= \u02c6S(dk). Consequently, the\nKaplan\u2013Meier survival curve has a step-like shape. The Kaplan\u2013Meier survival curve for theBrainCancer data is displayed\nin Figure11.2. Each point in the solid step-like curve shows the estimated\nprobability of surviving past the time indicated on the horizontal axis. The\nestimated probability of survival past 20 months is 71%, which is quite a\nbit higher than the naive estimates of 55% and 68% presented earlier. The sequential construction of the Kaplan\u2013Meier estimator \u2014 starting\nat time zero and mapping out the observed events as they unfold in time \u2014\nis fundamental to many of the key techniques in survival analysis. These\ninclude the log-rank test of Section11.4, and Cox\u2019s proportional hazard\nmodel of Section11.5.2. 11.4 The Log-Rank Test\nWe now continue our analysis of theBrainCancer data introduced in Sec-\ntion 11.3. We wish to compare the survival of males to that of females. Figure 11.3 shows the Kaplan\u2013Meier survival curves for the two groups. Females seem to fare a little better up to about 50 months, but then the\ntwo curves both level off to about 50%. How can we carry out a formal test\nof equality of the two survival curves? At first glance, a two-samplet-test seems like an obvious choice: we could\ntest whether the mean survival time among the females equals the mean\n\n11.4 The Log-Rank Test 467\n0 20 40 60 80\n0.0 0.2 0.4 0.6 0.8 1.0\nMonths\nEstimated Probability of SurvivalFemaleMale\nFIGURE 11.3.For theBrainCancer data, Kaplan\u2013Meier survival curves for\nmales and females are displayed. Group 1 Group 2 Total\nDied q1k q2k qk\nSurvived r1k \u2212 q1k r2k \u2212 q2k rk \u2212 qk\nTotal r1k r2k rk\nTABLE 11.1.Among the set of patients at risk at timedk, the number of patients\nwho died and survived in each of two groups is reported. survival time among the males. But the presence of censoring again creates\na complication. To overcome this challenge, we will conduct alog-rank test,4\nlog-rank test\nwhich examines how the events in each group unfold sequentially in time. Recall from Section11.3 that d1 <d 2 < \u00b7\u00b7\u00b7 <d K are the unique death\ntimes among the non-censored patients,rk is the number of patients at\nrisk at timedk, andqk is the number of patients who died at timedk.W e\nfurther definer1k and r2k to be the number of patients in groups 1 and 2,\nrespectively, who are at risk at timedk. Similarly, we defineq1k and q2k to\nbe the number of patients in groups 1 and 2, respectively, who died at time\ndk. Note thatr1k + r2k = rk and q1k + q2k = qk. At each death timedk, we construct a2 \u00d7 2 table of counts of the form\nshown in Table11.1. Note that if the death times are unique (i.e. no two\nindividuals die at the same time), then one ofq1k and q2k equals one, and\nthe other equals zero. The main idea behind the log-rank test statistic is as follows. In order\nto testH0 : E(X)= \u00b5 for some random variableX, one approach is to\n4The log-rank test is also known as theMantel\u2013Haenszel testor Cochran\u2013Mantel\u2013\nHaenszel test. 468 11. Survival Analysis and Censored Data\nconstruct a test statistic of the form\nW = X \u2212 \u00b5\u221a\nVar(X)\n. (11.4)\nTo construct the log-rank test statistic, we compute a quantity that takes\nexactly the form (11.4), withX = \u2211 K\nk=1 q1k, whereq1k is given in the top\nleft of Table11.1. In greater detail, if there is no difference in survival between the two\ngroups, and conditioning on the row and column totals in Table11.1, the\nexpected value ofq1k is\n\u00b5k = r1k\nrk\nqk. (11.5)\nSo the expected value ofX = \u2211 K\nk=1 q1k is \u00b5 = \u2211 K\nk=1\nr1k\nrk\nqk. Furthermore,\nit can be shown5 that the variance ofq1k is\nVar (q1k)= qk(r1k/rk)(1 \u2212 r1k/rk)(rk \u2212 qk)\nrk \u2212 1 . (11.6)\nThough q11,...,q 1K may be correlated, we nonetheless estimate\nVar\n( K\u2211\nk=1\nq1k\n)\n\u2248\nK\u2211\nk=1\nVar (q1k)=\nK\u2211\nk=1\nqk(r1k/rk)(1 \u2212 r1k/rk)(rk \u2212 qk)\nrk \u2212 1 . (11.7)\nTherefore, to compute the log-rank test statistic, we simply proceed as\nin (11.4), withX = \u2211 K\nk=1 q1k, making use of (11.5) and (11.7). That is, we\ncalculate\nW =\n\u2211 K\nk=1 (q1k \u2212 \u00b5k)\u221a \u2211 K\nk=1 Var (q1k)\n=\n\u2211 K\nk=1\n(\nq1k \u2212 qk\nrk\nr1k\n)\n\u221a \u2211 K\nk=1\nqk(r1k/rk)(1\u2212r1k/rk)(rk\u2212qk)\nrk\u22121\n. (11.8)\nWhen the sample size is large, the log-rank test statisticW has ap-\nproximately a standard normal distribution; this can be used to compute\na p-value for the null hypothesis that there is no difference between the\nsurvival curves in the two groups.6\nComparing the survival times of females and males on theBrainCancer\ndata gives a log-rank test statistic ofW =1 .2, which corresponds to a two-\nsided p-value of0.2 using the theoretical null distribution, and ap-value\nof 0.25 using the permutation null distribution with 1,000 permutations. 5For details, see Exercise7 at the end of this chapter. 6Alternatively, we can estimate thep-value via permutations, using ideas that will\nbe presented in Section13.5. The permutation distribution is obtained by randomly\nswapping the labels for the observations in the two groups. 11.5 Regression Models With a Survival Response 469\nThus,wecannotrejectthenullhypothesisofnodifferenceinsurvivalcurves\nbetween females and males. The log-rank test is closely related to Cox\u2019s proportional hazards model,\nwhich we discuss in Section11.5.2. 11.5 Regression Models With a Survival Response\nWe now consider the task of fitting a regression model to survival data. As in Section11.1, the observations are of the form(Y, \u03b4), whereY =\nmin(T,C ) is the (possibly censored) survival time, and\u03b4 is an indicator\nvariable that equals1 if T \u2264 C. Furthermore,X \u2208 Rp is a vector ofp\nfeatures. We wish to predict the true survival timeT. Since the observed quantityY is positive and may have a long right\ntail, we might be tempted to fit a linear regression oflog(Y ) on X. But\nas the reader will surely guess, censoring again creates a problem since\nwe are actually interested in predictingT and notY . To overcome this\ndi\ufb00iculty, we instead make use of a sequential construction, similar to the\nconstructions of the Kaplan\u2013Meier survival curve in Section11.3 and the\nlog-rank test in Section11.4. 11.5.1 The Hazard Function\nThe hazard functionor hazard rate\u2014 also known as theforce of mortalityhazard\nfunction\u2014 is formally defined as\nh(t) = lim\u2206t\u21920\nPr(t<T \u2264 t + \u2206t|T>t )\n\u2206t , (11.9)\nwhere T is the (unobserved) survival time. It is the death rate in the instant\nafter timet, given survival past that time.7 In (11.9), we take the limit as\n\u2206t approaches zero, so we can think of\u2206t as being an extremely tiny\nnumber. Thus, more informally, (11.9) implies that\nh(t) \u2248 Pr(t<T \u2264 t + \u2206t|T>t )\n\u2206t\nfor some arbitrarily small\u2206t. Why should we care about the hazard function? First of all, it is closely\nrelated to the survival curve (11.2), as we will see next. Second, it turns out\n7Due to the\u2206t in the denominator of (11.9), the hazard function is a rate of death,\nrather than a probability of death. However, higher values ofh(t) directly correspond\nto a higher probability of death, just as higher values of a probability density function\ncorrespond to more likely outcomes for a random variable. In fact,h(t) is the probability\ndensity function forT conditional onT>t .\n\nare 0.04, \u22120.3, 0,\n0.2, and\u22120.2, respectively. The coe\ufb00icient estimates resulting from the\nCox model are fairly accurate. 11.9 Exercises\nConceptual\n1. For each example, state whether or not the censoring mechanism is\nindependent. Justify your answer. (a) In a study of disease relapse, due to a careless research scientist,\nall patients whose phone numbers begin with the number \u201c2\u201d\nare lost to follow up. (b) In a study of longevity, a formatting error causes all patient ages\nthat exceed 99 years to be lost (i.e. we know that those patients\nare more than 99 years old, but we do not know their exact\nages). (c) Hospital A conducts a study of longevity. However, very sick\npatients tend to be transferred to Hospital B, and are lost to\nfollow up. (d) In a study of unemployment duration, the people who find work\nearlier are less motivated to stay in touch with study investiga-\ntors, and therefore are more likely to be lost to follow up. (e) In a study of pregnancy duration, women who deliver their ba-\nbies pre-term are more likely to do so away from their usual\nhospital, and thus are more likely to be censored, relative to\nwomen who deliver full-term babies. (f) A researcher wishes to model the number of years of education\nof the residents of a small town. Residents who enroll in college\nout of town are more likely to be lost to follow up, and are\nalso more likely to attend graduate school, relative to those who\nattend college in town. (g) Researchers conduct a study of disease-free survival (i.e.\n\n476 11. Survival Analysis and Censored Data\n0 20 40 60 80 100 120\n0.0 0.2 0.4 0.6 0.8 1.0\nMonths\nProbability of Not Being Published\nNegative ResultPositive Result\nFIGURE 11.5.Survival curves for time until publication for thePublication\ndata described in Section11.5.4, stratified by whether or not the study produced\na positive result. months until publication is recorded. Of the 244 trials, only 156 were pub-\nlished during the study period; the remaining studies were censored. The\ncovariatesincludewhetherthetrialfocusedonaclinicalendpoint( clinend),\nwhether the trial involved multiple centers (multi), the funding mechanism\nwithin the National Institutes of Health (mech), trial sample size (sampsize),\nbudget (budget), impact (impact, related to the number of citations), and\nwhether the trial produced a positive (significant) result (posres). The last\ncovariate is particularly interesting, as a number of studies have suggested\nthat positive trials have a higher publication rate. Figure11.5showstheKaplan\u2013Meiercurvesforthetimeuntilpublication,\nstratified by whether or not the study produced a positive result. We see\nslightevidencethattimeuntilpublicationislowerforstudieswithapositive\nresult. However, the log-rank test yields a very unimpressivep-value of0.36. We now consider a more careful analysis that makes use of all of the\navailable predictors. The results of fitting Cox\u2019s proportional hazards model\nusing all of the available features are shown in Table11.3. We find that the\nchance of publication of a study with a positive result ise0.55 =1 .74 times\nhigher than the chance of publication of a study with a negative result\nat any point in time, holding all other covariates fixed. The very small\np-value associated withposres in Table11.3 indicates that this result is\nhighly significant. This is striking, especially in light of our earlier finding\nthat a log-rank test comparing time to publication for studies with positive\nversus negative results yielded ap-value of0.36. How can we explain this\ndiscrepancy? The answer stems from the fact that the log-rank test did not\nconsider any other covariates, whereas the results in Table11.3 are based\non a Cox model using all of the available covariates. In other words, after\nwe adjust for all of the other covariates, then whether or not the study\nyielded a positive result is highly predictive of the time to publication.",
    "topic_10": "Rather, the goal is to discover interesting things about the measurements\non X1,X 2,...,X p. Is there an informative way to visualize the data? Can\nwe discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering\nquestions such as these. In this chapter, we will focus on two particu-\nlar types of unsupervised learning:principal components analysis, a tool\nused for data visualization or data pre-processing before supervised tech-\nniques are applied, andclustering, a broad class of methods for discovering\nunknown subgroups in data. 12.1 The Challenge of Unsupervised Learning\nSupervised learning is a well-understood area. In fact, if you have read\nthe preceding chapters in this book, then you should by now have a good\n\n496 12. Unsupervised Learning\ngrasp of supervised learning. For instance, if you are asked to predict a\nbinary outcome from a data set, you have a very well developed set of tools\nat your disposal (such as logistic regression, linear discriminant analysis,\nclassification trees, support vector machines, and more) as well as a clear\nunderstanding of how to assess the quality of the results obtained (using\ncross-validation, validation on an independent test set, and so forth). In contrast, unsupervised learning is often much more challenging. The\nexercise tends to be more subjective, and there is no simple goal for the\nanalysis, such as prediction of a response. Unsupervised learning is often\nperformed as part of anexploratory data analysis. Furthermore, it can beexploratory\ndata\nanalysis\nhard to assess the results obtained from unsupervised learning methods,\nsince there is no universally accepted mechanism for performing cross-\nvalidation or validating results on an independent data set. The reason\nfor this difference is simple. If we fit a predictive model using a supervised\nlearning technique, then it is possible tocheck our workby seeing how\nwell our model predicts the responseY on observations not used in fitting\nthe model. However, in unsupervised learning, there is no way to check our\nwork because we don\u2019t know the true answer\u2014the problem is unsupervised. Techniques for unsupervised learning are of growing importance in a\nnumber of fields. A cancer researcher might assay gene expression levels in\n100 patients with breast cancer. He or she might then look for subgroups\namong the breast cancer samples, or among the genes, in order to obtain\na better understanding of the disease. An online shopping site might try\nto identify groups of shoppers with similar browsing and purchase histo-\nries, as well as items that are of particular interest to the shoppers within\neach group. Then an individual shopper can be preferentially shown the\nitems in which he or she is particularly likely to be interested, based on\nthe purchase histories of similar shoppers. A search engine might choose\nwhich search results to display to a particular individual based on the click\nhistories of other individuals with similar search patterns. These statistical\nlearning tasks, and many more, can be performed via unsupervised learning\ntechniques. 12.2 Principal Components Analysis\nPrincipal componentsare discussed in Section6.3.1 in the context of\nprincipal components regression. When faced with a large set of corre-\nlated variables, principal components allow us to summarize this set with\na smaller number of representative variables that collectively explain most\nof the variability in the original set. The principal component directions\nare presented in Section6.3.1 as directions in feature space along which\nthe original data arehighly variable. These directions also define lines and\nsubspaces that areas close as possibleto the data cloud. To perform\n\n12.2 Principal Components Analysis 497\nprincipal components regression, we simply use principal components as\npredictors in a regression model in place of the original larger set of vari-\nables. Principal components analysis(PCA) refers to the process by which prin-principal\ncomponents\nanalysis\ncipal components are computed, and the subsequent use of these compo-\nnents in understanding the data. PCA is an unsupervised approach, since\nit involves only a set of featuresX1,X 2,...,X p, and no associated response\nY . Apart from producing derived variables for use in supervised learning\nproblems, PCA also serves as a tool for data visualization (visualization of\nthe observations or visualization of the variables). It can also be used as a\ntool for data imputation \u2014 that is, for filling in missing values in a data\nmatrix. We now discuss PCA in greater detail, focusing on the use of PCA as\na tool for unsupervised data exploration, in keeping with the topic of this\nchapter. 12.2.1 What Are Principal Components? Suppose that we wish to visualizen observations with measurements on a\nset ofp features, X1,X 2,...,X p, as part of an exploratory data analysis. We could do this by examining two-dimensional scatterplots of the data,\neach of which contains then observations\u2019 measurements on two of the\nfeatures. However, there are\n(p\n2\n)\n= p(p\u22121)/2 such scatterplots; for example,\nwith p = 10 there are 45 plots! Ifp is large, then it will certainly not be\npossible to look at all of them; moreover, most likely none of them will\nbe informative since they each contain just a small fraction of the total\ninformation present in the data set. Clearly, a better method is required to\nvisualize then observations whenp is large. In particular, we would like to\nfind a low-dimensional representation of the data that captures as much of\ntheinformationaspossible.Forinstance,ifwecanobtainatwo-dimensional\nrepresentation of the data that captures most of the information, then we\ncan plot the observations in this low-dimensional space. PCA provides a tool to do just this. It finds a low-dimensional represen-\ntation of a data set that contains as much as possible of the variation. The\nidea is that each of then observations lives inp-dimensional space, but not\nall of these dimensions are equally interesting. PCA seeks a small number\nof dimensions that are as interesting as possible, where the concept ofin-\nteresting is measured by the amount that the observations vary along each\ndimension. Each of the dimensions found by PCA is a linear combination\nof thep features. We now explain the manner in which these dimensions,\nor principal components, are found. The first principal componentof a set of featuresX1,X 2,...,X p is the\nnormalized linear combination of the features\nZ1 = \u03c611X1 + \u03c621X2 + \u00b7\u00b7\u00b7 + \u03c6p1Xp (12.1)\n\n498 12. Unsupervised Learning\nthat has the largest variance. Bynormalized, we mean that\u2211 p\nj=1 \u03c62\nj1 =1 . We refer to the elements\u03c611,..., \u03c6p1 as theloadings of the first principalloading\ncomponent; together, the loadings make up the principal component load-\ning vector,\u03c61 =( \u03c611 \u03c621 ... \u03c6p1)T . We constrain the loadings so that\ntheir sum of squares is equal to one, since otherwise setting these elements\nto be arbitrarily large in absolute value could result in an arbitrarily large\nvariance. Given ann \u00d7 p data setX, how do we compute the first principal com-\nponent? Since we are only interested in variance, we assume that each of\nthe variables inX has been centered to have mean zero (that is, the col-\numn means ofX are zero). We then look for the linear combination of the\nsample feature values of the form\nzi1 = \u03c611xi1 + \u03c621xi2 + \u00b7\u00b7\u00b7 + \u03c6p1xip (12.2)\nthathaslargestsamplevariance,subjecttotheconstraintthat \u2211 p\nj=1 \u03c62\nj1=1. In other words, the first principal component loading vector solves the op-\ntimization problem\nmaximize\u03c611,...,\u03c6p1\n\u23a7\n\u23aa\u23a8\n\u23aa\u23a9\n1\nn\nn\u2211\ni=1\n\u239b\n\u239d\np\u2211\nj=1\n\u03c6j1xij\n\u239e\n\u23a0\n2\u23ab\n\u23aa\u23ac\n\u23aa\u23ad\nsubject to\np\u2211\nj=1\n\u03c62\nj1 =1 . (12.3)\nFrom (12.2) we can write the objective in (12.3) as1\nn\n\u2211 n\ni=1 z2\ni1. Since\n1\nn\n\u2211 n\ni=1 xij =0 , the average of thez11,...,z n1 will be zero as well. Hence\nthe objective that we are maximizing in (12.3) is just the sample variance of\nthe n values ofzi1. We refer toz11,...,z n1 as thescoresof the first princi-score\npal component. Problem (12.3) can be solved via aneigen decomposition, eigen decom-\npositiona standard technique in linear algebra, but the details are outside of the\nscope of this book.1\nThere is a nice geometric interpretation of the first principal component. The loading vector\u03c61 with elements\u03c611, \u03c621,..., \u03c6p1 defines a direction in\nfeature space along which the data vary the most. If we project then data\npoints x1,...,x n onto this direction, the projected values are the princi-\npal component scoresz11,...,z n1 themselves. For instance, Figure6.14 on\npage 253 displays the first principal component loading vector (green solid\nline) on an advertising data set. In these data, there are only two features,\nand so the observations as well as the first principal component loading\nvector can be easily displayed. As can be seen from (6.19), in that data set\n\u03c611 =0 .839 and \u03c621 =0 .544. After the first principal componentZ1 of the features has been deter-\nmined, we can find the second principal componentZ2. The second princi-\n1As an alternative to the eigen decomposition, a related technique called the singular\nvalue decomposition can be used. This will be explored in the lab at the end of this\nchapter. 12.2 Principal Components Analysis 499\npal component is the linear combination ofX1,...,X p that has maximal\nvariance out of all linear combinations that areuncorrelatedwith Z1. The\nsecond principal component scoresz12,z 22,...,z n2 take the form\nzi2 = \u03c612xi1 + \u03c622xi2 + \u00b7\u00b7\u00b7 + \u03c6p2xip, (12.4)\nwhere \u03c62 is the second principal component loading vector, with elements\n\u03c612, \u03c622,..., \u03c6p2. It turns out that constrainingZ2 to be uncorrelated with\nZ1 is equivalent to constraining the direction\u03c62 to be orthogonal (perpen-\ndicular) to the direction\u03c61. In the example in Figure6.14, the observations\nlie in two-dimensional space (sincep =2 ), and so once we have found\u03c61,\nthere is only one possibility for\u03c62, which is shown as a blue dashed line. (From Section6.3.1, we know that\u03c612 =0 .544 and \u03c622 = \u22120.839.) But in\na larger data set withp> 2 variables, there are multiple distinct principal\ncomponents, and they are defined in a similar manner. To find\u03c62, we solve\na problem similar to (12.3) with\u03c62 replacing \u03c61, and with the additional\nconstraint that\u03c62 is orthogonal to\u03c61.2\nOnce we have computed the principal components, we can plot them\nagainst each other in order to produce low-dimensional views of the data. For instance, we can plot the score vectorZ1 against Z2, Z1 against Z3,\nZ2 against Z3, and so forth. Geometrically, this amounts to projecting the\noriginal data down onto the subspace spanned by\u03c61, \u03c62, and\u03c63, and\nplotting the projected points. We illustrate the use of PCA on theUSArrests data set. For each of the\n50 states in the United States, the data set contains the number of arrests\nper 100, 000 residents for each of three crimes:Assault, Murder, andRape. We also recordUrbanPop (the percent of the population in each state living\nin urban areas). The principal component score vectors have lengthn = 50,\nand the principal component loading vectors have lengthp =4 . PCA was\nperformedafterstandardizingeachvariabletohavemeanzeroandstandard\ndeviation one. Figure12.1 plots the first two principal components of these\ndata. The figure represents both the principal component scores and the\nloading vectors in a singlebiplot display. The loadings are also given inbiplot\nTable 12.2.1. In Figure12.1, we see that the first loading vector places approximately\nequal weight onAssault, Murder, andRape, but with much less weight on\nUrbanPop.Hencethiscomponentroughlycorrespondstoameasureofoverall\nrates of serious crimes. The second loading vector places most of its weight\non UrbanPop and much less weight on the other three features. Hence, this\ncomponent roughly corresponds to the level of urbanization of the state. Overall, we see that the crime-related variables (Murder, Assault, andRape)\narelocatedclosetoeachother,andthatthe UrbanPopvariableisfarfromthe\n2On a technical note, the principal component directions\u03c61, \u03c62, \u03c63,... are given\nby the ordered sequence of eigenvectors of the matrixXT X, and the variances of the\ncomponents are the eigenvalues.\n\nUnsupervised Learning\nExamples of genres include Romance, Western, and Action. Principal component models similar to Algorithm12.1 are at the heart\nof many recommender systems. Although the data matrices involved are\ntypically massive, algorithms have been developed that can exploit the high\nlevel of missingness in order to perform e\ufb00icient computations. 12.4 Clustering Methods\nClustering refers to a very broad set of techniques for findingsubgroups, orclustering\nclusters, in a data set. When we cluster the observations of a data set, we\nseek to partition them into distinct groups so that the observations within\neach group are quite similar to each other, while observations in different\ngroups are quite different from each other. Of course, to make this concrete,\nwe must define what it means for two or more observations to besimilar\nor different. Indeed, this is often a domain-specific consideration that must\nbe made based on knowledge of the data being studied. For instance, suppose that we have a set ofn observations, each withp\nfeatures. Then observations could correspond to tissue samples for patients\nwith breast cancer, and thep features could correspond to measurements\ncollected for each tissue sample; these could be clinical measurements, such\nas tumor stage or grade, or they could be gene expression measurements. We may have a reason to believe that there is some heterogeneity among\nthe n tissue samples; for instance, perhaps there are a few differentun-\nknown subtypes of breast cancer. Clustering could be used to find these\nsubgroups. This is an unsupervised problem because we are trying to dis-\ncover structure\u2014in this case, distinct clusters\u2014on the basis of a data set. The goal in supervised problems, on the other hand, is to try to predict\nsome outcome vector such as survival time or response to drug treatment. Both clustering and PCA seek to simplify the data via a small number\nof summaries, but their mechanisms are different:\n\u2022 PCA looks to find a low-dimensional representation of the observa-\ntions that explain a good fraction of the variance;\n\u2022 Clustering looks to find homogeneous subgroups among the observa-\ntions. Another application of clustering arises in marketing. We may have ac-\ncess to a large number of measurements (e.g. median household income,\noccupation, distance from nearest urban area, and so forth) for a large\nnumber of people. Our goal is to performmarket segmentationby identify-\ning subgroups of people who might be more receptive to a particular form\nof advertising, or more likely to purchase a particular product. The task of\nperforming market segmentation amounts to clustering the people in the\ndata set. 12.4 Clustering Methods 515\nSince clustering is popular in many fields, there exist a great num-\nber of clustering methods. In this section we focus on perhaps the two\nbest-known clustering approaches:K-means clusteringand hierarchical K-means\nclusteringclustering. InK-means clustering, we seek to partition the observations\nhierarchical\nclustering\ninto a pre-specified number of clusters. On the other hand, in hierarchical\nclustering, we do not know in advance how many clusters we want; in fact,\nwe end up with a tree-like visual representation of the observations, called\na dendrogram, that allows us to view at once the clusterings obtained fordendrogram\neach possible number of clusters, from1 to n. There are advantages and\ndisadvantages to each of these clustering approaches, which we highlight in\nthis chapter. In general, we can cluster observations on the basis of the features in\norder to identify subgroups among the observations, or we can cluster fea-\ntures on the basis of the observations in order to discover subgroups among\nthe features. In what follows, for simplicity we will discuss clustering obser-\nvations on the basis of the features, though the converse can be performed\nby simply transposing the data matrix. 12.4.1 K-Means Clustering\nK-means clustering is a simple and elegant approach for partitioning a\ndata set intoK distinct, non-overlapping clusters. To performK-means\nclustering, we must first specify the desired number of clustersK; then the\nK-means algorithm will assign each observation to exactly one of theK\nclusters. Figure12.7 shows the results obtained from performingK-means\nclustering on a simulated example consisting of150 observations in two\ndimensions, using three different values ofK. The K-means clustering procedure results from a simple and intuitive\nmathematicalproblem.Webeginbydefiningsomenotation.Let C1,...,C K\ndenote sets containing the indices of the observations in each cluster. These\nsets satisfy two properties:\n1. C1 \u222a C2 \u222a \u00b7\u00b7\u00b7 \u222a CK = {1,...,n }. In other words, each observation\nbelongs to at least one of theK clusters. 2. Ck \u2229 Ck\u2032 = \u2205 for allk \u0338= k\u2032. In other words, the clusters are non-\noverlapping: no observation belongs to more than one cluster. For instance, if theith observation is in thekth cluster, theni \u2208 Ck. The\nideabehind K-meansclusteringisthata goodclusteringisoneforwhichthe\nwithin-cluster variationis as small as possible. The within-cluster variation\nfor clusterCk is a measureW(Ck) of the amount by which the observations\nwithin a cluster differ from each other. Hence we want to solve the problem\nminimizeC1,...,CK\n{ K\u2211\nk=1\nW(Ck)\n}\n. (12.15)\n\n516 12. Unsupervised Learning\nK=2 K=3 K=4\nFIGURE 12.7.A simulated data set with 150 observations in two-dimensional\nspace. Panels show the results of applyingK-means clustering with different values\nof K, the number of clusters. The color of each observation indicates the cluster\nto which it was assigned using theK-means clustering algorithm. Note that there\nis no ordering of the clusters, so the cluster coloring is arbitrary. These cluster\nlabels were not used in clustering; instead, they are the outputs of the clustering\nprocedure. In words, this formula says that we want to partition the observations into\nK clusters such that the total within-cluster variation, summed over allK\nclusters, is as small as possible. Solving (12.15) seems like a reasonable idea, but in order to make it\nactionable we need to define the within-cluster variation. There are many\npossible ways to define this concept, but by far the most common choice\ninvolvessquared Euclidean distance. That is, we define\nW(Ck)= 1\n|Ck|\n\u2211\ni,i\u2032\u2208Ck\np\u2211\nj=1\n(xij \u2212 xi\u2032j)2, (12.16)\nwhere |Ck| denotes the number of observations in thekth cluster. In other\nwords, the within-cluster variation for thekth cluster is the sum of all of\nthe pairwise squared Euclidean distances between the observations in the\nkth cluster, divided by the total number of observations in thekth cluster. Combining (12.15) and (12.16) gives the optimization problem that defines\nK-means clustering,\nminimizeC1,...,CK\n\u23a7\n\u23a8\n\u23a9\nK\u2211\nk=1\n1\n|Ck|\n\u2211\ni,i\u2032\u2208Ck\np\u2211\nj=1\n(xij \u2212 xi\u2032j)2\n\u23ab\n\u23ac\n\u23ad. (12.17)\nNow, we would like to find an algorithm to solve (12.17)\u2014that is, a\nmethod to partition the observations intoK clusters such that the objective\n\n12.4 Clustering Methods 517\nof (12.17) is minimized. This is in fact a very di\ufb00icult problem to solve\nprecisely, since there are almostKn ways to partitionn observations intoK\nclusters. This is a huge number unlessK and n are tiny! Fortunately, a very\nsimple algorithm can be shown to provide a local optimum\u2014apretty good\nsolution\u2014to theK-means optimization problem (12.17). This approach is\nlaid out in Algorithm12.2. Algorithm 12.2K-Means Clustering\n1. Randomly assign a number, from1 to K, to each of the observations. These serve as initial cluster assignments for the observations. 2. Iterate until the cluster assignments stop changing:\n(a) For each of theK clusters, compute the clustercentroid. The\nkth cluster centroid is the vector of thep feature means for the\nobservations in thekth cluster. (b) Assign each observation to the cluster whose centroid is closest\n(where closest is defined using Euclidean distance). Algorithm12.2isguaranteedtodecreasethevalueoftheobjective( 12.17)\nat each step. To understand why, the following identity is illuminating:\n1\n|Ck|\n\u2211\ni,i\u2032\u2208Ck\np\u2211\nj=1\n(xij \u2212 xi\u2032j)2 =2\n\u2211\ni\u2208Ck\np\u2211\nj=1\n(xij \u2212 \u00afxkj)2, (12.18)\nwhere \u00afxkj = 1\n|Ck|\n\u2211\ni\u2208Ck\nxij is the mean for featurej in clusterCk. In Step 2(a) the cluster means for each feature are the constants that\nminimize the sum-of-squared deviations, and in Step 2(b), reallocating the\nobservations can only improve (12.18). This means that as the algorithm\nis run, the clustering obtained will continually improve until the result no\nlonger changes; the objective of (12.17) will never increase. When the result\nno longer changes, alocal optimumhas been reached. Figure12.8 shows\nthe progression of the algorithm on the toy example from Figure12.7. K-means clustering derives its name from the fact that in Step 2(a), the\ncluster centroids are computed as the mean of the observations assigned to\neach cluster. Because theK-means algorithm finds a local rather than a global opti-\nmum, the results obtained will depend on the initial (random) cluster as-\nsignment of each observation in Step 1 of Algorithm12.2. For this reason,\nit is important to run the algorithm multiple times from different random\ninitial configurations. Then one selects thebest solution, i.e. that for which\nthe objective (12.17) is smallest. Figure12.9 shows the local optima ob-\ntained by runningK-means clustering six times using six different initial\n\n518 12. Unsupervised Learning\nData Step 1 Iteration 1, Step 2a\nIteration 1, Step 2bIteration 2, Step 2aFinal Results\nFIGURE 12.8. The progress of the K-means algorithm on the example of\nFigure 12.7 with K=3. Top left:the observations are shown.Top center:in\nStep 1 of the algorithm, each observation is randomly assigned to a cluster.Top\nright: in Step 2(a), the cluster centroids are computed. These are shown as large\ncolored disks. Initially the centroids are almost completely overlapping because\nthe initial cluster assignments were chosen at random.Bottom left:in Step 2(b),\neach observation is assigned to the nearest centroid.Bottom center:Step 2(a) is\nonce again performed, leading to new cluster centroids.Bottom right:the results\nobtained after ten iterations. cluster assignments, using the toy data from Figure12.7. In this case, the\nbest clustering is the one with an objective value of 235.8. As we have seen, to performK-means clustering, we must decide how\nmany clusters we expect in the data. The problem of selectingK is far from\nsimple. This issue, along with other practical considerations that arise in\nperforming K-means clustering, is addressed in Section12.4.3. 12.4 Clustering Methods 519\n320.9 235.8 235.8\n235.8 235.8 310.9\nFIGURE 12.9. K-means clustering performed six times on the data from\nFigure 12.7 with K =3 , each time with a different random assignment of the\nobservations in Step 1 of theK-means algorithm. Above each plot is the value\nof the objective (12.17). Three different local optima were obtained, one of which\nresulted in a smaller value of the objective and provides better separation between\nthe clusters. Those labeled in red all achieved the same best solution, with an\nobjective value of 235.8. 12.4.2 Hierarchical Clustering\nOne potential disadvantage ofK-means clustering is that it requires us to\npre-specify the number of clustersK. Hierarchical clusteringis an alter-\nnative approach which does not require that we commit to a particular\nchoice ofK. Hierarchical clustering has an added advantage overK-means\nclustering in that it results in an attractive tree-based representation of the\nobservations, called adendrogram. In this section, we describebottom-up or agglomerative clustering. bottom-up\nagglomerativeThis is the most common type of hierarchical clustering, and refers to\n\n520 12. Unsupervised Learning\n\u22126 \u22124 \u221220 2\n\u221220 2 4\nX1\nX2\nFIGURE 12.10. Forty-five observations generated in two-dimensional space. In reality there are three distinct classes, shown in separate colors. However, we\nwill treat these class labels as unknown and will seek to cluster the observations\nin order to discover the classes from the data. the fact that a dendrogram (generally depicted as an upside-down tree; see\nFigure 12.11) is built starting from the leaves and combining clusters up to\nthe trunk. We will begin with a discussion of how to interpret a dendrogram\nand then discuss how hierarchical clustering is actually performed\u2014that is,\nhow the dendrogram is built. Interpreting a Dendrogram\nWe begin with the simulated data set shown in Figure12.10, consisting of\n45 observations in two-dimensional space. The data were generated from a\nthree-class model; the true class labels for each observation are shown in\ndistinct colors. However, suppose that the data were observed without the\nclass labels, and that we wanted to perform hierarchical clustering of the\ndata. Hierarchical clustering (with complete linkage, to be discussed later)\nyields the result shown in the left-hand panel of Figure12.11. How can we\ninterpret this dendrogram? In the left-hand panel of Figure12.11, eachleaf of the dendrogram rep-\nresents one of the45 observations in Figure12.10. However, as we move\nup the tree, some leaves begin tofuse into branches.\n\nIn the supervised learning setting, we typically\nhave access to a set ofp features X1,X 2,...,X p, measured onn obser-\nvations, and a responseY also measured on those samen observations. The goal is then to predictY using X1,X 2,...,X p. This chapter will instead focus onunsupervised learning, a set of sta-\ntistical tools intended for the setting in which we have only a set of fea-\ntures X1,X 2,...,X p measured onn observations. We are not interested\nin prediction, because we do not have an associated response variableY .\n\nl a b s ,\nmain = \"Hier. Clust . on First Five Score Vectors \")\n> table (cutree (hc.out, 4), nci.labs)\nNot surprisingly, these results are different from the ones that we obtained\nwhen we performed hierarchical clustering on the full data set. Sometimes\nperforming clustering on the first few principal component score vectors\ncan give better results than performing clustering on the full data. In this\nsituation, we might view the principal component step as one of denois-\ning the data. We could also performK-means clustering on the first few\nprincipal component score vectors rather than the full data set. 546 12. Unsupervised Learning\n12.6 Exercises\nConceptual\n1. This problem involves theK-means clustering algorithm. (a) Prove (12.18). (b) On the basis of this identity, argue that theK-means clustering\nalgorithm (Algorithm12.2) decreases the objective (12.17) at\neach iteration. 2. Suppose that we have four observations, for which we compute a\ndissimilarity matrix, given by\n\u23a1\n\u23a2\u23a2\u23a3\n0.30 .40 .7\n0.3 0.50 .8\n0.40 .5 0.45\n0.70 .80 .45\n\u23a4\n\u23a5\u23a5\u23a6. For instance, the dissimilarity between the first and second obser-\nvations is 0.3, and the dissimilarity between the second and fourth\nobservations is 0.8. (a) On the basis of this dissimilarity matrix, sketch the dendrogram\nthat results from hierarchically clustering these four observa-\ntions using complete linkage. Be sure to indicate on the plot the\nheight at which each fusion occurs, as well as the observations\ncorresponding to each leaf in the dendrogram. (b) Repeat (a), this time using single linkage clustering. (c) Suppose that we cut the dendrogram obtained in (a) such that\ntwo clusters result. Which observations are in each cluster? (d) Suppose that we cut the dendrogram obtained in (b) such that\ntwo clusters result. Which observations are in each cluster? (e) It is mentioned in the chapter that at each fusion in the den-\ndrogram, the position of the two clusters being fused can be\nswappedwithoutchangingthemeaningofthedendrogram.Draw\na dendrogram that is equivalent to the dendrogram in (a), for\nwhich two or more of the leaves are repositioned, but for which\nthe meaning of the dendrogram is the same. 3.",
    "topic_11": "578 13. Multiple Testing\nAlgorithm 13.4Plug-In FDR for a Two-SampleT-Test\n1. Select a thresholdc, wherec> 0. 2. For j =1 ,...,m :\n(a) Compute T(j), the two-samplet-statistic (13.11) for the null\nhypothesis H0j on the basis of the original data,x(j)\n1 ,...,x (j)\nnX\nand y(j)\n1 ,...,y (j)\nnY . (b) For b =1 ,...,B , whereB is a large number (e.g.B = 10,000):\ni. Permute thenX +nY observations at random. Call the first\nnX observations x\u2217(j)\n1 ,...,x \u2217(j)\nnX , and call the remaining ob-\nservations y\u2217(j)\n1 ,...,y \u2217(j)\nnY . ii. Compute (13.11) on the permuted datax\u2217(j)\n1 ,...,x \u2217(j)\nnX and\ny\u2217(j)\n1 ,...,y \u2217(j)\nnY , and call the resultT(j),\u2217b. 3. Compute R = \u2211 m\nj=1 1(|T(j)|\u2265c). 4. Compute \u02c6V =\n\u2211B\nb=1\n\u2211m\nj=1 1(|T(j),\u2217b|\u2265c)\nB . 5. The estimated FDR associated with the thresholdc is \u02c6V /R. to estimate the FDR by pluggingR into the denominator and an estimate\nfor E(V ) into the numerator. We apply the re-sampling approach to the FDR from Algorithm13.4,\nas well as the Benjamini\u2013Hochberg approach from Algorithm13.2 using\ntheoretical p-values, to them =2 ,308 genes in theKhan dataset. Results are\nshowninFigure 13.9.Weseethatforagivennumberofrejectedhypotheses,\nthe estimated FDRs are almost identical for the two methods. We began this section by noting that in order to control the FDR form\nhypothesis tests using a re-sampling approach, we could simply computem\nre-sampling p-values as in Section13.5.1, and then apply the Benjamini\u2013\nHochberg procedure of Section13.4.2 to thesep-values. It turns out that if\nwe define thejth re-samplingp-value as\npj =\n\u2211 m\nj\u2032=1\n\u2211 B\nb=1 1(|T\u2217b\nj\u2032 |\u2265|Tj|)\nBm (13.14)\nfor j =1 ,...,m , instead of as in (13.12), then applying the Benjamini\u2013\nHochberg procedure to these re-sampledp-values isexactly equivalent to\nAlgorithm 13.4. Note that (13.14) is an alternative to (13.12) that pools\nthe information across allm hypothesis tests in approximating the null\ndistribution. 13.5 A Re-Sampling Approach top-Values and False Discovery Rates 579\n0 500 1000 1500 2000\n0.00.20.40.60.81.0\nNumber of Rejections\nFalse Discovery Rate\nFIGURE 13.9.For j =1 ,...,m =2 ,308, we tested the null hypothesis that for\nthe jth gene in theKhandataset, the mean expression in Burkitt\u2019s lymphoma equals\nthe mean expression in rhabdomyosarcoma. For each value ofk from1 to2,308, the\ny-axis displays the estimated FDR associated with rejecting the null hypotheses\ncorresponding to thek smallest p-values. The orange dashed curve shows the\nFDR obtained using the Benjamini\u2013Hochberg procedure, whereas the blue solid\ncurve shows the FDR obtained using the re-sampling approach of Algorithm13.4,\nwith B = 10,000. There is very little difference between the two FDR estimates. According to either estimate, rejecting the null hypothesis for the 500 genes with\nthe smallestp-values corresponds to an FDR of around 17.7%. 13.5.3 When Are Re-Sampling Approaches Useful? In Sections13.5.1 and 13.5.2, we considered testing null hypotheses of the\nform H0 : E(X) = E(Y ) using a two-samplet-statistic (13.11), for which we\napproximatedthenulldistributionviaare-samplingapproach.Wesawthat\nusing the re-sampling approach gave us substantially different results from\nusingthetheoretical p-valueapproachinFigure 13.8,butnotinFigure 13.7. In general, there are two settings in which a re-sampling approach is\nparticularly useful:\n1. Perhaps no theoretical null distribution is available.\n\nIn Chapter12, we consider theunsupervised setting in which we have\ninput variables but no output variable. In particular, we presentprinci-\npal components analysis, K-means clustering, andhierarchical clustering. Finally, in Chapter13 we cover the very important topic of multiple hy-\npothesis testing. 1. Introduction 13\nAt the end of each chapter, we present one or moreR lab sections in\nwhich we systematically work through applications of the various methods\ndiscussed in that chapter. These labs demonstrate the strengths and weak-\nnesses of the various approaches, and also provide a useful reference for the\nsyntax required to implement the various methods. The reader may choose\nto work through the labs at their own pace, or the labs may be the focus of\ngroup sessions as part of a classroom environment. Within eachR lab, we\npresent the results that we obtained when we performed the lab at the time\nof writing this book. However, new versions ofR are continuously released,\nand over time, the packages called in the labs will be updated. Therefore,\nin the future, it is possible that the results shown in the lab sections may\nno longer correspond precisely to the results obtained by the reader who\nperforms the labs. As necessary, we will post updates to the labs on the\nbook website. We use the\n symbol to denote sections or exercises that contain more\nchallenging concepts. These can be easily skipped by readers who do not\nwish to delve as deeply into the material, or who lack the mathematical\nbackground. Data Sets Used in Labs and Exercises\nIn this textbook, we illustrate statistical learning methods using applica-\ntions from marketing, finance, biology, and other areas. TheISLR2 package\navailable on the book website and CRAN contains a number of data sets\nthat are required in order to perform the labs and exercises associated with\nthis book. One other data set is part of the baseR distribution. Table1.1\ncontains a summary of the data sets required to perform the labs and ex-\nercises. A couple of these data sets are also available as text files on the\nbook website, for use in Chapter2. 14 1. Introduction\nName Description\nAuto Gas mileage, horsepower, and other information for cars. Bikeshare Hourly usage of a bike sharing program in Washington, DC. Boston Housing values and other information about Boston census tracts. BrainCancer Survival times for patients diagnosed with brain cancer. Caravan Information about individuals offered caravan insurance. Carseats Information about car seat sales in 400 stores. College Demographic characteristics, tuition, and more for USA colleges.\n\n(g) Comment on the results obtained. How accurately can we pre-\ndict the number of college applications received? Is there much\ndifference among the test errors resulting from these five ap-\nproaches? 10. Wehaveseenthatasthenumberoffeaturesusedinamodelincreases,\nthe training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set. (a) Generate a data set withp = 20 features, n =1 ,000 observa-\ntions, and an associated quantitative response vector generated\naccording to the model\nY = X\u03b2 + \u03f5,\nwhere \u03b2 has some elements that are exactly equal to zero. (b) Splityourdatasetintoatrainingsetcontaining100observations\nand a test set containing 900 observations. (c) Perform best subset selection on the training set, and plot the\ntraining set MSE associated with the best model of each size. (d) Plot the test set MSE associated with the best model of each\nsize. (e) For which model size does the test set MSE take on its minimum\nvalue?Commentonyourresults.Ifittakesonitsminimumvalue\nfor a model containing only an intercept or a model containing\nall of the features, then play around with the way that you are\ngenerating the data in (a) until you come up with a scenario in\nwhich the test set MSE is minimized for an intermediate model\nsize. 288 6. Linear Model Selection and Regularization\n(f) How does the model at which the test set MSE is minimized\ncompare to the true model used to generate the data? Comment\non the coe\ufb00icient values. (g) Create a plot displaying\n\u221a \u2211 p\nj=1(\u03b2j \u2212 \u02c6\u03b2r\nj )2 for a range of values\nof r, where\u02c6\u03b2r\nj is thejth coe\ufb00icient estimate for the best model\ncontaining r coe\ufb00icients. Comment on what you observe. How\ndoes this compare to the test MSE plot from (d)? 11. We will now try to predict per capita crime rate in theBoston data\nset. (a) Try out some of the regression methods explored in this chapter,\nsuch as best subset selection, the lasso, ridge regression, and\nPCR. Present and discuss results for the approaches that you\nconsider. (b) Propose a model (or set of models) that seem to perform well on\nthis data set, and justify your answer. Make sure that you are\nevaluating model performance using validation set error, cross-\nvalidation, or some other reasonable alternative, as opposed to\nusing training error. (c) Does your chosen model involve all of the features in the data\nset?\n\nShe tells 1,024 (1,024 = 210) potential new clients that she can\ncorrectly predict whether Apple\u2019s stock price will increase or decrease for 10\ndays running. There are210 possibilities for how Apple\u2019s stock price might\nchange over the course of these 10 days. Therefore, she emails each client\none of these210 possibilities. The vast majority of her potential clients\nwill find that the stockbroker\u2019s predictions are no better than chance (and\nmany will find them to be even worse than chance). But a broken clock is\nright twice a day, and one of her potential clients will be really impressed\nto find that her predictions were correct for all 10 of the days! And so the\nstockbroker gains a new client. What happened here? Does the stockbroker have any actual insight into\nwhether Apple\u2019s stock price will increase or decrease? No. How, then, did\nshe manage to predict Apple\u2019s stock price perfectly for 10 days running? 13.3 The Family-Wise Error Rate 559\nThe answer is that she made a lot of guesses, and one of them happened\nto be exactly right. How does this relate to multiple testing? Suppose that we flip 1,024 fair\ncoins11 ten times each. Then we would expect (on average) one coin to\ncome up all tails. (There\u2019s a1/210 =1 /1,024 chance that any single coin\nwill come up all tails. So if we flip1,024 coins, then we expect one coin to\ncome up all tails, on average.) If one of our coins comes up all tails, then\nwe might therefore conclude that this particular coin is not fair. In fact, a\nstandard hypothesis test for the null hypothesis that this particular coin\nis fair would lead to ap-value below0.002!12 But it would be incorrect to\nconclude that the coin is not fair: in fact, the null hypothesis holds, and we\njust happen to have gotten ten tails in a row by chance. These examples illustrate the main challenge ofmultiple testing: whenmultiple\ntestingtesting a huge number of null hypotheses, we are bound to get some very\nsmallp-valuesbychance. If wemake a decision about whether to reject each\nnull hypothesis without accounting for the fact that we have performed a\nvery large number of tests, then we may end up rejecting a great number\nof true null hypotheses \u2014 that is, making a large number of Type I errors. How severe is the problem? Recall from the previous section that if we\nreject a single null hypothesis,H0, if itsp-value is less than, say,\u03b1 =0 .01,\nthen there is a 1% chance of making a false rejection ifH0 is in fact true. Now what if we testm null hypotheses,H01,...,H 0m, all of which are true? There\u2019s a 1% chance of rejecting any individual null hypothesis; therefore,\nwe expect to falsely reject approximately0.01 \u00d7m null hypotheses. Ifm =\n10,000, then that means that we expect to falsely reject100 null hypotheses\nby chance! That is alot of Type I errors. Thecruxoftheissueisasfollows:rejectinganullhypothesisifthe p-value\nis below\u03b1 controls the probability of falsely rejectingthat null hypothesis\nat level\u03b1. However, if we do this form null hypotheses, then the chance of\nfalsely rejectingat least one of them null hypothesesis quite a bit higher! We will investigate this issue in greater detail, and pose a solution to it, in\nSection 13.3. 13.3 The Family-Wise Error Rate\nIn the following sections, we will discuss testing multiple hypotheses while\ncontrolling the probability of making at least one Type I error. 11A fair coinis one that has an equal chance of landing heads or tails. 12Recall that thep-value is the probability of observing data at least this extreme,\nunder the null hypothesis. If the coin is fair, then the probability of observing at least\nten tails is(1/2)10 =1 /1,024 < 0.001. Thep-value is therefore2/1,024 < 0.002, since\nthis is the probability of observing ten heads or ten tails. 560 13. Multiple Testing\nH0 is True H0 is False Total\nReject H0 V S R\nDo Not RejectH0 U W m \u2212 R\nTotal m0 m \u2212 m0 m\nTABLE 13.2.A summary of the results of testingm null hypotheses. A given\nnull hypothesis is either true or false, and a test of that null hypothesis can either\nreject or fail to reject it. In practice, the individual values ofV , S, U, andW are\nunknown. However, we do have access toV + S = R and U + W = m \u2212R, which\nare the numbers of null hypotheses rejected and not rejected, respectively. 13.3.1 What is the Family-Wise Error Rate? Recall that the Type I error rate is the probability of rejectingH0 if H0 is\ntrue. Thefamily-wise error rate(FWER) generalizes this notion to the set-family-wise\nerror rateting ofm null hypotheses,H01,...,H 0m, and is defined as the probability\nof makingat least oneType I error. To state this idea more formally, con-\nsider Table13.2, which summarizes the possible outcomes when performing\nm hypothesis tests. Here,V represents the number of Type I errors (also\nknown as false positives or false discoveries),S the number of true posi-\ntives, U the number of true negatives, andW the number of Type II errors\n(also known as false negatives). Then the family-wise error rate is given by\nFWER = Pr(V \u2265 1). (13.3)\nA strategy of rejecting any null hypothesis for which thep-value is below\n\u03b1 (i.e. controlling the Type I error for each null hypothesis at level\u03b1) leads\nto a FWER of\nFWER(\u03b1)=1 \u2212 Pr(V = 0)\n=1 \u2212 Pr(do not falsely reject any null hypotheses)\n=1 \u2212 Pr\n(\u22c2m\nj=1 {do not falsely rejectH0j}\n)\n. (13.4)\nRecall from basic probability that if two eventsA and B are independent,\nthen Pr(A\u2229B) = Pr(A) Pr(B). Therefore, if we make the additional rather\nstrong assumptions that them tests are independent and that allm null\nhypotheses are true, then\nFWER(\u03b1)=1 \u2212\nm\u220f\nj=1\n(1 \u2212 \u03b1)=1 \u2212 (1 \u2212 \u03b1)m. (13.5)\nHence, if we test only one null hypothesis, then FWER(\u03b1)=1 \u2212(1\u2212\u03b1)1 =\n\u03b1, so the Type I error rate and the FWER are equal. However, if weperform\nm = 100independent tests, then FWER(\u03b1)=1 \u2212(1 \u2212\u03b1)100. For instance,\ntaking \u03b1 =0 .05 leads to a FWER of1 \u2212 (1 \u2212 0.05)100 =0 .994. In other\nwords, we are virtually guaranteed to make at least one Type I error! 13.3 The Family-Wise Error Rate 561\n1 2 5 10 20 50 100 200 500\n0.0 0.2 0.4 0.6 0.8 1.0\nNumber of Hypotheses\nFamily\u2212Wise Error Rate\n\u03b1=0.05\u03b1=0.01\u03b1=0.001\nFIGURE 13.2.The family-wise error rate, as a function of the number of\nhypotheses tested (displayed on the log scale), for three values of\u03b1: \u03b1 =0 .05\n(orange), \u03b1 =0 .01 (blue), and\u03b1 =0 .001 (purple). The dashed line indicates\n0.05. For example, in order to control the FWER at0.05 when testingm = 50\nnull hypotheses, we must control the Type I error for each null hypothesis at level\n\u03b1 =0 .001. Figure 13.2 displays (13.5) for various values ofm, the number of hy-\npotheses, and\u03b1, the Type I error. We see that setting\u03b1 =0 .05 results in\na high FWER even for moderatem. With\u03b1 =0 .01, we can test no more\nthan five null hypotheses before the FWER exceeds0.05. Only for very\nsmall values, such as\u03b1 =0 .001, do we manage to ensure a small FWER,\nat least for moderately-sizedm. We now briefly return to the example in Section13.1.1, in which we\nconsider testing a single null hypothesis of the formH0 : \u00b5t = \u00b5c using a\ntwo-sample t-statistic. Recall from Figure13.1 that in order to guarantee\nthat the Type I error does not exceed0.02, we decide whether or not to\nreject H0 using a cutpoint of2.33 (i.e. we rejectH0 if |T| \u2265 2.33). Now,\nwhat if we wish to test 10 null hypotheses using two-samplet-statistics,\ninstead of just one? We will see in Section13.3.2 that we can guarantee\nthat the FWER does not exceed0.02 by rejecting only null hypotheses\nfor which thep-value falls below0.002. This corresponds to a much more\nstringent cutpoint of3.09 (i.e. we should rejectH0j only if its test statistic\n|Tj| \u2265 3.09, forj =1 ,..., 10). In other words, controlling the FWER at\nlevel \u03b1 amounts to a much higher bar, in terms of evidence required to\nreject any given null hypothesis, than simply controlling the Type I error\nfor each null hypothesis at level\u03b1. 562 13. Multiple Testing\nManager Mean, \u00afx Standard Deviation,st -statistic p-value\nOne 3.0 7.4 2.86 0.006\nTwo -0.1 6.9 -0.10 0.918\nThree 2.8 7.5 2.62 0.012\nFour 0.5 6.7 0.53 0.601\nFive 0.3 6.8 0.31 0.756\nTABLE 13.3.The first two columns correspond to the sample mean and sample\nstandard deviation of the percentage excess return, overn = 50 months, for the\nfirst five managers in theFund dataset. The last two columns provide thet-statistic\n(\u221an \u00b7 \u00afX/S) and associatedp-value for testingH0j : \u00b5j =0 , the null hypothesis\nthat the (population) mean return for thejth hedge fund manager equals zero. 13.3.2 Approaches to Control the Family-Wise Error Rate\nIn this section, we briefly survey some approaches to control the FWER. We will illustrate these approaches on theFund dataset, which records the\nmonthly percentage excess returns for 2,000 fund managers overn = 50\nmonths.13 Table13.3 provides relevant summary statistics for the first five\nmanagers. We first present the Bonferroni method and Holm\u2019s step-down proce-\ndure, which are very general-purpose approaches for controlling the FWER\nthat can be applied whenevermp -values have been computed, regardless\nof the form of the null hypotheses, the choice of test statistics, or the\n(in)dependence of thep-values. We then briefly discuss Tukey\u2019s method\nand Scheff\u00e9\u2019s method in order to illustrate the fact that, in certain sit-\nuations, more specialized approaches for controlling the FWER may be\npreferable. The Bonferroni Method\nAs in the previous section, suppose we wish to testH01,...,H 0m. LetAj\ndenote the event that we make a Type I error for thejth null hypothesis,\nfor j =1 ,...,m . Then\nFWER = Pr(falsely reject at least one null hypothesis)\n= Pr(\u222am\nj=1Aj)\n\u2264\nm\u2211\nj=1\nPr(Aj). (13.6)\nIn (13.6), the inequality results from the fact that for any two eventsA\nand B, Pr(A \u222a B) \u2264 Pr(A) + Pr(B), regardless of whetherA and B are\n13Excess returns correspond to the additional return the fund manager achieves beyond\nthe market\u2019s overall return. So if the market increases by5% during a given period and\nthe fund manager achieves a7% return, theirexcess returnwould be7% \u2212 5% = 2%. 13.3 The Family-Wise Error Rate 563\nindependent. TheBonferroni method, orBonferroni correction, sets the\nthreshold for rejecting each hypothesis test to\u03b1/m, so thatPr(Aj) \u2264 \u03b1/m. Equation 13.6 implies that\nFWER(\u03b1/m) \u2264 m \u00d7 \u03b1\nm = \u03b1,\nso this procedure controls the FWER at level\u03b1. For instance, in order to\ncontrol the FWER at level0.1 while testingm = 100null hypotheses, the\nBonferroni procedure requires us to control the Type I error for each null\nhypothesis at level0.1/100 = 0.001, i.e. to reject all null hypotheses for\nwhich thep-value is below0.001. We now consider theFund dataset in Table13.3. If we control the Type\nI error at level\u03b1 =0 .05 for each fund manager separately, then we will\nconclude that the first and third managers have significantly non-zero ex-\ncess returns; in other words, we will rejectH01 : \u00b51 =0 and H03 : \u00b53 =0 . However, as discussed in previous sections, this procedure does not account\nfor the fact that we have tested multiple hypotheses, and therefore it will\nlead to a FWER greater than0.05. If we instead wish to control the FWER\nat level0.05, then, using a Bonferroni correction, we must control the Type\nI error for each individual manager at level\u03b1/m =0 .05/5=0 .01. Conse-\nquently, we will reject the null hypothesis only for the first manager, since\nthe p-values for all other managers exceed0.01. The Bonferroni correction\ngives us peace of mind that we have not falsely rejected too many null\nhypotheses, but for a price: we reject few null hypotheses, and thus will\ntypically make quite a few Type II errors. The Bonferroni correction is by far the best-known and most commonly-\nused multiplicity correction in all of statistics. Its ubiquity is due in large\npart to the fact that it is very easy to understand and simple to implement,\nand also from the fact that it successfully controls Type I error regardless\nof whether them hypothesis tests are independent. However, as we will see,\nit is typically neither the most powerful nor the best approach for multiple\ntesting correction. In particular, the Bonferroni correction can be quite\nconservative, in the sense that the true FWER is often quite a bit lower\nthan the nominal (or target) FWER; this results from the inequality in\n(13.6). By contrast, a less conservative procedure might allow us to control\nthe FWER while rejecting more null hypotheses, and therefore making\nfewer Type II errors. Holm\u2019s Step-Down Procedure\nHolm\u2019s method, also known as Holm\u2019s step-down procedure or the Holm\u2013Holm\u2019s\nmethodBonferroni method, is an alternative to the Bonferroni procedure. Holm\u2019s\nmethod controls the FWER, but it is less conservative than Bonferroni, in\nthesensethatitwillrejectmorenullhypotheses,typicallyresultinginfewer\nType II errors and hence greater power. The procedure is summarized in\nAlgorithm 13.1."
  }
}