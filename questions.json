{
  "questions": [
    {
      "question": "What is the primary goal of statistical learning methods when applied to training data?",
      "choices": {
        "A": "To optimize the theoretical properties of the model",
        "B": "To estimate the unknown function that relates inputs to outputs",
        "C": "To minimize the number of predictors used in the model",
        "D": "To maximize the complexity of the model for better predictions"
      },
      "correct_answer": "B",
      "explanation": "The primary goal of statistical learning methods is to estimate the unknown function f such that the response variable Y is approximately equal to the predicted value from the function f(X) for any observation (X, Y). This involves understanding the relationship between inputs and outputs.",
      "difficulty": -2.0,
      "category": "statistical learning",
      "level": "basic",
      "topic": "statistical learning",
      "source": "topic_0"
    },
    {
      "question": "Which of the following best describes a clustering problem?",
      "choices": {
        "A": "Predicting the output variable based on input variables",
        "B": "Grouping individuals based on observed characteristics without an output variable",
        "C": "Estimating the probability of an event occurring",
        "D": "Determining the direction of movement in the stock market"
      },
      "correct_answer": "B",
      "explanation": "Clustering involves grouping individuals based on observed characteristics when there is no corresponding output variable. It is a type of unsupervised learning where the goal is to identify similarities among data points.",
      "difficulty": -2.0,
      "category": "unsupervised learning",
      "level": "basic",
      "topic": "statistical learning",
      "source": "topic_0"
    },
    {
      "question": "Why might linear models be preferred over non-linear models in certain applications?",
      "choices": {
        "A": "Linear models always provide more accurate predictions",
        "B": "Linear models are easier to interpret and allow for simpler inference",
        "C": "Non-linear models are less flexible and cannot handle complex data",
        "D": "Linear models require more computational resources"
      },
      "correct_answer": "B",
      "explanation": "Linear models are often preferred because they are relatively simple and interpretable, making inference straightforward. While they may not always provide the most accurate predictions, their simplicity is advantageous in applications where interpretability is crucial.",
      "difficulty": -2.0,
      "category": "linear regression",
      "level": "basic",
      "topic": "statistical learning",
      "source": "topic_0"
    },
    {
      "question": "Why does the relationship between ice cream sales and shark attacks appear to be significant in a simple regression model?",
      "choices": {
        "A": "Ice cream sales directly cause shark attacks.",
        "B": "Both ice cream sales and shark attacks are influenced by a common variable, temperature.",
        "C": "Shark attacks increase the demand for ice cream.",
        "D": "There is a causal relationship between ice cream sales and shark attacks."
      },
      "correct_answer": "B",
      "explanation": "The apparent relationship is due to a confounding variable, temperature, which affects both ice cream sales and shark attacks. When temperature is accounted for, the relationship between ice cream sales and shark attacks is no longer significant.",
      "difficulty": -1.5,
      "category": "linear regression",
      "level": "basic",
      "topic": "linear regression",
      "source": "topic_1"
    },
    {
      "question": "What is the purpose of including temperature in the regression model of shark attacks and ice cream sales?",
      "choices": {
        "A": "To prove that ice cream sales cause shark attacks.",
        "B": "To eliminate the effect of temperature as a confounding variable.",
        "C": "To increase the number of predictors in the model.",
        "D": "To show that temperature has no effect on shark attacks."
      },
      "correct_answer": "B",
      "explanation": "Including temperature helps to control for its confounding effect, allowing us to see the true relationship between ice cream sales and shark attacks, which is not significant after adjusting for temperature.",
      "difficulty": -1.5,
      "category": "linear regression",
      "level": "basic",
      "topic": "linear regression",
      "source": "topic_1"
    },
    {
      "question": "In the context of the regression model discussed, what does it mean when ice cream sales is no longer a significant predictor after adjusting for temperature?",
      "choices": {
        "A": "Ice cream sales have no relationship with shark attacks.",
        "B": "Temperature is the only factor affecting shark attacks.",
        "C": "The initial observed relationship was due to the confounding effect of temperature.",
        "D": "Ice cream sales and shark attacks are independent variables."
      },
      "correct_answer": "C",
      "explanation": "The initial observed relationship between ice cream sales and shark attacks was confounded by temperature. Once temperature is included in the model, it accounts for the variation in both variables, showing that ice cream sales alone do not predict shark attacks.",
      "difficulty": -1.5,
      "category": "linear regression",
      "level": "basic",
      "topic": "linear regression",
      "source": "topic_1"
    },
    {
      "question": "What is the primary purpose of a separating hyperplane in classification?",
      "choices": {
        "A": "To minimize the distance between observations",
        "B": "To divide the space into multiple regions",
        "C": "To separate observations into two distinct classes",
        "D": "To maximize the number of observations in one class"
      },
      "correct_answer": "C",
      "explanation": "A separating hyperplane is used to divide observations into two distinct classes based on their features. It ensures that observations from different classes fall on opposite sides of the hyperplane.",
      "difficulty": -1.1,
      "category": "support vector machines",
      "level": "basic",
      "topic": "classification",
      "source": "topic_2"
    },
    {
      "question": "Why is the maximal margin hyperplane considered optimal in classification?",
      "choices": {
        "A": "It minimizes the number of support vectors",
        "B": "It maximizes the distance between the hyperplane and the closest observations",
        "C": "It ensures all observations are perfectly classified",
        "D": "It minimizes the computational complexity of the model"
      },
      "correct_answer": "B",
      "explanation": "The maximal margin hyperplane is optimal because it maximizes the margin, which is the distance between the hyperplane and the nearest observations. This helps in achieving better generalization on unseen data.",
      "difficulty": -1.2,
      "category": "support vector machines",
      "level": "basic",
      "topic": "classification",
      "source": "topic_2"
    },
    {
      "question": "In the context of support vector machines, what is a support vector?",
      "choices": {
        "A": "An observation that lies on the hyperplane",
        "B": "An observation that is farthest from the hyperplane",
        "C": "An observation that directly influences the position of the hyperplane",
        "D": "An observation that does not affect the hyperplane"
      },
      "correct_answer": "C",
      "explanation": "Support vectors are the observations that lie closest to the hyperplane and directly influence its position. They are critical in determining the optimal separating hyperplane.",
      "difficulty": -1.3,
      "category": "support vector machines",
      "level": "basic",
      "topic": "classification",
      "source": "topic_2"
    },
    {
      "question": "What is the primary purpose of using the bootstrap method in statistical learning?",
      "choices": {
        "A": "To increase the size of the dataset",
        "B": "To estimate the accuracy of a parameter estimate",
        "C": "To reduce the dimensionality of the data",
        "D": "To improve the computational efficiency of model fitting"
      },
      "correct_answer": "B",
      "explanation": "The bootstrap method is primarily used to provide a measure of accuracy of a parameter estimate or a given statistical learning method by repeatedly sampling from the data and refitting the model.",
      "difficulty": -0.6,
      "category": "resampling methods",
      "level": "basic",
      "topic": "resampling methods",
      "source": "topic_3"
    },
    {
      "question": "How does the bootstrap method differ from traditional methods in estimating standard errors?",
      "choices": {
        "A": "Bootstrap uses the entire dataset without sampling",
        "B": "Bootstrap estimates standard errors by fitting the model once",
        "C": "Bootstrap relies on resampling with replacement to estimate variability",
        "D": "Bootstrap requires a larger sample size than traditional methods"
      },
      "correct_answer": "C",
      "explanation": "Bootstrap differs from traditional methods by using resampling with replacement to estimate the variability of a parameter, allowing for the calculation of standard errors without relying on parametric assumptions.",
      "difficulty": -0.5,
      "category": "resampling methods",
      "level": "basic",
      "topic": "resampling methods",
      "source": "topic_3"
    },
    {
      "question": "What does the result of the bootstrap function indicate about the standard error of the intercept in the given example?",
      "choices": {
        "A": "The bootstrap standard error is higher than the formula-based estimate",
        "B": "The bootstrap standard error is lower than the formula-based estimate",
        "C": "The bootstrap standard error is equal to the formula-based estimate",
        "D": "The bootstrap standard error is not applicable in this context"
      },
      "correct_answer": "A",
      "explanation": "In the example, the bootstrap estimate for the standard error of the intercept (0.84) is higher than the formula-based estimate (0.717), indicating a difference in variability estimation between the two methods.",
      "difficulty": -0.7,
      "category": "resampling methods",
      "level": "basic",
      "topic": "resampling methods",
      "source": "topic_3"
    },
    {
      "question": "Which of the following statements about the lasso and ridge regression is true?",
      "choices": {
        "A": "Both lasso and ridge regression can result in some coefficients being exactly zero.",
        "B": "Ridge regression can perform variable selection by setting some coefficients to zero.",
        "C": "The lasso can result in sparse models by setting some coefficients to zero.",
        "D": "Both lasso and ridge regression use an L2 penalty to shrink coefficients."
      },
      "correct_answer": "C",
      "explanation": "The lasso uses an L1 penalty, which can force some coefficients to be exactly zero, resulting in sparse models. Ridge regression uses an L2 penalty, which shrinks coefficients but does not set them to zero, hence it does not perform variable selection.",
      "difficulty": -0.1,
      "category": "linear model selection and regularization",
      "level": "intermediate",
      "topic": "linear model selection and regularization",
      "source": "topic_4"
    },
    {
      "question": "What is a key difference between the lasso and ridge regression in terms of their constraint regions?",
      "choices": {
        "A": "The lasso constraint region is circular, while the ridge regression constraint region is diamond-shaped.",
        "B": "The lasso constraint region is diamond-shaped, while the ridge regression constraint region is circular.",
        "C": "Both the lasso and ridge regression have circular constraint regions.",
        "D": "Both the lasso and ridge regression have diamond-shaped constraint regions."
      },
      "correct_answer": "B",
      "explanation": "The lasso constraint region is diamond-shaped due to the L1 penalty, which allows for some coefficients to be exactly zero. Ridge regression has a circular constraint region due to the L2 penalty, which does not allow coefficients to be exactly zero.",
      "difficulty": -0.2,
      "category": "linear model selection and regularization",
      "level": "intermediate",
      "topic": "linear model selection and regularization",
      "source": "topic_4"
    },
    {
      "question": "In the context of linear model selection, what is a primary advantage of using the lasso over ridge regression?",
      "choices": {
        "A": "The lasso always includes all predictors in the model.",
        "B": "The lasso can handle more predictors than ridge regression.",
        "C": "The lasso can perform variable selection by setting some coefficients to zero.",
        "D": "The lasso minimizes the sum of squared coefficients."
      },
      "correct_answer": "C",
      "explanation": "The lasso can perform variable selection by setting some coefficients to zero due to its L1 penalty, which is not possible with ridge regression's L2 penalty. This makes lasso models easier to interpret.",
      "difficulty": -0.15,
      "category": "linear model selection and regularization",
      "level": "intermediate",
      "topic": "linear model selection and regularization",
      "source": "topic_4"
    },
    {
      "question": "Which of the following methods allows for modeling non-linear relationships while maintaining interpretability?",
      "choices": {
        "A": "Linear regression",
        "B": "Polynomial regression",
        "C": "K-Nearest Neighbors",
        "D": "Support Vector Machines"
      },
      "correct_answer": "B",
      "explanation": "Polynomial regression extends the linear model by adding predictors raised to a power, allowing for non-linear fits while maintaining interpretability through the use of least squares estimation.",
      "difficulty": 0.3,
      "category": "moving beyond linearity",
      "level": "intermediate",
      "topic": "moving beyond linearity",
      "source": "topic_5"
    },
    {
      "question": "What is a key advantage of regression splines over polynomial regression?",
      "choices": {
        "A": "Regression splines are easier to interpret.",
        "B": "Regression splines can fit data with fewer predictors.",
        "C": "Regression splines ensure smooth transitions at region boundaries.",
        "D": "Regression splines require fewer computations."
      },
      "correct_answer": "C",
      "explanation": "Regression splines divide the range into regions and fit polynomials within each region, ensuring smooth transitions at the boundaries, which is a key advantage over polynomial regression that can produce strange shapes near boundaries.",
      "difficulty": 0.4,
      "category": "moving beyond linearity",
      "level": "intermediate",
      "topic": "moving beyond linearity",
      "source": "topic_5"
    },
    {
      "question": "Why might a high-degree polynomial regression be problematic?",
      "choices": {
        "A": "It can lead to underfitting the data.",
        "B": "It can result in a model that is too simple.",
        "C": "It can produce overly flexible curves with strange shapes.",
        "D": "It can be difficult to compute the coefficients."
      },
      "correct_answer": "C",
      "explanation": "High-degree polynomial regression can lead to overly flexible curves that take on strange shapes, especially near the boundaries of the predictor variable, which can result in overfitting.",
      "difficulty": 0.35,
      "category": "moving beyond linearity",
      "level": "intermediate",
      "topic": "moving beyond linearity",
      "source": "topic_5"
    },
    {
      "question": "Which of the following is a key advantage of using tree-based methods for regression and classification?",
      "choices": {
        "A": "They always provide the highest prediction accuracy compared to other methods.",
        "B": "They are highly interpretable and can be graphically displayed.",
        "C": "They require a large amount of data to perform well.",
        "D": "They are less prone to overfitting than linear models."
      },
      "correct_answer": "B",
      "explanation": "Tree-based methods are known for their interpretability and the ability to be graphically displayed, making them useful for understanding the decision-making process. While they may not always provide the highest prediction accuracy, their visual representation is a significant advantage.",
      "difficulty": 0.7272727272727271,
      "category": "tree-based methods",
      "level": "intermediate",
      "topic": "tree-based methods",
      "source": "topic_6"
    },
    {
      "question": "What is the primary purpose of pruning a decision tree?",
      "choices": {
        "A": "To increase the number of terminal nodes.",
        "B": "To improve the interpretability of the tree.",
        "C": "To increase the training error rate.",
        "D": "To ensure the tree uses all available predictors."
      },
      "correct_answer": "B",
      "explanation": "Pruning a decision tree is primarily done to improve its interpretability by reducing its complexity, which can also help in reducing overfitting and potentially improve the test error rate.",
      "difficulty": 0.827272727272727,
      "category": "tree-based methods",
      "level": "intermediate",
      "topic": "tree-based methods",
      "source": "topic_6"
    },
    {
      "question": "In the context of tree-based methods, what is the role of the 'importance()' function when using random forests?",
      "choices": {
        "A": "It determines the optimal number of trees to grow.",
        "B": "It identifies the most important variables for making predictions.",
        "C": "It calculates the test error rate for the model.",
        "D": "It prunes the trees to reduce complexity."
      },
      "correct_answer": "B",
      "explanation": "The 'importance()' function in random forests is used to identify which variables are most important for making predictions, helping to understand the model's decision-making process.",
      "difficulty": 0.9272727272727271,
      "category": "tree-based methods",
      "level": "intermediate",
      "topic": "tree-based methods",
      "source": "topic_6"
    },
    {
      "question": "What is the primary advantage of using a kernel in support vector machines?",
      "choices": {
        "A": "It reduces the computational complexity by avoiding explicit feature space transformation.",
        "B": "It guarantees a linear decision boundary in all cases.",
        "C": "It ensures that all data points are perfectly classified.",
        "D": "It automatically selects the best features for classification."
      },
      "correct_answer": "A",
      "explanation": "The use of kernels in SVMs allows for the computation of similarities between observations without explicitly transforming the data into a higher-dimensional feature space, which can be computationally expensive or infeasible.",
      "difficulty": 1.2,
      "category": "support vector machines",
      "level": "advanced",
      "topic": "support vector machines",
      "source": "topic_7"
    },
    {
      "question": "How does the maximal margin classifier determine the optimal separating hyperplane?",
      "choices": {
        "A": "By minimizing the number of misclassified points.",
        "B": "By maximizing the distance between the hyperplane and the nearest data points.",
        "C": "By ensuring that the hyperplane passes through the origin.",
        "D": "By using a polynomial kernel to transform the data."
      },
      "correct_answer": "B",
      "explanation": "The maximal margin classifier selects the hyperplane that maximizes the margin, which is the perpendicular distance between the hyperplane and the nearest data points from either class.",
      "difficulty": 1.3,
      "category": "support vector machines",
      "level": "advanced",
      "topic": "support vector machines",
      "source": "topic_7"
    },
    {
      "question": "In the context of support vector machines, what is the role of support vectors?",
      "choices": {
        "A": "They are the points that lie on the decision boundary.",
        "B": "They are the points that determine the width of the margin.",
        "C": "They are the points that are misclassified by the model.",
        "D": "They are the points that have the highest influence on the model's decision boundary."
      },
      "correct_answer": "D",
      "explanation": "Support vectors are the data points that lie closest to the decision boundary and have the highest influence on its position. They are critical in defining the margin and the optimal hyperplane.",
      "difficulty": 1.1,
      "category": "support vector machines",
      "level": "advanced",
      "topic": "support vector machines",
      "source": "topic_7"
    },
    {
      "question": "What is the primary reason for the resurgence of neural networks under the name 'deep learning' after 2010?",
      "choices": {
        "A": "The introduction of support vector machines and boosting",
        "B": "The availability of larger training datasets due to digitization",
        "C": "The development of new automatic methods that require less tinkering",
        "D": "The decline in popularity of random forests"
      },
      "correct_answer": "B",
      "explanation": "Deep learning gained popularity due to the availability of larger training datasets, which allowed neural networks to be trained more effectively on complex problems such as image and video classification.",
      "difficulty": 1.7,
      "category": "deep learning",
      "level": "advanced",
      "topic": "deep learning",
      "source": "topic_8"
    },
    {
      "question": "Which regularization technique is commonly used in deep learning to prevent overfitting by randomly dropping units during training?",
      "choices": {
        "A": "Ridge regularization",
        "B": "Lasso regularization",
        "C": "Dropout regularization",
        "D": "Weight freezing"
      },
      "correct_answer": "C",
      "explanation": "Dropout regularization involves randomly dropping units during training to prevent overfitting. This technique helps in making the model robust by reducing reliance on specific neurons.",
      "difficulty": 1.8,
      "category": "deep learning",
      "level": "advanced",
      "topic": "deep learning",
      "source": "topic_8"
    },
    {
      "question": "In the context of convolutional neural networks (CNNs), what is the purpose of 'weight freezing'?",
      "choices": {
        "A": "To penalize the weights of the output layer",
        "B": "To use pretrained hidden layers for new problems with smaller datasets",
        "C": "To enforce quadratic regularization during training",
        "D": "To increase the number of training examples"
      },
      "correct_answer": "B",
      "explanation": "Weight freezing allows the use of pretrained hidden layers from models trained on large datasets for new problems with smaller datasets, reducing the need for extensive data and training.",
      "difficulty": 1.9,
      "category": "deep learning",
      "level": "advanced",
      "topic": "deep learning",
      "source": "topic_8"
    },
    {
      "question": "In survival analysis, what is the primary challenge when dealing with censored data?",
      "choices": {
        "A": "Estimating the exact survival time for all individuals",
        "B": "Handling missing data due to non-response",
        "C": "Incorporating censored observations without biasing the survival estimates",
        "D": "Predicting future events based on past data"
      },
      "correct_answer": "C",
      "explanation": "The primary challenge in survival analysis with censored data is incorporating censored observations without biasing the survival estimates. Censored data provides partial information about survival times, and improper handling can lead to biased estimates.",
      "difficulty": 2.1,
      "category": "survival analysis and censored data",
      "level": "advanced",
      "topic": "survival analysis and censored data",
      "source": "topic_9"
    },
    {
      "question": "Which assumption is crucial for the validity of survival analysis techniques when dealing with censored data?",
      "choices": {
        "A": "The censoring mechanism is dependent on the survival time",
        "B": "The censoring mechanism is independent of the survival time",
        "C": "The censoring mechanism is random and unpredictable",
        "D": "The censoring mechanism is constant over time"
      },
      "correct_answer": "B",
      "explanation": "Survival analysis techniques often assume that the censoring mechanism is independent of the survival time. This means that the reason for censoring is not related to the survival time itself, allowing for unbiased estimation of survival probabilities.",
      "difficulty": 2.2,
      "category": "survival analysis and censored data",
      "level": "advanced",
      "topic": "survival analysis and censored data",
      "source": "topic_9"
    },
    {
      "question": "What is the Kaplan-Meier estimator used for in survival analysis?",
      "choices": {
        "A": "Estimating the hazard function",
        "B": "Predicting future survival times",
        "C": "Estimating the survival function from censored data",
        "D": "Determining the cause of censoring in a dataset"
      },
      "correct_answer": "C",
      "explanation": "The Kaplan-Meier estimator is used to estimate the survival function from censored data. It provides a step-like survival curve that accounts for censored observations, allowing for the estimation of survival probabilities over time.",
      "difficulty": 2.3,
      "category": "survival analysis and censored data",
      "level": "advanced",
      "topic": "survival analysis and censored data",
      "source": "topic_9"
    },
    {
      "question": "Which of the following best describes the primary goal of Principal Components Analysis (PCA) in unsupervised learning?",
      "choices": {
        "A": "To predict a response variable using a set of features",
        "B": "To find a low-dimensional representation of data that captures most of the variance",
        "C": "To cluster observations into distinct groups based on similarity",
        "D": "To validate the results of a supervised learning model"
      },
      "correct_answer": "B",
      "explanation": "PCA is used to reduce the dimensionality of a dataset while retaining as much variance as possible. It does not involve prediction or clustering, and it is not used for validating supervised learning models.",
      "difficulty": 2.5,
      "category": "unsupervised learning",
      "level": "advanced",
      "topic": "unsupervised learning",
      "source": "topic_10"
    },
    {
      "question": "In the context of clustering methods, what is a key difference between K-means clustering and hierarchical clustering?",
      "choices": {
        "A": "K-means clustering requires the number of clusters to be specified in advance, while hierarchical clustering does not.",
        "B": "Hierarchical clustering requires the number of clusters to be specified in advance, while K-means clustering does not.",
        "C": "K-means clustering can only be used for binary data, while hierarchical clustering can be used for any type of data.",
        "D": "Hierarchical clustering can only be used for binary data, while K-means clustering can be used for any type of data."
      },
      "correct_answer": "A",
      "explanation": "K-means clustering requires the user to specify the number of clusters (K) beforehand, whereas hierarchical clustering builds a dendrogram that does not require a pre-specified number of clusters.",
      "difficulty": 2.6,
      "category": "clustering",
      "level": "advanced",
      "topic": "unsupervised learning",
      "source": "topic_10"
    },
    {
      "question": "What is a significant challenge of unsupervised learning compared to supervised learning?",
      "choices": {
        "A": "Unsupervised learning requires labeled data, while supervised learning does not.",
        "B": "There is no universally accepted method for validating unsupervised learning results.",
        "C": "Unsupervised learning is only applicable to small datasets.",
        "D": "Supervised learning cannot be used for exploratory data analysis."
      },
      "correct_answer": "B",
      "explanation": "Unsupervised learning lacks a clear mechanism for validation because there is no response variable to predict, making it challenging to assess the quality of the results.",
      "difficulty": 2.7,
      "category": "unsupervised learning",
      "level": "advanced",
      "topic": "unsupervised learning",
      "source": "topic_10"
    },
    {
      "question": "In the context of multiple testing, what is the primary advantage of using a re-sampling approach to estimate the false discovery rate (FDR)?",
      "choices": {
        "A": "It provides exact p-values for each hypothesis test.",
        "B": "It allows for the estimation of FDR without assuming a specific null distribution.",
        "C": "It guarantees a lower Type I error rate compared to theoretical methods.",
        "D": "It is computationally less intensive than using theoretical p-values."
      },
      "correct_answer": "B",
      "explanation": "The re-sampling approach is particularly useful when no theoretical null distribution is available, allowing for the estimation of FDR by approximating the null distribution through permutation tests. This flexibility is its primary advantage over methods that rely on theoretical distributions.",
      "difficulty": 2.8,
      "category": "multiple testing",
      "level": "advanced",
      "topic": "multiple testing",
      "source": "topic_11"
    },
    {
      "question": "Why might the Bonferroni correction be considered overly conservative in controlling the family-wise error rate (FWER)?",
      "choices": {
        "A": "It assumes all tests are dependent, leading to a higher FWER.",
        "B": "It requires a larger sample size to achieve the same power as other methods.",
        "C": "It controls the FWER by setting a very stringent threshold for each individual test, often resulting in fewer rejections.",
        "D": "It increases the Type I error rate to control the FWER."
      },
      "correct_answer": "C",
      "explanation": "The Bonferroni correction sets a stringent threshold (alpha/m) for each test, which can lead to fewer rejections of null hypotheses, thus increasing the likelihood of Type II errors. This conservativeness ensures control of the FWER but at the cost of reduced power.",
      "difficulty": 3.0,
      "category": "multiple testing",
      "level": "advanced",
      "topic": "multiple testing",
      "source": "topic_11"
    },
    {
      "question": "How does Holm's step-down procedure improve upon the Bonferroni correction in the context of multiple hypothesis testing?",
      "choices": {
        "A": "It increases the Type I error rate to allow more rejections.",
        "B": "It applies a uniform threshold to all tests, simplifying the process.",
        "C": "It sequentially tests hypotheses, allowing for more rejections while still controlling the FWER.",
        "D": "It uses a Bayesian approach to adjust p-values dynamically."
      },
      "correct_answer": "C",
      "explanation": "Holm's step-down procedure is less conservative than the Bonferroni correction because it sequentially tests hypotheses, adjusting the threshold as hypotheses are rejected. This allows for more rejections while still controlling the FWER, thus increasing the power of the test.",
      "difficulty": 2.9,
      "category": "multiple testing",
      "level": "advanced",
      "topic": "multiple testing",
      "source": "topic_11"
    }
  ]
}